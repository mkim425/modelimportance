[{"path":"https://mkim425.github.io/modelimportance/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 modelimportance authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (‚ÄúSoftware‚Äù), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED ‚Äú‚Äù, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://mkim425.github.io/modelimportance/articles/get-started.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Simple working examples","text":"first load necessary packages. use example forecast target data hubExamples package, provides sample datasets multiple modeling hubs hubverse format.","code":"library(hubExamples) library(modelimportance) library(dplyr) library(ggplot2)"},{"path":"https://mkim425.github.io/modelimportance/articles/get-started.html","id":"example-data","dir":"Articles","previous_headings":"","what":"Example data","title":"Simple working examples","text":"forecast data used contains forecasts weekly incident influenza hospitalizations US Massachusetts (FIPS code 25) Texas (FIPS code 48), generated November 19, 2022. forecasts two target end dates, November 26, 2022 (horizon 1), December 10, 2022 (horizon 3), produced three models: ‚ÄòFlusight-baseline‚Äô, ‚ÄòMOBS-GLEAM_FLUH‚Äô, ‚ÄòPSI-DICE‚Äô. output type median output_type_id column NAs specification required output type. modified example data slightly: forecasts removed demonstrate handling missing values. Therefore, MOBS-GLEAM_FLUH‚Äôs forecast Massachusetts November 26, 2022, PSI-DICE‚Äôs forecast Texas December 10, 2022, missing. corresponding target data contains observed hospitalization counts dates locations. visualize forecasts observed values.  expected, prediction errors increase longer horizons due greater uncertainty, forecasts December 10, 2022, showing larger deviations observed values compared November 26, 2022. Additionally, forecasts Massachusetts relatively accurate compared Texas, tend higher errors.","code":"forecast_data <- hubExamples::forecast_outputs |>   dplyr::filter(     output_type %in% c(\"median\"),     target_end_date %in% as.Date(c(\"2022-11-26\", \"2022-12-10\"))   ) |>   filter(     !(       model_id == \"MOBS-GLEAM_FLUH\" &         location == \"25\" &         target_end_date == as.Date(\"2022-11-26\")     ),     !(       model_id == \"PSI-DICE\" &         location == \"48\" &         target_end_date == as.Date(\"2022-12-10\")     )   )  forecast_data |>   knitr::kable(     format = \"html\"   ) |>   kableExtra::kable_styling(     font_size = 12,     bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),     full_width = TRUE   ) target_data <- hubExamples::forecast_target_ts |>   dplyr::filter(     target_end_date %in% unique(forecast_data$target_end_date),     location %in% unique(forecast_data$location),     target == \"wk inc flu hosp\"   ) |>   # Rename columns to match the oracle output format   rename(oracle_value = observation)  target_data |>   knitr::kable(     format = \"html\"   ) |>   kableExtra::kable_styling(     bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),     full_width = TRUE   ) forecast_data |>   ggplot(aes(x = target_end_date)) +   geom_point(aes(y = value, color = model_id), size = 2) +   facet_wrap(~location,     scales = \"free_y\",     labeller = labeller(location = function(x) paste0(\"Location: \", x))   ) +   geom_point(     data = target_data,     aes(y = oracle_value, group = 1, shape = \"Observed\"),     alpha = 1, size = 2   ) +   scale_x_date(     breaks = target_data$target_end_date,     date_labels = \"%Y-%m-%d\", expand = expansion(add = c(5, 5))   ) +   scale_color_manual(     name = \"model_id/Observed\",     values = c(       \"Flusight-baseline\" = \"#619CFF\",       \"MOBS-GLEAM_FLUH\" = \"#00BA38\", \"PSI-DICE\" = \"#F8766D\"     ),     limits = c(\"Flusight-baseline\", \"MOBS-GLEAM_FLUH\", \"PSI-DICE\")   ) +   scale_shape_manual(values = c(\"Observed\" = 1)) +   labs(     x = \"Date\", y = \"Weekly Hospitalization\",     title = \"Forecasts of incident deaths generated on November 19, 2022\"   )"},{"path":"https://mkim425.github.io/modelimportance/articles/get-started.html","id":"evaluation-using-lomo-algorithm","dir":"Articles","previous_headings":"","what":"Evaluation using LOMO algorithm","title":"Simple working examples","text":"quantify contribution model within ensemble using model_importance() function. following code evaluates importance ensemble member simple mean ensemble using LOMO algorithm. models missed forecasts certain tasks, NA values assigned importance column tasks. summarize importance scores model averaging across tasks. NA values removed averaging process setting na_action argument \"drop\". results show model ‚ÄòPSI-DICE‚Äô highest importance score, followed ‚ÄòFlusight-baseline‚Äô ‚ÄòMOBS-GLEAM_FLUH‚Äô. , ‚ÄòPSI-DICE‚Äô contributes improving ensemble‚Äôs predictive performance, whereas ‚ÄòMOBS-GLEAM_FLUH‚Äô, negative score, detracts ensemble‚Äôs performance. Another approach handling NA values use \"worst\" option na_action, replaces NA values worst (.e., minimum) score among models task. results show importance scores ‚ÄòFlusight-baseline‚Äô unchanged missing forecast. observe importance score ‚ÄòPSI-DICE‚Äô, previously positive, now decreased negative value compared evaluation using \"drop\" option na_action. Moreover, ‚ÄòMOBS-GLEAM_FLUH‚Äô still ranks lowest, importance score increased. also possible impute missing scores intermediate values assigning average importance scores models task. strategy may offer balanced trade-mitigating influence missing data without overly penalizing overlooking .","code":"scores_lomo <- model_importance(   forecast_data = forecast_data, oracle_output_data = target_data,   ensemble_fun = \"simple_ensemble\",   importance_algorithm = \"lomo\" )  scores_lomo |>   knitr::kable(     format = \"html\"   ) |>   kableExtra::kable_styling(     font_size = 12,     bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),     full_width = TRUE   ) summary.lomo.drop <- model_importance_summary(   scores_lomo,   by = \"model_id\", na_action = \"drop\", fun = mean )  summary.lomo.drop |>   knitr::kable(     format = \"html\"   ) |>   kableExtra::kable_styling(     bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),     full_width = TRUE   ) summary.lomo.worst <- model_importance_summary(   scores_lomo,   by = \"model_id\", na_action = \"worst\", fun = mean )  summary.lomo.worst |>   knitr::kable(     format = \"html\"   ) |>   kableExtra::kable_styling(     bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),     full_width = TRUE   ) summary.lomo.avg <- model_importance_summary(   scores_lomo,   by = \"model_id\", na_action = \"average\", fun = mean )  summary.lomo.avg |>   knitr::kable(     format = \"html\"   ) |>   kableExtra::kable_styling(     bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),     full_width = TRUE   )"},{"path":"https://mkim425.github.io/modelimportance/articles/get-started.html","id":"evaluation-using-lasomo-algorithm","dir":"Articles","previous_headings":"","what":"Evaluation using LASOMO algorithm","title":"Simple working examples","text":"Now demonstrate use LASOMO algorithm evaluation model importance. explored difference na_action options previous LOMO example, focus options subset_wt, specifies weights assigned subsets models calculating importance scores, na_action fixed \"drop\". following code corresponding outputs illustrate evaluation using weighting scheme. example, three models (n=3n = 3), weights differ significantly two weighting schemes. Therefore, resulting outputs show little difference. However, general, larger number models, two weighting schemes may yield different importance scores model. Note computational time 0.3 seconds LOMO LASOMO algorithms. However, time can increased substantially large number models tasks. See detailed discussions execution time computational feasibility Computational complexity section accompanying article. extensive application complex scenarios larger number models can found Kim, Ray, Reich (2024).","code":"# LASOMO - equal weights scores_lasomo_eq <- model_importance(   forecast_data = forecast_data, oracle_output_data = target_data,   ensemble_fun = \"simple_ensemble\",   importance_algorithm = \"lasomo\", subset_wt = \"equal\" )  summary.lasomo_eq <- model_importance_summary(   scores_lasomo_eq,   by = \"model_id\", na_action = \"drop\", fun = mean )  summary.lasomo_eq |>   knitr::kable(     format = \"html\"   ) |>   kableExtra::kable_styling(     bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),     full_width = TRUE   ) # LASOMO - perm based weights scores_lasomo_perm <- model_importance(   forecast_data = forecast_data,   oracle_output_data = target_data,   ensemble_fun = \"simple_ensemble\",   importance_algorithm = \"lasomo\", subset_wt = \"perm_based\" )  summary.lasomo_perm <- model_importance_summary(   scores_lasomo_perm,   by = \"model_id\", na_action = \"drop\", fun = mean )  summary.lasomo_perm |>   knitr::kable(     format = \"html\"   ) |>   kableExtra::kable_styling(     bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),     full_width = TRUE   )"},{"path":[]},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance-article.html","id":"abstract","dir":"Articles","previous_headings":"","what":"Abstract","title":"`modelimportance`: Evaluating model importance within a multi-model ensemble in R","text":"Ensemble forecasts commonly used support decision-making policy planning across various fields often offer improved accuracy stability compared individual models. model unique characteristics, understanding measuring value constituent model adds overall accuracy ensemble can support construction effective ensembles. Kim, Ray, Reich (2024) introduced metrics quantify component model contributes accuracy ensemble performance demonstrated use context probabilistic forecasting. Building work, modelimportance package provides tools extended applications, embracing point probabilistic forecasts. supports multiple functionalities, allowing users specify ensemble approach implement model importance metric use. Additionally, software offers customizable options handling missing values. features enable package serve versatile tool researchers practitioners. helps constructing effective ensemble model across wide range forecasting tasks, also understanding role model within ensemble gaining insights individual models . package follows ‚Äòhubverse‚Äô (Consortium Infectious Disease Modeling Hubs 2024) framework, collection open-source software tools developed promote collaborative modeling hub efforts simplify setup operation. Accordingly, depends several packages hubverse ecosystem, hubUtils, hubEnsembles, hubEvals, hubExamples. enables seamless integration flexibility forecasting tools systems, allowing many analyses performed existing ongoing hubs.","code":""},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance-article.html","id":"sec:intro","dir":"Articles","previous_headings":"","what":"Introduction","title":"`modelimportance`: Evaluating model importance within a multi-model ensemble in R","text":"Ensemble forecasting method produce single, consolidated prediction combining forecasts generated different models. model‚Äôs strengths pronounced, weaknesses counterbalanced, leads ensemble forecast robust accurate (Gneiting Raftery 2005; Hastie, Tibshirani, Friedman 2001). Specifically, ensembles effectively mitigate bias variance arising predictions individual models averaging , aggregating way can reduce prediction errors improve overall performance. Enhanced prediction accuracy robustness enable achievement reliable predictions, thereby improving decision-making. reason, ensemble forecasting widely used across various domains weather forecasting (Jordan . Guerra et al. 2020; Gneiting Raftery 2005), financial modeling (Sun, Wang, Wei 2020; et al. 2023), infectious disease outbreak forecasting (Evan L. Ray Reich 2018; N. G. Reich et al. 2019; Lutz et al. 2019; Viboud et al. 2018). example, throughout COVID-19 pandemic, US COVID-19 Forecast Hub collected individual models developed 90 different research groups built probabilistic ensemble forecasting model COVID-19 cases, hospitalizations, deaths US based models‚Äô predictions, served official short-term forecasts US Centers Disease Control Prevention (CDC) (Cramer et al. 2022). quality forecasts assessed evaluating error, bias, sharpness, /calibration using different scoring metrics. selection scoring metrics depends type forecast: point forecasts (e.g., mean, median) probabilistic forecasts (e.g., quantiles, samples, predictive cumulative distribution functions, probability mass function). Commonly used assessment tools point forecasts mean absolute error (MAE) mean squared error (MSE), calculate average magnitude forecast errors. Scoring metrics probabilistic forecasts consider uncertainty variability predictions provide concise evaluations numerical scores (Gneiting Raftery 2007). examples include weighted interval score (WIS) quantile-based forecasts continuous ranked probability score (CRPS) forecasts taking form predictive cumulative distribution functions (Bracher et al. 2021). note CRPS general scoring rule can computed either analytically closed form numerically samples, WIS quantile-based approximation CRPS. Several R packages developed purpose. name , fable package (O‚ÄôHara-Wild, Hyndman, Wang 2024) widely used univariate time series forecasting includes functions accuracy measurement. Metrics (Hamner Frasco 2018) MLmetrics (Yan 2024) provide wide range performance metrics specifically designed evaluating machine learning models. scoringRules (Jordan, Kr√ºger, Lerch 2019) package offers comprehensive set proper scoring rules evaluating probabilistic forecasts supports univariate multivariate settings. scoringutils (Bosse et al. 2022) package offers additional features functionality provided scoringRules, makes useful certain tasks, summarizing, comparing, visualizing forecast performance. packages valuable evaluate individual models independent entities, using performance metrics selected specific situation problem type. However, measure individual models‚Äô contributions enhanced predictive accuracy used part ensemble. Kim, Ray, Reich (2024) demonstrate model‚Äôs individual performance necessarily correspond contribution component within ensemble. developed package introduces capability. modelimportance package provides tools evaluate role model ensemble member within ensemble model, rather focusing individual predictive performance per se. ensemble forecasting, certain models contribute significantly overall predictions others. Assessing impact component model ensemble predictions methodologically similar determining variable importance traditional regression machine learning models, variable importance measures evaluate much individual variables decrease accuracy model‚Äôs predictive performance reduce average loss. R packages randomForest (Liaw Wiener 2002), caret (Kuhn 2008), xgboost (Chen et al. 2024), gbm (Ridgeway Developers 2024) implement functions different types models: random forest models, general machine learning models, extreme gradient boosting models, generalized boosted regression models, respectively. packages focus feature-level importance within single model measure contribution individual models within ensemble. modelimportance package addresses limitation. tools modelimportance quantify component model helps enhance ensemble model‚Äôs predictive performance. assign numerical scores model based selected metric measures forecast accuracy, depending forecast type. capabilities provide unique support hub organizers refer entity responsible launching managing collective modeling hub. coordinate multiple teams produce forecasts integrate predictions ensemble forecast (Shandross et al. 2024), , mentioned earlier, known better performance compared individual models. Examples include US CDC European Centre Disease Prevention Control. modelimportance package can even strengthen benefits multi-model ensemble helping organizations create effective ensemble forecasts based precise evaluation model‚Äôs contribution. Specifically, modelimportance follows ‚Äòhubverse‚Äô standards, ‚Äòhubverse‚Äô offers set publicly available software data tools developed promote collaborative modeling hub efforts reduce effort required set operate (Consortium Infectious Disease Modeling Hubs 2024). Adherence model output formats specified hubverse convention enables many analyses performed existing ongoing hubs seamless integration flexibility forecasting tools systems. note dozen active hubs running fall 2025, planning stages. highlight strong development practices employed, unit testing individual functions, continuous integration testing different operating systems, independent code review. emphasis quality control key strength work distinguishes academic software development projects. paper proceeds follows. describes package relates hubverse framework, including dependencies, model output formats defined within hubverse, structure data presentation forecasts actual observations. presents two algorithms implemented calculating model importance metric: leave-one-model-leave--subsets--models-. demonstrate various functionalities supports highlight quality assurance measures open access . examples provided . close paper concluding remarks discussion possible extensions.","code":""},{"path":[]},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance-article.html","id":"subsec:dependence_hubverse","dir":"Articles","previous_headings":"Data","what":"Relationship and dependencies on hubverse","title":"`modelimportance`: Evaluating model importance within a multi-model ensemble in R","text":"modelimportance package designed work hubverse framework , accordingly, depends several packages hubverse ecosystem, hubUtils (Krystalli Shandross (2025)), hubEnsembles (Shandross, Howerton, Ray (2025)), hubEvals (N. Reich et al. (2025)), hubExamples (Evan L. Ray, Sweger, Contamin (2025)). modelimportance uses model_out_tbl S3 class model output format defined hubUtils, consists utility functions standardize prediction files data formats (details ). Ensembling predictions multiple models relies hubEnsembles, offers broadly applicable framework construct multi-model ensembles using various ensemble methods. Calculation forecast accuracy using various metrics based hubEvals, internally leverages scoringutils. use example datasets hubExamples testing demonstration purposes (see ).","code":""},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance-article.html","id":"subsec:model_output_format","dir":"Articles","previous_headings":"Data","what":"Model output format","title":"`modelimportance`: Evaluating model importance within a multi-model ensemble in R","text":"Model outputs structured tabular format designed specifically predictions, formal S3 object called model_out_tbl. hubverse standard, row represents individual prediction component prediction single task, details described multiple columns one can identify unique label assigned forecasting model, task characteristics, prediction representation type, predicted values (Shandross et al. 2024). elaborate task characteristics, prediction task means specific forecasting problem can described set task ID variables. Examples variables include date forecasts generated, target predict (e.g., flu-related incident deaths, cases, hospitalizations), prediction horizon, length time future point model generate forecast, specific location certain target date. (tbl-example-model_output?) illustrates short-term forecasts weekly incident influenza hospitalizations US Massachusetts, generated model ‚ÄòFlusight-baseline‚Äô December 17, 2022, model_out_tbl format. model_id column lists uniquely identified model name. reference_date, target, horizon, location, target_end_date columns referred task ID variables, together defines task characteristics. Note forecast generation date target date prediction made mapped reference_date target_end_date columns, respectively, location represented based FIPS code (e.g., ‚Äò25‚Äô Massachusetts). time length target_end_date, number weeks ahead reference_date, indicated horizon column. prediction representation specified ‚Äòquantile‚Äô output_type column, details represented output_type_id column seven quantiles 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95 target end date. predicted value corresponding quantile recorded value column. Table 1: Example model output incident influenza hospitalizations (top 10 rows) extracted forecast_outputs data hubExamples package. (fig-example-model_output?) visualize information prediction task provided (tbl-example-model_output?) three models. model, quantile-based forecasts shown target end dates December 24, 2022 (horizon 1), December 31, 2022 (horizon 2), January 07, 2023 (horizon 3), made December 17, 2022 based historical data available date. prediction intervals defined lowest highest quantiles (0.05 0.95) represent uncertainty predictions. give brief details interpretation, Flusight-baseline model -predicted outcomes first two target dates (horizon 1 2), -predicted outcome last target date (horizon 3). prediction intervals narrow compared two models, indicates confident predictions. However, two three prediction intervals (horizons 1 2) failed cover eventually observed values, implying model apparently overconfident. Figure 1: Example plot three distributional forecasts corresponding model output incident influenza hospitalizations shown Table 1. Solid black dots indicate historically available data forecast generation date, open black circles indicate eventually observed values. blue dots represent predictive medians blue shaded area represents corresponding 90% prediction interval defined 0.05 0.95 quantiles.","code":""},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance-article.html","id":"subsec:model_output","dir":"Articles","previous_headings":"Data","what":"Forecast data representation","title":"`modelimportance`: Evaluating model importance within a multi-model ensemble in R","text":"Generally, quantitative forecasts can categorized either point forecasts probabilistic forecasts. specific prediction task, point forecasts, represented single predicted value, provide clear concise prediction, making easy interpret communicate. Probabilistic forecasts, hand, provide probability distribution possible future values, inherently involves uncertainty. represented various ways, probability mass functions (pmf), cumulative distribution functions (cdf), samples, probability quantiles (intervals). output_type output_type_id columns model output format, defined hubverse convention, specify forecast structure. one output_type allowed, must one ‚Äòmean‚Äô, ‚Äòmedian‚Äô, ‚Äòquantile‚Äô, ‚Äòpmf‚Äô modelimportance package: ‚Äòmean‚Äô ‚Äòmedian‚Äô point forecasts ‚Äòquantile‚Äô ‚Äòpmf‚Äô probabilistic forecasts. aforementioned, output_type_id column identifies addtional detailed information, specific quantile levels (e.g., ‚Äú0.1‚Äù, ‚Äú0.25‚Äù, ‚Äú0.5‚Äù, ‚Äú0.75‚Äù, ‚Äú0.9‚Äù) ‚Äòquantile‚Äô output type categorical values (e.g., ‚Äúlow‚Äù, ‚Äúmoderate‚Äù, ‚Äúhigh‚Äù, ‚Äúhigh‚Äù) ‚Äòpmf‚Äô output type. predicted values pmf constrained 0 1, indicating probability categorical level, unbounded numeric otherwise. Different output types correspond different scoring rules evaluating model‚Äôs prediction performance. (tbl-pair-output-scoringrule?) presents output types associated scoring rules supported modelimportance package. Table 2: Pairs output types associated scoring rules evaluating prediction performance.","code":""},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance-article.html","id":"subsec:oracle_output_data","dir":"Articles","previous_headings":"Data","what":"Oracle output data","title":"`modelimportance`: Evaluating model importance within a multi-model ensemble in R","text":"oracle_output_data data frame contains ground truth values variables used define modeling targets. referred ‚Äúoracle‚Äù formatted oracle made perfect point prediction equal truth. data must follow oracle output format defined hubverse standard, includes independent task ID columns (e.g., location, target_date), output_type column specifying output type predictions oracle_value column observed values. forecast data, output_type either \"quantile\" \"pmf\", output_type_id column often required provide identifying information. model_out_tbl oracle_output_data must task ID columns output_type, including output_type_id necessary, used match predictions ground truth data.","code":""},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance-article.html","id":"sec:algorithms","dir":"Articles","previous_headings":"","what":"Algorithms","title":"`modelimportance`: Evaluating model importance within a multi-model ensemble in R","text":"section provides brief description leave one model (LOMO) leave subsets models (LASOMO) algorithms, used compute model importance score. basic idea measuring importance component model evaluate change ensemble performance model included excluded ensemble construction. specifically, compare performance ensemble without specific model specific task, consider difference performance importance model task. apply idea many tasks aggregate importance scores model across tasks using averages. (Details can found Kim, Ray, Reich (2024).) LOMO involves creating ensemble excluding one component model entire set models. Let ùíú{\\mathcal } set nn models FiF^forecast produced model ii, =1,2,‚Ä¶,n.= 1,2, \\dots, n. ensemble excludes exactly one model including others. Fùíú‚àí^{{\\mathcal }^{-}} denotes ensemble forecast constructed using forecasts FùíúF^{\\mathcal } except FiF^. Model ii‚Äôs importance score using LOMO calculated difference accuracy, measured specific score, Fùíú‚àí^{{\\mathcal }^{-}} FùíúF^{\\mathcal } (). example, evaluating model 1 within ensemble three models (n=3n=3), LOMO creates ensemble forecast F{2,3}F^{\\{2,3\\}} using F2F^2 F3F^3. performance reduced ensemble compared full ensemble forecast F{1,2,3}F^{\\{1,2,3\\}}, incorporates three models. note model can make ensemble better worse, thus importance score model 1 can positive negative accordingly. Algorithm 1: Leave one model (LOMO) hand, LASOMO involves ensemble constructions possible subsets models. subset SS contain model ii, S‚à™{}S \\cup \\{\\} plays role ùíú{\\mathcal } LOMO; score associated subset SS difference measures FSF^S FS‚à™{}F^{S \\cup \\{\\}}. , scores aggregated across possible subsets model ii belong (). example, using earlier setup three forecast models, LASOMO considers three subsets, denote S1={2}S_1=\\{2\\}, S2={3}S_2=\\{3\\}, S3={2,3}S_3=\\{2, 3\\}, calculate importance score model 1 (excluding subsets include model 1). ensemble forecasts F{2},F{3}F^{\\{2\\}}, F^{\\{3\\}}, F{2,3}F^{\\{2,3\\}} compared F{1,2},F{1,3}F^{\\{1,2\\}}, F^{\\{1,3\\}}, F{1,2,3}F^{\\{1,2,3\\}}, respectively. performance differences attributable model 1‚Äôs inclusion aggregated, results importance score model 1. note subsets (e.g., S1,S2,S_1, S_2, S3S_3) may different weights aggregating process. Algorithm 2: Leave subsets models (LASOMO) modelimportance package offers two weighting options subsets: one assigns equal (uniform) weights subsets, assigns weights based size, similar concept Shapley values cooperative game theory, measure player‚Äôs average contribution possible coalitions (, equivalently, permutations players) (Shapley (1953)). Users can choose one evaluate contribution model manner suited preferred framework. Algorithms outline steps implement LOMO LASOMO single prediction task, respectively.","code":""},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance-article.html","id":"comparison-of-weighting-schemes-in-lasomo","dir":"Articles","previous_headings":"Algorithms","what":"Comparison of weighting schemes in LASOMO","title":"`modelimportance`: Evaluating model importance within a multi-model ensemble in R","text":"differences two weighting schemes influence importance scores become pronounced number models increases. described , formulas subset weights 12n‚àí1‚àí1and1(n‚àí1)(n‚àí1k),\\frac{1}{2^{n-1}-1} \\quad\\text{}\\quad \\frac{1}{(n-1)\\binom{n-1}{k}}, kk size subset nn total number models. equal scheme treats subsets equally, medium-sized subsets considerable influence final result, many subsets. contrast, permutation-based scheme adjusts weights according subset size, giving greatest weight smallest largest subsets assigning small weights mid-sized subsets. Moreover, weights assigned mid-sized subsets permutation-based approach decrease much faster nn equal weighting scheme (see details). Consequently, nn large, middle-sized subsets play dominant role determining importance scores equal weighting scheme, whereas extreme-sized subsets primarily drive scores permutation-based weighting approach. Overall, difference two weighting schemes likely arise mainly extreme-sized subsets nn large. weights given mid-sized subsets become increasingly similar, small values order 10‚àí310^{-3} even n=8n=8, weights assigned smallest largest subsets remain substantially different ((fig-lasomo-weights?)). Figure 2:Comparison weights assigned subset. plot shows weights assigned subset number models nn increases 2 10. red line represents weights equal weighting scheme , blue green lines represent minimum maximum weights, respectively, permutation-based weighting scheme. minimum weight occurs subset size around (n‚àí1)/2(n-1)/2, maximum weight occurs subset size n‚àí1n-1. number models increases, weights assigned two schemes become increasingly similar mid-sized subsets whereas substantial differences remain extreme-sized subsets.","code":""},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance-article.html","id":"sec:main-functions","dir":"Articles","previous_headings":"","what":"Main functions","title":"`modelimportance`: Evaluating model importance within a multi-model ensemble in R","text":"section, describe usage function model_importance(), multiple options available customize evaluation framework ((tbl-arguments1?), (tbl-arguments2?)).","code":""},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance-article.html","id":"model_importance","dir":"Articles","previous_headings":"Main functions","what":"model_importance( )","title":"`modelimportance`: Evaluating model importance within a multi-model ensemble in R","text":"model_importance() function calculates importance scores ensemble component models based contributions improving ensemble prediction accuracy prediction task. returns single data frame importance scores combined across tasks. model missed predictions specific task, NA value assigned task. forecast_data data frame containing predictions can coerced model_out_tbl format, standard S3 class model output format defined hubverse convention. fails coerced model_out_tbl format, error message returned hubUtils, provides function as_model_out_tbl() purpose. Users may need manually transform data meet hubverse standards. oracle_output_data data frame containing actual observed values variables used specify modeling targets. Details provided . ensemble_fun argument specifies ensemble method used evaluating model importance, relies implementations package (Shandross, Howerton, Ray (2025)). currently supported methods \"simple_ensemble\" \"linear_pool\". \"simple_ensemble\" method returns average predicted values component models per prediction task defined task IDs, output_type, output_type_id columns. default aggregation function method \"mean\", can customized specifying additional arguments ..., agg_fun=\"median\". \"linear_pool\" specified, ensemble model outputs created linear pool component model outputs. method supports output_type \"mean\", \"quantile\", \"pmf\". Table 3: Description arguments model_importance() function, including purpose, possible values, default settings. importance_algorithm argument specifies algorithm model importance calculation, can either \"lomo\" (leave one model ) \"lasomo\" (leave subsets models ). subset_wt argument employed \"lasomo\" algorithm. argument two options: \"equal\" assigns equal weight subsets \"perm_based\" assigns weight averaged possible permutations formula Shapley values (). default values importance_algorithm subset_wt \"lomo\" \"equal\", respectively. min_log_score argument relevant output_type \"pmf\", uses Log Score scoring rule. sets minimum threshold log scores avoid issues extremely low probabilities assigned true outcome, can lead undefined negative infinite log scores. probability lower threshold adjusted minimum value calculating importance metric based log score. default value set -10, following CDC FluSight thresholding convention (Brooks et al. 2018; N. G. Reich et al. 2019). Users may choose different value based practical needs.","code":"model_importance(forecast_data, oracle_output_data, ensemble_fun,                  importance_algorithm, subset_wt, min_log_score,                  ...)"},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance-article.html","id":"model_importance_summary","dir":"Articles","previous_headings":"Main functions","what":"model_importance_summary( )","title":"`modelimportance`: Evaluating model importance within a multi-model ensemble in R","text":"model_importance_summary() function summarizes importance scores produced model_importance() across tasks model. importance_scores data frame containing model importance scores individual prediction tasks, produced model_importance() function. argument specifies grouping variable(s) summarization. default \"model_id\" summarize importance scores model. Users can also specify columns present importance_scores data frame needed. na_action argument allows specifying handle NA values generated importance score calculation task, occurring model contribute ensemble prediction given task missing forecast submission. Three options available: \"worst\", \"average\", \"drop\". specific prediction task, model missing predictions, \"worst\" option replaces NA values smallest value among models‚Äô importance metrics, \"average\" option replaces average models‚Äô importance metrics task. \"drop\" option removes NA values, results exclusion model evaluation task. fun argument specifies function used summarize importance scores. fun = mean default choice, summary functions also applicable (e.g., fun = median). Additional arguments can passed summary function fun ... needed (e.g., fun = quantile, probs = 0.25 quartile summary). Table 4: Description arguments model_importance_summary() function, including purpose, possible values, default settings.","code":"model_importance_summary(importance_scores, by = \"model_id\",                          na_action = c(\"drop\", \"worst\", \"average\"),                          fun = mean, ...)"},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance-article.html","id":"sec:implementation-and-availability","dir":"Articles","previous_headings":"","what":"Implementation and availability","title":"`modelimportance`: Evaluating model importance within a multi-model ensemble in R","text":"package implemented R distributed via CRAN GitHhub. conducted unit tests using testthat package (Wickham 2011) ensure functions work correctly expected, including used internally. also performed continuous integration testing using GitHub Actions maintain functionality across platforms, including Windows, macOS, Linux. Integrated GitHub Action, used package maintain consistent code style, code quality, detection potential issues. code changes systematically reviewed fellow team members, enhanced reliability.","code":""},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance-article.html","id":"sec:examples","dir":"Articles","previous_headings":"","what":"Examples","title":"`modelimportance`: Evaluating model importance within a multi-model ensemble in R","text":"examples section illustrate use model_importance() function evaluate importance component models within ensemble, using various combinations arguments described . use example forecast target data hubExamples package, provides sample datasets multiple modeling hubs hubverse format.","code":""},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance-article.html","id":"sec:example-data","dir":"Articles","previous_headings":"Examples","what":"Example data","title":"`modelimportance`: Evaluating model importance within a multi-model ensemble in R","text":"forecast data used contains forecasts weekly incident influenza hospitalizations US Massachusetts (FIPS code 25) Texas (FIPS code 48), generated November 19, 2022. forecasts two target end dates, November 26, 2022 (horizon 1), December 10, 2022 (horizon 3), produced three models: ‚ÄòFlusight-baseline‚Äô, ‚ÄòMOBS-GLEAM_FLUH‚Äô, ‚ÄòPSI-DICE‚Äô. output type median output_type_id column NAs specification required output type. modified example data slightly: forecasts removed demonstrate handling missing values. Therefore, MOBS-GLEAM_FLUH‚Äôs forecast Massachusetts November 26, 2022, PSI-DICE‚Äôs forecast Texas December 10, 2022, missing. corresponding target data contains observed hospitalization counts dates locations. Figure 3: Plot three point forecasts (median) eventually observed values forecast_data target_data weekly incident influenza hospitalizations Massachusetts (FIPS code 25) Texas (FIPS code 48). Colored dots indicate forecasts three models, generated November 19, 2022. Open black circles indicate eventually observed values. MOBS-GLEAM_FLUH‚Äôs forecast Massachusetts November 26, 2022, PSI-DICE‚Äôs forecast Texas December 10, 2022, missing. expected, prediction errors increase longer horizons due greater uncertainty, forecasts December 10, 2022, showing larger deviations observed values compared November 26, 2022. Additionally, forecasts Massachusetts relatively accurate compared Texas, tend higher errors ((fig-example-median-lomo?)).","code":"forecast_data #> # A tibble: 10 √ó 9 #>    model_id          reference_date target          horizon location target_end_date output_type output_type_id value #>    <chr>             <date>         <chr>             <int> <chr>    <date>          <chr>       <chr>          <dbl> #>  1 Flusight-baseline 2022-11-19     wk inc flu hosp       1 25       2022-11-26      median      NA                51 #>  2 Flusight-baseline 2022-11-19     wk inc flu hosp       3 25       2022-12-10      median      NA                51 #>  3 Flusight-baseline 2022-11-19     wk inc flu hosp       1 48       2022-11-26      median      NA              1052 #>  4 Flusight-baseline 2022-11-19     wk inc flu hosp       3 48       2022-12-10      median      NA              1052 #>  5 MOBS-GLEAM_FLUH   2022-11-19     wk inc flu hosp       3 25       2022-12-10      median      NA                43 #>  6 MOBS-GLEAM_FLUH   2022-11-19     wk inc flu hosp       1 48       2022-11-26      median      NA              1072 #>  7 MOBS-GLEAM_FLUH   2022-11-19     wk inc flu hosp       3 48       2022-12-10      median      NA               688 #>  8 PSI-DICE          2022-11-19     wk inc flu hosp       1 25       2022-11-26      median      NA                90 #>  9 PSI-DICE          2022-11-19     wk inc flu hosp       3 25       2022-12-10      median      NA               159 #> 10 PSI-DICE          2022-11-19     wk inc flu hosp       1 48       2022-11-26      median      NA              1226 target_data #> # A tibble: 4 √ó 4 #>   target_end_date target          location oracle_value #>   <date>          <chr>           <chr>           <dbl> #> 1 2022-11-26      wk inc flu hosp 25                221 #> 2 2022-11-26      wk inc flu hosp 48               1929 #> 3 2022-12-10      wk inc flu hosp 25                578 #> 4 2022-12-10      wk inc flu hosp 48               1781"},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance-article.html","id":"sec:example-lomo","dir":"Articles","previous_headings":"Examples","what":"Evaluation using LOMO algorithm","title":"`modelimportance`: Evaluating model importance within a multi-model ensemble in R","text":"quantify contribution model within ensemble using model_importance() function. following code evaluates importance ensemble member simple mean ensemble using LOMO algorithm. call generates result informative messages, summarizing input data, including number dates forecasts produced number models ids follows. function output data frame containing model ids corresponding importance scores prediction task, along task id columns. models missed forecasts certain tasks, NA values assigned importance column tasks. summarize importance scores model averaging across tasks. NA values removed averaging process setting na_action argument \"drop\". results show model ‚ÄòPSI-DICE‚Äô highest importance score, followed ‚ÄòFlusight-baseline‚Äô ‚ÄòMOBS-GLEAM_FLUH‚Äô. , ‚ÄòPSI-DICE‚Äô contributes improving ensemble‚Äôs predictive performance, whereas ‚ÄòMOBS-GLEAM_FLUH‚Äô, negative score, detracts ensemble‚Äôs performance. low importance score ‚ÄòMOBS-GLEAM_FLUH‚Äô mainly due substantially larger prediction error Texas target end date December 10, 2022, compared models, missing forecast Massachusetts November 26, 2022, factored evaluation. single large error significantly affected contribution score. Another approach handling missing values use \"worst\" option na_action, replaces missing values worst (.e., minimum) score among models task. results show importance scores ‚ÄòFlusight-baseline‚Äô unchanged missing forecast. observe importance score ‚ÄòPSI-DICE‚Äô, previously positive, now decreased negative value compared evaluation using \"drop\" option na_action. Moreover, ‚ÄòMOBS-GLEAM_FLUH‚Äô still ranks lowest, importance score increased. change related varying forecast accuracy across different tasks. target end date November 26, 2022, Massachusetts, forecasts relatively accurate. Thus, even ‚ÄòMOBS-GLEAM_FLUH‚Äô assigned worst value importance score missing forecast, including value averaging detrimental overall importance metric; rather, beneficial excluding . contrast, target end date December 10, 2022, Texas, forecasts much larger errors across board, assigning worst value importance score missing forecast ‚ÄòPSI-DICE‚Äô task detrimental effect averaging importance scores. scale importance scores influenced magnitude prediction errors: tasks small errors, scores remain moderate, tasks large errors can yield importance scores much greater magnitude. also possible impute missing scores intermediate values assigning average importance scores models task. strategy may offer balanced trade-mitigating influence missing data without overly penalizing overlooking .","code":"model_importance(   forecast_data = forecast_data,   oracle_output_data = target_data,   ensemble_fun = \"simple_ensemble\",   importance_algorithm = \"lomo\" ) Forecasts from 2022-11-19 to 2022-11-19 (a total of 1 forecast date(s)). The available model IDs are:      Flusight-baseline      MOBS-GLEAM_FLUH      PSI-DICE  (a total of 3 models) #> Model importance result by task #> --------------------------------- #>             model_id reference_date          target horizon location target_end_date output_type importance #> 1  Flusight-baseline     2022-11-19 wk inc flu hosp       1       25      2022-11-26      median  -19.50000 #> 2    MOBS-GLEAM_FLUH     2022-11-19 wk inc flu hosp       1       25      2022-11-26      median         NA #> 3           PSI-DICE     2022-11-19 wk inc flu hosp       1       25      2022-11-26      median   19.50000 #> 4  Flusight-baseline     2022-11-19 wk inc flu hosp       1       48      2022-11-26      median  -32.33333 #> 5    MOBS-GLEAM_FLUH     2022-11-19 wk inc flu hosp       1       48      2022-11-26      median  -22.33333 #> 6           PSI-DICE     2022-11-19 wk inc flu hosp       1       48      2022-11-26      median   54.66667 #> 7  Flusight-baseline     2022-11-19 wk inc flu hosp       3       25      2022-12-10      median  -16.66667 #> 8    MOBS-GLEAM_FLUH     2022-11-19 wk inc flu hosp       3       25      2022-12-10      median  -20.66667 #> 9           PSI-DICE     2022-11-19 wk inc flu hosp       3       25      2022-12-10      median   37.33333 #> 10 Flusight-baseline     2022-11-19 wk inc flu hosp       3       48      2022-12-10      median  182.00000 #> 11   MOBS-GLEAM_FLUH     2022-11-19 wk inc flu hosp       3       48      2022-12-10      median -182.00000 #> 12          PSI-DICE     2022-11-19 wk inc flu hosp       3       48      2022-12-10      median         NA scores_lomo <- model_importance(   forecast_data = forecast_data,   oracle_output_data = target_data,   ensemble_fun = \"simple_ensemble\",   importance_algorithm = \"lomo\" )  model_importance_summary(   importance_scores = scores_lomo,   by = \"model_id\",   na_action = \"drop\",   fun = mean ) #> Overall model importance across tasks #> ----------------------------------------  #>            model_id importance_score_mean #> 1          PSI-DICE              37.16667 #> 2 Flusight-baseline              28.37500 #> 3   MOBS-GLEAM_FLUH             -75.00000 model_importance_summary(   importance_scores = scores_lomo,   by = \"model_id\",   na_action = \"worst\",   fun = mean ) #> Overall model importance across tasks #> ----------------------------------------  #>            model_id importance_score_mean #> 1 Flusight-baseline                28.375 #> 2          PSI-DICE               -17.625 #> 3   MOBS-GLEAM_FLUH               -61.125 model_importance_summary(   scores_lomo, by = \"model_id\", na_action = \"average\", fun = mean ) #> Overall model importance across tasks #> ----------------------------------------  #>            model_id importance_score_mean #> 1 Flusight-baseline                28.375 #> 2          PSI-DICE                27.875 #> 3   MOBS-GLEAM_FLUH               -56.250"},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance-article.html","id":"sec:example-lasomo","dir":"Articles","previous_headings":"Examples","what":"Evaluation using LASOMO algorithm","title":"`modelimportance`: Evaluating model importance within a multi-model ensemble in R","text":"Now demonstrate use LASOMO algorithm evaluation model importance. explored difference na_action options previous LOMO example (), focus options subset_wt, specifies weights assigned subsets models calculating importance scores, na_action fixed \"drop\". following code corresponding outputs illustrate evaluation using weighting scheme. example, three models (n=3n = 3), weights differ significantly two weighting schemes. Therefore, resulting outputs show little difference. However, general, larger number models, two weighting schemes may yield quite different importance scores model. section, explored component model‚Äôs contribution ensemble accuracy three models. extensive application complex scenarios larger number models can found Kim, Ray, Reich (2024).","code":"scores_lasomo_eq <- model_importance(   forecast_data = forecast_data,   oracle_output_data = target_data,   ensemble_fun = \"simple_ensemble\",   importance_algorithm = \"lasomo\",   subset_wt = \"equal\" ) model_importance_summary(   scores_lasomo_eq, by = \"model_id\", na_action = \"drop\", fun = mean ) #> Overall model importance across tasks #> ----------------------------------------  #>            model_id importance_score_mean #> 1          PSI-DICE              47.38889 #> 2 Flusight-baseline              24.29167 #> 3   MOBS-GLEAM_FLUH             -79.77778 scores_lasomo_perm <- model_importance(   forecast_data = forecast_data,   oracle_output_data = target_data,   ensemble_fun = \"simple_ensemble\",   importance_algorithm = \"lasomo\",   subset_wt = \"perm_based\" ) model_importance_summary(   scores_lasomo_perm, by = \"model_id\", na_action = \"drop\", fun = mean ) #> Overall model importance across tasks #> ----------------------------------------  #>            model_id importance_score_mean #> 1          PSI-DICE              44.83333 #> 2 Flusight-baseline              25.31250 #> 3   MOBS-GLEAM_FLUH             -78.58333"},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance-article.html","id":"sec:computational-complexity","dir":"Articles","previous_headings":"","what":"Computational complexity","title":"`modelimportance`: Evaluating model importance within a multi-model ensemble in R","text":"Content coming soon. section describe computational complexity LOMO LASOMO algorithms implemented package, depending numbers models tasks.","code":""},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance-article.html","id":"sec:discussion","dir":"Articles","previous_headings":"","what":"Summary and discussion","title":"`modelimportance`: Evaluating model importance within a multi-model ensemble in R","text":"Multi-model ensemble forecasts often provide better accuracy robustness single models, widely used decision-making policy planning across various domains. contribution component model accuracy ensemble depends unique characteristics. modelimportance package enables quantification value component model adds ensemble performance different evaluation contexts. primary function package model_importance(), returns data frame component models importance metrics. Users can choose various ensemble methods apply model importance algorithm LOMO LASOMO. Additionally, customizable options available handling missing values. features enable package serve versatile tool aid collaborative efforts construct effective ensemble model across wide range forecasting tasks. note unit testing continuous integration ensures reliability functions overall quality code across multiple platforms. room enhance current version package. Although package supports four different output types (‚Äòmean‚Äô, ‚Äòmedian‚Äô, ‚Äòquantile, ‚Äôpmf‚Äô), output types widely used practice. example, ‚Äòsample‚Äô output type commonly used US Flu Scenario Modeling Hub (Flu Scenario Modeling Hub 2024). format includes multiple simulated values (samples) forecast distribution. output_type_id specified sample, typically indexes samples indicates source, depending context. package can extended support output type, consideration future releases. extensions aim broaden scope applications real-world forecasting tasks.","code":""},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance-article.html","id":"acknowledgements","dir":"Articles","previous_headings":"","what":"Acknowledgements","title":"`modelimportance`: Evaluating model importance within a multi-model ensemble in R","text":"acknowledge Zhian N. Kamvar debugging resolving coding issues developing package. also grateful Mattew Cornell advice unit testing, greatly helped us improve structure testing code solid understanding unit testing practices.","code":""},{"path":[]},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance-article.html","id":"sec:appendix","dir":"Articles","previous_headings":"Appendix","what":"Weights for subsets in LASOMO","title":"`modelimportance`: Evaluating model importance within a multi-model ensemble in R","text":"LASOMO algorithm, two weighting schemes available subsets models calculation model importance scores: equal weights permutation-based weights. Let nn total number models kk size subset include model evaluated. formulas weights scheme follows: weq=12n‚àí1‚àí1,wperm=1(n‚àí1)(n‚àí1k),\\begin{align*} w^{\\text{eq}} &= \\frac{1}{2^{n-1}-1}, \\\\ w^{\\text{perm}} &= \\frac{1}{(n-1)\\binom{n-1}{k}}, \\end{align*} superscripts ‚Äúeq‚Äù ‚Äúperm‚Äù denote equal permutation-based weighting schemes, respectively. maximum weight permutation-based scheme occurs k=n‚àí1k=n-1, 1/(n‚àí1){1}/{(n-1)}. minimum weight occurs subset size around (n‚àí1)/2{(n-1)}/{2} (.e., k=‚åä(n‚àí1)/2‚åãk=\\lfloor (n-1)/2 \\rfloor), approximately œÄ(n‚àí1)/2(n‚àí1)2n‚àí1\\displaystyle\\frac{\\sqrt{\\pi(n-1)/2}}{(n-1)2^{n-1}} Stirling‚Äôs approximation. Given fixed mid-sized subset, nn increases, weight assigned subset equal weighting scheme decreases rate O(1/2n)O({1}/{2^n}), permutation-based scheme, decreases much faster rate O(1/(n2n))O({1}/({\\sqrt{n}\\,2^n})). indicates number models grows, mid-sized subset becomes significantly less influential determining model importance scores using permutation-based weighting scheme compared equal weighting scheme. hand, subsets extreme sizes (e.g., k=1k=1 k=n‚àí1k=n-1), weights permutation-based weighting scheme decrease O(1/n)O({1}/{n}), much slower equal weighting scheme. implies scenarios large number models, contributions extreme-sized subsets play relatively larger role calculation model importance scores using permutation-based weights compared equal weighting approach.","code":""},{"path":[]},{"path":"https://mkim425.github.io/modelimportance/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Minsu Kim. Author, maintainer. Li Shandross. Author, contributor. Zhian Kamvar. Contributor. Nicholas Reich. Author. Evan Ray. Author.","code":""},{"path":"https://mkim425.github.io/modelimportance/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Kim M, Shandross L, Reich N, Ray E (2026). modelimportance: Measuring Contributions Component Models Ensemble Forecast Accuracy. R package version 0.1.0, https://mkim425.github.io/modelimportance/.","code":"@Manual{,   title = {modelimportance: Measuring Contributions of Component Models to Ensemble Forecast Accuracy},   author = {Minsu Kim and Li Shandross and Nicholas Reich and Evan Ray},   year = {2026},   note = {R package version 0.1.0},   url = {https://mkim425.github.io/modelimportance/}, }"},{"path":[]},{"path":"https://mkim425.github.io/modelimportance/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Measuring Contributions of Component Models to Ensemble Forecast Accuracy","text":"goal modelimportance provide tools quantify individual model‚Äôs contribution ensemble model‚Äôs predictive performance. Importance scores ensemble member computed based impact ensemble‚Äôs accuracy, helping users understand models influential improving ensemble‚Äôs predictions. package designed work standard S3 class model output format defined hubverse convention.","code":""},{"path":"https://mkim425.github.io/modelimportance/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Measuring Contributions of Component Models to Ensemble Forecast Accuracy","text":"can install development version modelimportance GitHub :","code":"remotes::install_github(\"mkim425/modelimportance\")"},{"path":"https://mkim425.github.io/modelimportance/index.html","id":"getting-started","dir":"","previous_headings":"","what":"Getting started","title":"Measuring Contributions of Component Models to Ensemble Forecast Accuracy","text":"modelimportance provides two main functions: model_importance() calculates importance score model ensemble individual prediction tasks. model_importance_summary() summarizes importance scores across multiple tasks provide overall importance score model.","code":"library(modelimportance)"},{"path":"https://mkim425.github.io/modelimportance/index.html","id":"learn-more","dir":"","previous_headings":"","what":"Learn more","title":"Measuring Contributions of Component Models to Ensemble Forecast Accuracy","text":"Learn package use vignette. vignette provides detailed examples theoretical background algorithms implemented package.","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/model_importance.html","id":null,"dir":"Reference","previous_headings":"","what":"Quantifies the contribution of ensemble component models to ensemble prediction accuracy for each prediction task. ‚Äî model_importance","title":"Quantifies the contribution of ensemble component models to ensemble prediction accuracy for each prediction task. ‚Äî model_importance","text":"measure ensemble component model's contribution ensemble prediction accuracy model task. (See also model_importance_summary summary importance scores across multiple tasks.) function requires one column represent forecast date (date forecast originates made reference ) column named one forecast_date, origin_date, reference_date. output_type, corresponding scoring rule applied calculate importance follows.","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/model_importance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quantifies the contribution of ensemble component models to ensemble prediction accuracy for each prediction task. ‚Äî model_importance","text":"","code":"model_importance(   forecast_data,   oracle_output_data,   ensemble_fun = c(\"simple_ensemble\", \"linear_pool\"),   importance_algorithm = c(\"lomo\", \"lasomo\"),   subset_wt = c(\"equal\", \"perm_based\"),   min_log_score = -10,   ... )"},{"path":"https://mkim425.github.io/modelimportance/reference/model_importance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quantifies the contribution of ensemble component models to ensemble prediction accuracy for each prediction task. ‚Äî model_importance","text":"forecast_data data.frame predictions can coerced model_out_tbl format, standard S3 class model output format defined 'hubverse' convention (https://docs.hubverse.io/en/latest/#). fails coerced model_out_tbl format, error message returned. one output_type allowed data.frame, must one following: mean, median, quantile, pmf. oracle_output_data Ground truth data variables used define modeling targets. data must follow oracle output format. See 'Details'. ensemble_fun character string specifying ensemble method, either \"simple_ensemble\" \"linear_pool\"; c(\"simple_ensemble\", \"linear_pool\"). \"simple_ensemble\" specified, ensemble generated using optional agg_fun function ... (see 'Details'). \"linear_pool\" specified, ensemble model outputs created linear pool component model outputs. method supports output_type mean, quantile, pmf. importance_algorithm character string specifying algorithm model importance calculation; c(\"lomo\", \"lasomo\"). \"lomo\" stands leave-one-model-\"lasomo\" stands leave subsets models . \"lasomo\", 'furrr' 'future' packages need installed parallel execution. subset_wt character string specifying method assigning weight subsets using lasomo algorithm; c(\"equal\", \"perm_based\"). \"equal\" assigns equal weight subsets. \"perm_based\" assigns weight averaged possible permutations Shapley value. Ignored lomo method used. Default \"equal\", specified. min_log_score numeric value specifying minimum threshold log scores pmf output. threshold prevents issues extremely low probabilities assigned true outcome, otherwise lead undefined negative infinite log scores. probability lower threshold adjusted minimum value. default value set -10, following CDC FluSight thresholding convention. Users may choose different value based practical needs. ... Optional arguments passed ensemble_fun specified \"simple_ensemble\". See 'Details'.","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/model_importance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Quantifies the contribution of ensemble component models to ensemble prediction accuracy for each prediction task. ‚Äî model_importance","text":"model_imp_tbl S3 class object columns model_id, reference_date, output_type, importance, along task ID columns (e.g., location, horizon, target_end_date) present input forecast_data. Note reference_date used name forecast date column, regardless original name input forecast_data.","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/model_importance.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Quantifies the contribution of ensemble component models to ensemble prediction accuracy for each prediction task. ‚Äî model_importance","text":"oracle_output_data data frame contains ground truth values variables used define modeling targets. referred ‚Äúoracle‚Äù formatted oracle made perfect point prediction equal truth. data must follow oracle output format defined hubverse standard, includes independent task ID columns (e.g., location, target_date), output_type column specifying output type predictions oracle_value column observed values. forecast data, output_type either \"quantile\" \"pmf\", output_type_id column often required provide identifying information. model_out_tbl oracle_output_data must task ID columns output_type, including output_type_id necessary, used match predictions ground truth data. Additional argument ... agg_fun, character string name function specifying aggregation method component model outputs. Default mean, indicating equally weighted mean calculated across component model outputs unique output_type_id. can median custom function (e.g., geometric_mean. Details can found https://hubverse-org.github.io/hubEnsembles/articles/hubEnsembles.html). function uses furrr future parallelization. enable parallel execution, please set parallel backend, e.g., via future::plan().","code":""},{"path":[]},{"path":"https://mkim425.github.io/modelimportance/reference/model_importance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Quantifies the contribution of ensemble component models to ensemble prediction accuracy for each prediction task. ‚Äî model_importance","text":"","code":"if (FALSE) { # \\dontrun{ library(dplyr) forecast_data <- hubExamples::forecast_outputs |>   dplyr::filter(     output_type %in% c(\"quantile\"),     location == \"25\",     horizon == 1   ) target_data <- hubExamples::forecast_target_ts |>   dplyr::filter(     target_end_date %in% unique(forecast_data$target_end_date),     location == \"25\"   ) |>   # Rename columns to match the oracle output format   rename(     oracle_value = observation   ) # Example with the default arguments. model_importance(   forecast_data = forecast_data, oracle_output_data = target_data,   ensemble_fun = \"simple_ensemble\", importance_algorithm = \"lomo\",   subset_wt = \"equal\" ) # Example with the additional argument in `...`. model_importance(   forecast_data = forecast_data, oracle_output_data = target_data,   ensemble_fun = \"simple_ensemble\", importance_algorithm = \"lomo\",   subset_wt = \"equal\", agg_fun = median ) } # }"},{"path":"https://mkim425.github.io/modelimportance/reference/model_importance_summary.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize model importance scores produced by model_importance() across tasks ‚Äî model_importance_summary","title":"Summarize model importance scores produced by model_importance() across tasks ‚Äî model_importance_summary","text":"model_importance_summary summarizes model importance scores calculated individual prediction tasks model_importance summary statistic model. function handles NA values importance scores generated model contribute ensemble prediction given task missing forecast submission.","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/model_importance_summary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize model importance scores produced by model_importance() across tasks ‚Äî model_importance_summary","text":"","code":"model_importance_summary(   importance_scores,   by = \"model_id\",   na_action = c(\"drop\", \"worst\", \"average\"),   fun = mean,   ... )"},{"path":"https://mkim425.github.io/modelimportance/reference/model_importance_summary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize model importance scores produced by model_importance() across tasks ‚Äî model_importance_summary","text":"importance_scores data frame containing model importance scores individual prediction tasks, produced model_importance() function. data frame include columns model_id, reference_date, task ID columns (e.g., location, horizon, target_end_date), output_type, importance. character vector column names specifying grouping variable(s) summarization. Default \"model_id\", summarizes importance scores model across tasks. na_action character string specifying handle NA values generated importance score calculation task, occurring model contribute ensemble prediction given task missing forecast submission. Three options available: c(\"drop\", \"worst\", \"average\"). specific prediction task, option works follows: \"drop\" removes NAs. \"worst\" replaces NAs smallest value among importance metrics models. \"average\" replaces NAs average value models' importance metrics. fun function used summarize importance scores. Default mean() ... Additional arguments passed summary function fun. See documentation corresponding function details.","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/model_importance_summary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize model importance scores produced by model_importance() across tasks ‚Äî model_importance_summary","text":"importance_summary S3 class object columns model_id importance_score_<fun>, <fun> name summary function used (e.g., importance_score_mean fun = mean). output sorted descending order summary importance scores.","code":""},{"path":[]},{"path":"https://mkim425.github.io/modelimportance/reference/model_importance_summary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize model importance scores produced by model_importance() across tasks ‚Äî model_importance_summary","text":"","code":"if (FALSE) { # \\dontrun{ library(dplyr) forecast_data <- hubExamples::forecast_outputs |>   dplyr::filter(     output_type %in% c(\"quantile\"),     location == \"25\",     horizon == 1   ) target_data <- hubExamples::forecast_target_ts |>   dplyr::filter(     target_end_date %in% unique(forecast_data$target_end_date),     location == \"25\"   ) |>   # Rename columns to match the oracle output format   rename(     oracle_value = observation   ) # Example with the default arguments. importance_scores <- model_importance(   forecast_data = forecast_data, oracle_output_data = target_data,   ensemble_fun = \"simple_ensemble\", importance_algorithm = \"lomo\",   subset_wt = \"equal\" ) model_importance_summary(importance_scores, by = \"model_id\") } # }"},{"path":"https://mkim425.github.io/modelimportance/reference/plot.importance_summary.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot method for overall importance summary ‚Äî plot.importance_summary","title":"Plot method for overall importance summary ‚Äî plot.importance_summary","text":"Plot method overall importance summary","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/plot.importance_summary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot method for overall importance summary ‚Äî plot.importance_summary","text":"","code":"# S3 method for class 'importance_summary' plot(x, ...)"},{"path":"https://mkim425.github.io/modelimportance/reference/plot.importance_summary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot method for overall importance summary ‚Äî plot.importance_summary","text":"x object class importance_summary. ... Additional arguments passed plot method.","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/plot.model_imp_tbl.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot method for model importance score table ‚Äî plot.model_imp_tbl","title":"Plot method for model importance score table ‚Äî plot.model_imp_tbl","text":"Plot method model importance score table","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/plot.model_imp_tbl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot method for model importance score table ‚Äî plot.model_imp_tbl","text":"","code":"# S3 method for class 'model_imp_tbl' plot(x, ...)"},{"path":"https://mkim425.github.io/modelimportance/reference/plot.model_imp_tbl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot method for model importance score table ‚Äî plot.model_imp_tbl","text":"x object class model_imp_tbl. ... Additional arguments passed plot method.","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/print.importance_summary.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for overall importance summary ‚Äî print.importance_summary","title":"Print method for overall importance summary ‚Äî print.importance_summary","text":"Print method overall importance summary","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/print.importance_summary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for overall importance summary ‚Äî print.importance_summary","text":"","code":"# S3 method for class 'importance_summary' print(x, ...)"},{"path":"https://mkim425.github.io/modelimportance/reference/print.importance_summary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for overall importance summary ‚Äî print.importance_summary","text":"x object class importance_summary. ... Additional arguments passed print method.","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/print.model_imp_tbl.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for model importance score table ‚Äî print.model_imp_tbl","title":"Print method for model importance score table ‚Äî print.model_imp_tbl","text":"Print method model importance score table","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/print.model_imp_tbl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for model importance score table ‚Äî print.model_imp_tbl","text":"","code":"# S3 method for class 'model_imp_tbl' print(x, ...)"},{"path":"https://mkim425.github.io/modelimportance/reference/print.model_imp_tbl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for model importance score table ‚Äî print.model_imp_tbl","text":"x object class model_imp_tbl. ... Additional arguments passed print method.","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/print.summary.model_imp_tbl.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for summary of model importance score table ‚Äî print.summary.model_imp_tbl","title":"Print method for summary of model importance score table ‚Äî print.summary.model_imp_tbl","text":"Print method summary model importance score table","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/print.summary.model_imp_tbl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for summary of model importance score table ‚Äî print.summary.model_imp_tbl","text":"","code":"# S3 method for class 'summary.model_imp_tbl' print(x, ...)"},{"path":"https://mkim425.github.io/modelimportance/reference/print.summary.model_imp_tbl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for summary of model importance score table ‚Äî print.summary.model_imp_tbl","text":"x object class summary.model_imp_tbl. ... Additional arguments passed print method.","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/summary.importance_summary.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary method for overall importance summary ‚Äî summary.importance_summary","title":"Summary method for overall importance summary ‚Äî summary.importance_summary","text":"Summary method overall importance summary","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/summary.importance_summary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary method for overall importance summary ‚Äî summary.importance_summary","text":"","code":"# S3 method for class 'importance_summary' summary(object, ...)"},{"path":"https://mkim425.github.io/modelimportance/reference/summary.importance_summary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary method for overall importance summary ‚Äî summary.importance_summary","text":"object object class importance_summary. ... Additional arguments passed summary method.","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/summary.model_imp_tbl.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary method for model importance score table ‚Äî summary.model_imp_tbl","title":"Summary method for model importance score table ‚Äî summary.model_imp_tbl","text":"Summary method model importance score table","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/summary.model_imp_tbl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary method for model importance score table ‚Äî summary.model_imp_tbl","text":"","code":"# S3 method for class 'model_imp_tbl' summary(object, ...)"},{"path":"https://mkim425.github.io/modelimportance/reference/summary.model_imp_tbl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary method for model importance score table ‚Äî summary.model_imp_tbl","text":"object object class model_imp_tbl. ... Additional arguments passed summary method.","code":""}]
