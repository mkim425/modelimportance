[{"path":"https://mkim425.github.io/modelimportance/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 modelimportance authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"`modelimportance`: Assessing Model Contribution to Ensemble Accuracy","text":"Ensemble forecasts commonly used support decision-making policy planning across various fields often offer improved accuracy stability compared individual models. model unique characteristics, understanding measuring value constituent model adds overall accuracy ensemble can support construction effective ensembles. modelimportance package provides tools quantify component model contributes accuracy ensemble performance point probabilistic forecasts. supports multiple functionalities; allows users specify ensemble approach implement model importance metric use. Additionally, software offers customizable options handling missing values. features enable package serve versatile tool researchers practitioners. helps constructing effective ensemble model across wide range forecasting tasks, also understanding role model within ensemble gaining insights individual models . package follows ‘hubverse’ framework, collection open-source software tools developed promote collaborative modeling hub efforts simplify setup operation. Accordingly, depends several packages hubverse ecosystem, hubUtils, hubEnsembles, hubEvals, hubExamples. enables seamless integration flexibility forecasting tools systems, allowing many analyses performed existing ongoing hubs.","code":""},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance.html","id":"model-importance-metric-description","dir":"Articles","previous_headings":"","what":"Model importance metric description","title":"`modelimportance`: Assessing Model Contribution to Ensemble Accuracy","text":"modelimportance package provides two model importance metrics based algorithms : Leave-one-model-(LOMO) Leave--subsets--models-(LASOMO). basic idea measuring importance component model evaluate change ensemble performance model included excluded ensemble construction. specifically, compare performance ensemble without specific model specific task, consider difference performance importance model task. apply idea many tasks aggregate importance scores model across tasks using averages.","code":""},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance.html","id":"leave-one-model-out-lomo","dir":"Articles","previous_headings":"Model importance metric description","what":"Leave-one-model-out (LOMO)","title":"`modelimportance`: Assessing Model Contribution to Ensemble Accuracy","text":"LOMO involves creating ensemble excluding one component model entire set models. Let {} set nn models FiF^forecast produced model ii, =1,2,…,n.= 1,2, \\dots, n. ensemble excludes exactly one model including others. FA−^{{}^{-}} denotes ensemble forecast constructed using forecasts FAF^{}except FiF^. importance score using LOMO calculated difference accuracy, measured specific score, FA−^{{}^{-}} FAF^{}. example, evaluating model 1 within ensemble three models (n=3n=3), LOMO creates ensemble forecast F{2,3}F^{\\{2,3\\}} using F2F^2 F3F^3. performance reduced ensemble compared full ensemble forecast F{1,2,3}F^{\\{1,2,3\\}}, incorporates three models. note model can make ensemble better worse, thus importance score model 1 can positive negative accordingly.","code":""},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance.html","id":"leave-all-subsets-of-models-out-lasomo","dir":"Articles","previous_headings":"Model importance metric description","what":"Leave-all-subsets-of-models-out (LASOMO)","title":"`modelimportance`: Assessing Model Contribution to Ensemble Accuracy","text":"LASOMO involves ensemble constructions possible subsets models. subset SS contain model ii, S∪{}S \\cup \\{\\} plays role {} LOMO; score associated subset SS difference measures FSF^S FS∪{}F^{S \\cup \\{\\}}. , scores aggregated across possible subsets model ii belong . example, using earlier setup three forecast models, LASOMO considers three subsets, denote S1={2}S_1=\\{2\\}, S2={3}S_2=\\{3\\}, S3={2,3}S_3=\\{2, 3\\}, calculate importance score model 1 (excluding subsets include model 1). ensemble forecasts F{2},F{3}F^{\\{2\\}}, F^{\\{3\\}}, F{2,3}F^{\\{2,3\\}} compared F{1,2},F{1,3}F^{\\{1,2\\}}, F^{\\{1,3\\}}, F{1,2,3}F^{\\{1,2,3\\}}, respectively. performance differences attributable model 1’s inclusion aggregated, results importance score model 1. note subsets (e.g., S1,S2,S_1, S_2, S3S_3) may different weights aggregating process. modelimportance package offers two weighting options subsets: one assigns equal (uniform) weights subsets, assigns weights based size, similar concept Shapley values cooperative game theory, measure player’s average contribution possible coalitions (, equivalently, permutations players) (Shapley (1953)). formulas weights scheme follows: weq=12n−1−1,wperm=1(n−1)(n−1k),\\begin{align*} w^{\\text{eq}} &= \\frac{1}{2^{n-1}-1}, \\\\ w^{\\text{perm}} &= \\frac{1}{(n-1)\\binom{n-1}{k}}, \\end{align*} kk size subset, superscripts “eq” “perm” denote equal permutation-based weighting schemes, respectively. Users can choose one evaluate contribution model manner suited preferred framework.","code":""},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance.html","id":"comparison-of-weighting-schemes-in-lasomo","dir":"Articles","previous_headings":"Model importance metric description","what":"Comparison of weighting schemes in LASOMO","title":"`modelimportance`: Assessing Model Contribution to Ensemble Accuracy","text":"maximum weight permutation-based scheme occurs k=n−1k=n-1, 1/(n−1){1}/{(n-1)}. minimum weight occurs subset size around (n−1)/2{(n-1)}/{2} (.e., k=⌊(n−1)/2⌋k=\\lfloor (n-1)/2 \\rfloor), approximately π(n−1)/2(n−1)2n−1\\frac{\\sqrt{\\pi(n-1)/2}}{(n-1)2^{n-1}} Stirling’s approximation. Given fixed mid-sized subset, nn increases, weight assigned subset equal weighting scheme decreases rate O(1/2n)O({1}/{2^n}), permutation-based scheme, decreases much faster rate O(1/(n2n))O({1}/({\\sqrt{n}\\,2^n})). indicates number models grows, mid-sized subset becomes significantly less influential determining model importance scores using permutation-based weighting scheme compared equal weighting scheme. hand, subsets extreme sizes (e.g., k=1k=1 k=n−1k=n-1), weights permutation-based weighting scheme decrease O(1/n)O({1}/{n}), much slower equal weighting scheme. implies scenarios large number models, contributions extreme-sized subsets play relatively larger role calculation model importance scores using permutation-based weights compared equal weighting approach. Overall, difference two weighting schemes likely arise mainly extreme-sized subsets nn large. weights given mid-sized subsets become increasingly similar, small values order 10−310^{-3} even n=8n=8, weights assigned smallest largest subsets remain substantially different shown following figure. Figure: Comparison weights assigned subset. plot shows weights assigned subset number models nn increases 2 10. red line represents weights equal weighting scheme, blue green lines represent minimum maximum weights, respectively, permutation-based weighting scheme. minimum weight occurs subset size around (n−1)/2(n-1)/2, maximum weight occurs subset size n−1n-1. number models increases, weights assigned two schemes become increasingly similar mid-sized subsets whereas substantial differences remain extreme-sized subsets.","code":""},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance.html","id":"scoring-rules","dir":"Articles","previous_headings":"Model importance metric description","what":"Scoring rules","title":"`modelimportance`: Assessing Model Contribution to Ensemble Accuracy","text":"model’s prediction performance evaluated different scoring rules depending different output types. following table presents output types associated scoring rules supported modelimportance package.","code":""},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance.html","id":"main-functions","dir":"Articles","previous_headings":"","what":"Main functions","title":"`modelimportance`: Assessing Model Contribution to Ensemble Accuracy","text":"section, describe usage two main functions, multiple options available customize evaluation framework.","code":""},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance.html","id":"model_importance","dir":"Articles","previous_headings":"Main functions","what":"model_importance( )","title":"`modelimportance`: Assessing Model Contribution to Ensemble Accuracy","text":"model_importance() function calculates importance scores ensemble component models based contributions improving ensemble prediction accuracy prediction task. returns single data frame importance scores combined across tasks. model missed predictions specific task, NA value assigned task. forecast_data data frame containing predictions can coerced model_out_tbl format, standard S3 class model output format defined hubverse convention. fails coerced model_out_tbl format, error message returned hubUtils package, provides function as_model_out_tbl() purpose. Users may need manually transform data meet hubverse standards. oracle_output_data data frame contains ground truth values variables used define modeling targets. referred “oracle” formatted oracle made perfect point prediction equal truth. data must follow oracle output format defined hubverse standard, includes independent task ID columns (e.g., location, target_date), output_type column specifying output type predictions oracle_value column observed values. forecast data, output_type either \"quantile\" \"pmf\", output_type_id column often required provide identifying information. model_out_tbl oracle_output_data must task ID columns output_type, including output_type_id necessary, used match predictions ground truth data. ensemble_fun argument specifies ensemble method used evaluating model importance, relies implementations hubEnsembles package. currently supported methods \"simple_ensemble\" \"linear_pool\". \"simple_ensemble\" method returns average predicted values component models per prediction task defined task IDs, output_type, output_type_id columns. default aggregation function method \"mean\", can customized specifying additional arguments ..., agg_fun=\"median\". \"linear_pool\" specified, ensemble model outputs created linear pool component model outputs. method supports output_type \"mean\", \"quantile\", \"pmf\". importance_algorithm argument specifies algorithm model importance calculation, can either \"lomo\" (leave one model ) \"lasomo\" (leave subsets models ). subset_wt argument employed \"lasomo\" algorithm. argument two options: \"equal\" assigns equal weight subsets \"perm_based\" assigns weight averaged possible permutations formula Shapley values. default values importance_algorithm subset_wt \"lomo\" \"equal\", respectively. min_log_score argument relevant output_type \"pmf\", uses Log Score scoring rule. sets minimum threshold log scores avoid issues extremely low probabilities assigned true outcome, can lead undefined negative infinite log scores. probability lower threshold adjusted minimum value calculating importance metric based log score. default value set -10, arbitrary choice. Users may choose different value based practical needs.","code":"> model_importance(forecast_data, oracle_output_data, ensemble_fun,                      importance_algorithm, subset_wt, min_log_score,                      ...)"},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance.html","id":"model_importance_summary","dir":"Articles","previous_headings":"Main functions","what":"model_importance_summary( )","title":"`modelimportance`: Assessing Model Contribution to Ensemble Accuracy","text":"model_importance_summary() function summarizes importance scores produced model_importance() across tasks model. importance_scores data frame containing model importance scores individual prediction tasks, produced model_importance() function. argument specifies grouping variable(s) summarization. default \"model_id\" summarize importance scores model. Users can also specify columns present importance_scores data frame needed. na_action argument allows specifying handle NA values generated importance score calculation task, occurring model contribute ensemble prediction given task missing forecast submission. Three options available: \"worst\", \"average\", \"drop\". specific prediction task, model missing predictions, \"worst\" option replaces NA values smallest value among models’ importance metrics, \"average\" option replaces average models’ importance metrics task. \"drop\" option removes NA values, results exclusion model evaluation task. fun argument specifies function used summarize importance scores. fun = mean default choice, summary functions also applicable (e.g., fun = median). Additional arguments can passed summary function fun ... needed (e.g., fun = quantile, probs = 0.25 quartile summary).","code":"> model_importance_summary(importance_scores, by = \"model_id\",                            na_action = c(\"drop\", \"worst\", \"average\"),                            fun = mean, ...)"},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance.html","id":"examples","dir":"Articles","previous_headings":"","what":"Examples","title":"`modelimportance`: Assessing Model Contribution to Ensemble Accuracy","text":"illustrate implementation model_importance() function, using various combinations arguments. use example forecast target data hubExamples package, provides sample datasets multiple modeling hubs hubverse format.","code":""},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance.html","id":"setup","dir":"Articles","previous_headings":"Examples","what":"Setup","title":"`modelimportance`: Assessing Model Contribution to Ensemble Accuracy","text":"load necessary packages.","code":"library(hubExamples) library(modelimportance) library(dplyr) library(ggplot2)"},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance.html","id":"example-data","dir":"Articles","previous_headings":"Examples","what":"Example data","title":"`modelimportance`: Assessing Model Contribution to Ensemble Accuracy","text":"forecast data used contains forecasts weekly incident influenza hospitalizations US Massachusetts (FIPS code 25) Texas (FIPS code 48), generated November 19, 2022. forecasts two target end dates, November 26, 2022 (horizon 1), December 10, 2022 (horizon 3), produced three models: ‘Flusight-baseline’, ‘MOBS-GLEAM_FLUH’, ‘PSI-DICE’. output type median output_type_id column NAs specification required output type. modified example data slightly: forecasts removed demonstrate handling missing values. Therefore, MOBS-GLEAM_FLUH’s forecast Massachusetts November 26, 2022, PSI-DICE’s forecast Texas December 10, 2022, missing. corresponding target data contains observed hospitalization counts dates locations. visualize forecasts observed values.  Overall, forecasts tend larger prediction errors target end date December 10, 2022, compared November 26, 2022, expected due increased uncertainty longer horizons. Additionally, forecasts Massachusetts relatively accurate compared Texas, tend higher errors.","code":"forecast_data <- hubExamples::forecast_outputs |>   dplyr::filter(     output_type %in% c(\"median\"),     target_end_date %in% as.Date(c(\"2022-11-26\", \"2022-12-10\"))   ) |>   filter(     !(model_id == \"MOBS-GLEAM_FLUH\" & location == \"25\" &         target_end_date == as.Date(\"2022-11-26\")),     !(model_id == \"PSI-DICE\" & location == \"48\" &         target_end_date == as.Date(\"2022-12-10\"))   )  forecast_data #> # A tibble: 10 × 9 #>    model_id          reference_date target          horizon location target_end_date output_type output_type_id value #>    <chr>             <date>         <chr>             <int> <chr>    <date>          <chr>       <chr>          <dbl> #>  1 Flusight-baseline 2022-11-19     wk inc flu hosp       1 25       2022-11-26      median      NA                51 #>  2 Flusight-baseline 2022-11-19     wk inc flu hosp       3 25       2022-12-10      median      NA                51 #>  3 Flusight-baseline 2022-11-19     wk inc flu hosp       1 48       2022-11-26      median      NA              1052 #>  4 Flusight-baseline 2022-11-19     wk inc flu hosp       3 48       2022-12-10      median      NA              1052 #>  5 MOBS-GLEAM_FLUH   2022-11-19     wk inc flu hosp       3 25       2022-12-10      median      NA                43 #>  6 MOBS-GLEAM_FLUH   2022-11-19     wk inc flu hosp       1 48       2022-11-26      median      NA              1072 #>  7 MOBS-GLEAM_FLUH   2022-11-19     wk inc flu hosp       3 48       2022-12-10      median      NA               688 #>  8 PSI-DICE          2022-11-19     wk inc flu hosp       1 25       2022-11-26      median      NA                90 #>  9 PSI-DICE          2022-11-19     wk inc flu hosp       3 25       2022-12-10      median      NA               159 #> 10 PSI-DICE          2022-11-19     wk inc flu hosp       1 48       2022-11-26      median      NA              1226 target_data <- hubExamples::forecast_target_ts |>   dplyr::filter(     target_end_date %in% unique(forecast_data$target_end_date),     location %in% unique(forecast_data$location),     target == \"wk inc flu hosp\"   ) |>   # Rename columns to match the oracle output format   rename(oracle_value = observation)  target_data #> # A tibble: 4 × 4 #>   target_end_date target          location oracle_value #>   <date>          <chr>           <chr>           <dbl> #> 1 2022-11-26      wk inc flu hosp 25                221 #> 2 2022-11-26      wk inc flu hosp 48               1929 #> 3 2022-12-10      wk inc flu hosp 25                578 #> 4 2022-12-10      wk inc flu hosp 48               1781 forecast_data |>   ggplot(aes(x = target_end_date)) +   geom_point(aes(y = value, color = model_id), size = 2) +   facet_wrap(~location,     scales = \"free_y\",     labeller = labeller(location = function(x) paste0(\"Location: \", x))   ) +   geom_point(     data = target_data,     aes(y = oracle_value, group = 1, shape = \"Observed\"),     alpha = 1, size = 2   ) +   scale_x_date(     breaks = target_data$target_end_date,     date_labels = \"%Y-%m-%d\", expand = expansion(add = c(5, 5))   ) +   scale_color_manual(     name = \"model_id/Observed\",     values = c(       \"Flusight-baseline\" = \"#619CFF\",       \"MOBS-GLEAM_FLUH\" = \"#00BA38\", \"PSI-DICE\" = \"#F8766D\"     ),     limits = c(\"Flusight-baseline\", \"MOBS-GLEAM_FLUH\", \"PSI-DICE\")   ) +   scale_shape_manual(values = c(\"Observed\" = 1)) +   labs(     x = \"Date\", y = \"Weekly Hospitalization\",     title = \"Forecasts of incident deaths generated on November 19, 2022\"   )"},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance.html","id":"evaluation-using-lomo-algorithm","dir":"Articles","previous_headings":"Examples > Example data","what":"Evaluation using LOMO algorithm","title":"`modelimportance`: Assessing Model Contribution to Ensemble Accuracy","text":"quantify contribution model within ensemble using model_importance() function. following code evaluates importance ensemble member simple mean ensemble using LOMO algorithm. models missed forecasts certain tasks, NA values assigned importance column tasks. summarize importance scores model averaging across tasks. NA values removed averaging process setting na_action argument \"drop\". results show model ‘PSI-DICE’ highest importance score, followed ‘Flusight-baseline’ ‘MOBS-GLEAM_FLUH’. , ‘PSI-DICE’ contributes improving ensemble’s predictive performance, whereas ‘MOBS-GLEAM_FLUH’, negative score, detracts ensemble’s performance. low importance score ‘MOBS-GLEAM_FLUH’ mainly due substantially larger prediction error Texas target end date December 10, 2022, compared models, missing forecast Massachusetts November 26, 2022, factored evaluation. single large error significantly affected contribution score. Another approach handling NA values use \"worst\" option na_action, replaces NA values worst (.e., minimum) score among models task. results show importance scores ‘Flusight-baseline’ unchanged missing forecast. observe importance score ‘PSI-DICE’, previously positive, now decreased negative value compared evaluation using \"drop\" option na_action. Moreover, ‘MOBS-GLEAM_FLUH’ still ranks lowest, importance score increased. change related varying forecast accuracy across different tasks. target end date November 26, 2022, Massachusetts, forecasts relatively accurate. Thus, even ‘MOBS-GLEAM_FLUH’ assigned worst value importance score missing forecast, including value averaging detrimental overall importance metric; rather, beneficial excluding . contrast, target end date December 10, 2022, Texas, forecasts much larger errors across board, assigning worst value importance score missing forecast ‘PSI-DICE’ task detrimental effect averaging importance scores. scale importance scores influenced magnitude prediction errors: tasks small errors, scores remain moderate, tasks large errors can yield importance scores much greater magnitude. also possible impute missing scores intermediate values assigning average importance scores models task. strategy may offer balanced trade-mitigating influence missing data without overly penalizing overlooking .","code":"scores_lomo <- model_importance(   forecast_data = forecast_data, oracle_output_data = target_data,   ensemble_fun = \"simple_ensemble\",   importance_algorithm = \"lomo\" ) scores_lomo #>             model_id reference_date          target horizon location target_end_date output_type importance #> 1  Flusight-baseline     2022-11-19 wk inc flu hosp       1       25      2022-11-26      median  -19.50000 #> 2    MOBS-GLEAM_FLUH     2022-11-19 wk inc flu hosp       1       25      2022-11-26      median         NA #> 3           PSI-DICE     2022-11-19 wk inc flu hosp       1       25      2022-11-26      median   19.50000 #> 4  Flusight-baseline     2022-11-19 wk inc flu hosp       1       48      2022-11-26      median  -32.33333 #> 5    MOBS-GLEAM_FLUH     2022-11-19 wk inc flu hosp       1       48      2022-11-26      median  -22.33333 #> 6           PSI-DICE     2022-11-19 wk inc flu hosp       1       48      2022-11-26      median   54.66667 #> 7  Flusight-baseline     2022-11-19 wk inc flu hosp       3       25      2022-12-10      median  -16.66667 #> 8    MOBS-GLEAM_FLUH     2022-11-19 wk inc flu hosp       3       25      2022-12-10      median  -20.66667 #> 9           PSI-DICE     2022-11-19 wk inc flu hosp       3       25      2022-12-10      median   37.33333 #> 10 Flusight-baseline     2022-11-19 wk inc flu hosp       3       48      2022-12-10      median  182.00000 #> 11   MOBS-GLEAM_FLUH     2022-11-19 wk inc flu hosp       3       48      2022-12-10      median -182.00000 #> 12          PSI-DICE     2022-11-19 wk inc flu hosp       3       48      2022-12-10      median         NA model_importance_summary(   scores_lomo, by = \"model_id\", na_action = \"drop\", fun = mean ) #> # A tibble: 3 × 2 #>   model_id          importance_score_mean #>   <chr>                             <dbl> #> 1 PSI-DICE                           37.2 #> 2 Flusight-baseline                  28.4 #> 3 MOBS-GLEAM_FLUH                   -75 model_importance_summary(   scores_lomo, by = \"model_id\", na_action = \"worst\", fun = mean ) #> # A tibble: 3 × 2 #>   model_id          importance_score_mean #>   <chr>                             <dbl> #> 1 Flusight-baseline                  28.4 #> 2 PSI-DICE                          -17.6 #> 3 MOBS-GLEAM_FLUH                   -61.1 model_importance_summary(   scores_lomo, by = \"model_id\", na_action = \"average\", fun = mean ) #> # A tibble: 3 × 2 #>   model_id          importance_score_mean #>   <chr>                             <dbl> #> 1 Flusight-baseline                  28.4 #> 2 PSI-DICE                           27.9 #> 3 MOBS-GLEAM_FLUH                   -56.2"},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance.html","id":"evaluation-using-lasomo-algorithm","dir":"Articles","previous_headings":"Examples > Example data","what":"Evaluation using LASOMO algorithm","title":"`modelimportance`: Assessing Model Contribution to Ensemble Accuracy","text":"Now demonstrate use LASOMO algorithm evaluation model importance. explored difference na_action options previous LOMO example, focus options subset_wt, specifies weights assigned subsets models calculating importance scores, na_action fixed \"drop\". following code corresponding outputs illustrate evaluation using weighting scheme. example, three models (n=3n = 3), weights differ significantly two weighting schemes. Therefore, resulting outputs show little difference. However, general, larger number models, two weighting schemes may yield different importance scores model. extensive application complex scenarios larger number models can found Kim et al. (2025).","code":"# LASOMO - equal weights scores_lasomo_eq <- model_importance(   forecast_data = forecast_data, oracle_output_data = target_data,   ensemble_fun = \"simple_ensemble\",   importance_algorithm = \"lasomo\", subset_wt = \"equal\" ) model_importance_summary(   scores_lasomo_eq, by = \"model_id\", na_action = \"drop\", fun = mean ) #> # A tibble: 3 × 2 #>   model_id          importance_score_mean #>   <chr>                             <dbl> #> 1 PSI-DICE                           47.4 #> 2 Flusight-baseline                  24.3 #> 3 MOBS-GLEAM_FLUH                   -79.8 # LASOMO - perm based weights scores_lasomo_perm <- model_importance(   forecast_data = forecast_data,   oracle_output_data = target_data,   ensemble_fun = \"simple_ensemble\",   importance_algorithm = \"lasomo\", subset_wt = \"perm_based\" ) model_importance_summary(   scores_lasomo_perm, by = \"model_id\", na_action = \"drop\", fun = mean ) #> # A tibble: 3 × 2 #>   model_id          importance_score_mean #>   <chr>                             <dbl> #> 1 PSI-DICE                           44.8 #> 2 Flusight-baseline                  25.3 #> 3 MOBS-GLEAM_FLUH                   -78.6"},{"path":"https://mkim425.github.io/modelimportance/articles/modelimportance.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"`modelimportance`: Assessing Model Contribution to Ensemble Accuracy","text":"Shapley, Lloyd S. value n-person games. Contribution Theory Games 2 (1953). Kim, Minsu, Ray, Evan L, Reich, Nicholas G. Beyond forecast leaderboards: Measuring individual model importance based contribution ensemble accuracy. arXiv preprint arXiv:2412.08916 (2024).","code":""},{"path":"https://mkim425.github.io/modelimportance/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Minsu Kim. Author, maintainer. Li Shandross. Author, contributor. Zhian Kamvar. Contributor. Nicholas Reich. Author. Evan Ray. Author.","code":""},{"path":"https://mkim425.github.io/modelimportance/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Kim M, Shandross L, Reich N, Ray E (2026). modelimportance: Measuring Contributions Component Models Ensemble Forecast Accuracy. R package version 0.1.0, https://mkim425.github.io/modelimportance/.","code":"@Manual{,   title = {modelimportance: Measuring Contributions of Component Models to Ensemble Forecast Accuracy},   author = {Minsu Kim and Li Shandross and Nicholas Reich and Evan Ray},   year = {2026},   note = {R package version 0.1.0},   url = {https://mkim425.github.io/modelimportance/}, }"},{"path":"https://mkim425.github.io/modelimportance/index.html","id":"modelimportance","dir":"","previous_headings":"","what":"Measuring Contributions of Component Models to Ensemble Forecast Accuracy","title":"Measuring Contributions of Component Models to Ensemble Forecast Accuracy","text":"goal modelimportance …","code":""},{"path":"https://mkim425.github.io/modelimportance/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Measuring Contributions of Component Models to Ensemble Forecast Accuracy","text":"can install development version modelimportance GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"mkim425/modelimportance\")"},{"path":"https://mkim425.github.io/modelimportance/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Measuring Contributions of Component Models to Ensemble Forecast Accuracy","text":"basic example shows solve common problem:","code":"library(modelimportance) ## basic example code"},{"path":"https://mkim425.github.io/modelimportance/reference/compute_importance.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate model importance for a single task — compute_importance","title":"Calculate model importance for a single task — compute_importance","text":"Evaluate importance ensemble component models quantifying contribution prediction accuracy ensemble combination model task.","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/compute_importance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate model importance for a single task — compute_importance","text":"","code":"compute_importance(   single_task_data,   oracle_output_data,   model_id_list,   ensemble_fun,   importance_algorithm,   subset_wt,   metric,   min_log_score,   ... )"},{"path":"https://mkim425.github.io/modelimportance/reference/compute_importance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate model importance for a single task — compute_importance","text":"single_task_data data.frame predictions single forecast task model_out_tbl format. data must contain one output_type, must one following: mean, median, quantile, pmf. oracle_output_data Ground truth data variables used define modeling targets. data must follow oracle output format. See 'Details'. model_id_list list component model IDs used ensemble. model present single_task_data, means model submit predictions given task. list used identify missing models ensemble. ensemble_fun character string specifying ensemble method, either \"simple_ensemble\" \"linear_pool\"; c(\"simple_ensemble\", \"linear_pool\"). \"simple_ensemble\" specified, ensemble generated using optional agg_fun function ... (see 'Details'). \"linear_pool\" specified, ensemble model outputs created linear pool component model outputs. method supports output_type mean, quantile, pmf. importance_algorithm character string specifying algorithm model importance calculation; c(\"lomo\", \"lasomo\"). \"lomo\" stands leave-one-model-\"lasomo\" stands leave subsets models . \"lasomo\", 'furrr' 'future' packages need installed parallel execution. subset_wt character string specifying method assigning weight subsets using lasomo algorithm; c(\"equal\", \"perm_based\"). \"equal\" assigns equal weight subsets. \"perm_based\" assigns weight averaged possible permutations Shapley value. Ignored lomo method used. Default \"equal\", specified. metric character string specifying metric used scoring model output. metric determined output_type must one following: se_point, ae_point, wis, log_score. Note mean output type, se_point used default, convert rse_point ensure consistency units importance score. min_log_score numeric value specifying minimum threshold log scores pmf output avoid issues extremely low probabilities assigned true outcome, can lead undefined negative infinite log scores. probability lower threshold adjusted minimum value. default value set -10, arbitrary choice. Users may choose different value based practical needs. ... Optional arguments passed ensemble_fun specified \"simple_ensemble\". See 'Details'.","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/compute_importance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate model importance for a single task — compute_importance","text":"data.frame columns task_id, output_type, model_id, (task level) importance.","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/compute_importance.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate model importance for a single task — compute_importance","text":"oracle_output_data data frame contains ground truth values variables used define modeling targets. referred “oracle” formatted oracle made perfect point prediction equal truth. data must follow oracle output format defined hubverse standard, includes independent task ID columns (e.g., location, target_date), output_type column specifying output type predictions oracle_value column observed values. forecast data, output_type either \"quantile\" \"pmf\", output_type_id column often required provide identifying information. model_out_tbl oracle_output_data must task ID columns output_type, including output_type_id necessary, used match predictions ground truth data. Additional argument ... agg_fun, character string name function specifying aggregation method component model outputs. Default mean, indicating equally weighted mean calculated across component model outputs unique output_type_id. can median custom function (e.g., geometric_mean. Details can found https://hubverse-org.github.io/hubEnsembles/articles/hubEnsembles.html). function uses furrr future parallelization. enable parallel execution, please set parallel backend, e.g., via future::plan().","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/model_importance.html","id":null,"dir":"Reference","previous_headings":"","what":"Quantifies the contribution of ensemble component models to ensemble prediction accuracy for each prediction task. — model_importance","title":"Quantifies the contribution of ensemble component models to ensemble prediction accuracy for each prediction task. — model_importance","text":"measure ensemble component model's contribution ensemble prediction accuracy model task. function requires one column represent forecast date (date forecast originates made reference ) column named one forecast_date, origin_date, reference_date. output_type, corresponding scoring rule applied calculate importance follows.","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/model_importance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quantifies the contribution of ensemble component models to ensemble prediction accuracy for each prediction task. — model_importance","text":"","code":"model_importance(   forecast_data,   oracle_output_data,   ensemble_fun = c(\"simple_ensemble\", \"linear_pool\"),   importance_algorithm = c(\"lomo\", \"lasomo\"),   subset_wt = c(\"equal\", \"perm_based\"),   min_log_score = -10,   ... )"},{"path":"https://mkim425.github.io/modelimportance/reference/model_importance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quantifies the contribution of ensemble component models to ensemble prediction accuracy for each prediction task. — model_importance","text":"forecast_data data.frame predictions can coerced model_out_tbl format, standard S3 class model output format defined 'hubverse' convention (https://docs.hubverse.io/en/latest/#). fails coerced model_out_tbl format, error message returned. one output_type allowed data.frame, must one following: mean, median, quantile, pmf. oracle_output_data Ground truth data variables used define modeling targets. data must follow oracle output format. See 'Details'. ensemble_fun character string specifying ensemble method, either \"simple_ensemble\" \"linear_pool\"; c(\"simple_ensemble\", \"linear_pool\"). \"simple_ensemble\" specified, ensemble generated using optional agg_fun function ... (see 'Details'). \"linear_pool\" specified, ensemble model outputs created linear pool component model outputs. method supports output_type mean, quantile, pmf. importance_algorithm character string specifying algorithm model importance calculation; c(\"lomo\", \"lasomo\"). \"lomo\" stands leave-one-model-\"lasomo\" stands leave subsets models . \"lasomo\", 'furrr' 'future' packages need installed parallel execution. subset_wt character string specifying method assigning weight subsets using lasomo algorithm; c(\"equal\", \"perm_based\"). \"equal\" assigns equal weight subsets. \"perm_based\" assigns weight averaged possible permutations Shapley value. Ignored lomo method used. Default \"equal\", specified. min_log_score numeric value specifying minimum threshold log scores pmf output avoid issues extremely low probabilities assigned true outcome, can lead undefined negative infinite log scores. probability lower threshold adjusted minimum value. default value set -10, arbitrary choice. Users may choose different value based practical needs. ... Optional arguments passed ensemble_fun specified \"simple_ensemble\". See 'Details'.","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/model_importance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Quantifies the contribution of ensemble component models to ensemble prediction accuracy for each prediction task. — model_importance","text":"data.frame columns model_id, reference_date, output_type, importance, along task ID columns (e.g., location, horizon, target_end_date) present input forecast_data. Note reference_date used name forecast date column, regardless original name input forecast_data.","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/model_importance.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Quantifies the contribution of ensemble component models to ensemble prediction accuracy for each prediction task. — model_importance","text":"oracle_output_data data frame contains ground truth values variables used define modeling targets. referred “oracle” formatted oracle made perfect point prediction equal truth. data must follow oracle output format defined hubverse standard, includes independent task ID columns (e.g., location, target_date), output_type column specifying output type predictions oracle_value column observed values. forecast data, output_type either \"quantile\" \"pmf\", output_type_id column often required provide identifying information. model_out_tbl oracle_output_data must task ID columns output_type, including output_type_id necessary, used match predictions ground truth data. Additional argument ... agg_fun, character string name function specifying aggregation method component model outputs. Default mean, indicating equally weighted mean calculated across component model outputs unique output_type_id. can median custom function (e.g., geometric_mean. Details can found https://hubverse-org.github.io/hubEnsembles/articles/hubEnsembles.html). function uses furrr future parallelization. enable parallel execution, please set parallel backend, e.g., via future::plan().","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/model_importance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Quantifies the contribution of ensemble component models to ensemble prediction accuracy for each prediction task. — model_importance","text":"","code":"if (FALSE) { # \\dontrun{ library(dplyr) library(hubExamples) forecast_data <- hubExamples::forecast_outputs |>   dplyr::filter(     output_type %in% c(\"quantile\"),     location == \"25\",     horizon == 1   ) target_data <- hubExamples::forecast_target_ts |>   dplyr::filter(     date %in% unique(forecast_data$target_end_date),     location == \"25\"   ) |>   # Rename columns to match the oracle output format   rename(     target_end_date = date,     oracle_value = observation   ) # Example with the default arguments. model_importance(   forecast_data = forecast_data, oracle_output_data = target_data,   ensemble_fun = \"simple_ensemble\", importance_algorithm = \"lomo\",   subset_wt = \"equal\" ) # Example with the additional argument in `...`. model_importance(   forecast_data = forecast_data, oracle_output_data = target_data,   ensemble_fun = \"simple_ensemble\", importance_algorithm = \"lomo\",   subset_wt = \"equal\", agg_fun = median ) } # }"},{"path":"https://mkim425.github.io/modelimportance/reference/model_importance_summary.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize model importance scores produced by model_importance() across tasks — model_importance_summary","title":"Summarize model importance scores produced by model_importance() across tasks — model_importance_summary","text":"model_importance_summary summarizes model importance scores calculated individual prediction tasks summary statistic model. function handles NA values importance scores generated model contribute ensemble prediction given task missing forecast submission.","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/model_importance_summary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize model importance scores produced by model_importance() across tasks — model_importance_summary","text":"","code":"model_importance_summary(   importance_scores,   by = \"model_id\",   na_action = c(\"drop\", \"worst\", \"average\"),   fun = mean,   ... )"},{"path":"https://mkim425.github.io/modelimportance/reference/model_importance_summary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize model importance scores produced by model_importance() across tasks — model_importance_summary","text":"importance_scores data frame containing model importance scores individual prediction tasks, produced model_importance() function. data frame include columns model_id, reference_date, task ID columns (e.g., location, horizon, target_end_date), output_type, importance. character vector column names specifying grouping variable(s) summarization. Default \"model_id\", summarizes importance scores model across tasks. na_action character string specifying handle NA values generated importance score calculation task, occurring model contribute ensemble prediction given task missing forecast submission. Three options available: c(\"drop\", \"worst\", \"average\"). specific prediction task, option works follows: \"drop\" removes NAs. \"worst\" replaces NAs smallest value among importance metrics models. \"average\" replaces NAs average value models' importance metrics. fun function used summarize importance scores. Default mean() ... Additional arguments passed summary function fun. See documentation corresponding function details.","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/model_importance_summary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize model importance scores produced by model_importance() across tasks — model_importance_summary","text":"data.frame columns model_id importance_score_<fun>, <fun> name summary function used (e.g., importance_score_mean fun = mean). output sorted descending order summary importance scores.","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/model_importance_summary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize model importance scores produced by model_importance() across tasks — model_importance_summary","text":"","code":"if (FALSE) { # \\dontrun{ library(dplyr) library(hubExamples) forecast_data <- hubExamples::forecast_outputs |>   dplyr::filter(     output_type %in% c(\"quantile\"),     location == \"25\",     horizon == 1   ) target_data <- hubExamples::forecast_target_ts |>   dplyr::filter(     date %in% unique(forecast_data$target_end_date),     location == \"25\"   ) |>   # Rename columns to match the oracle output format   rename(     target_end_date = date,     oracle_value = observation   ) # Example with the default arguments. importance_scores <- model_importance(   forecast_data = forecast_data, oracle_output_data = target_data,   ensemble_fun = \"simple_ensemble\", importance_algorithm = \"lomo\",   subset_wt = \"equal\" ) model_importance_summary(importance_scores, by = \"model_id\") } # }"},{"path":"https://mkim425.github.io/modelimportance/reference/split_data_by_task.html","id":null,"dir":"Reference","previous_headings":"","what":"Split the input data by a single task and make a list of data sets — split_data_by_task","title":"Split the input data by a single task and make a list of data sets — split_data_by_task","text":"function splits input data single task combination task IDs returns list data sets, corresponding single task.","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/split_data_by_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split the input data by a single task and make a list of data sets — split_data_by_task","text":"","code":"split_data_by_task(valid_tbl)"},{"path":"https://mkim425.github.io/modelimportance/reference/split_data_by_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split the input data by a single task and make a list of data sets — split_data_by_task","text":"valid_tbl data.frame containing forecast data target data, processed function validate_input_data().","code":""},{"path":"https://mkim425.github.io/modelimportance/reference/split_data_by_task.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Split the input data by a single task and make a list of data sets — split_data_by_task","text":"list data sets, corresponding single task.","code":""}]
