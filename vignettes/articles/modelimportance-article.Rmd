---
title: "modelimportance-paper"
bibliography: references.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(modelimportance)
```

Ensemble forecasting is a method to produce a single, consolidated prediction by combining forecasts generated from different models. While each model's strengths are pronounced, its weaknesses are counterbalanced, which leads to an ensemble forecast that is more robust and accurate [@gneiting2005weather; @hastie01statisticallearning].
Specifically, ensembles effectively mitigate the bias and variance arising from the predictions of individual models by averaging them out, and aggregating in this way can reduce prediction errors and improve overall performance. 
Enhanced prediction accuracy and robustness enable the achievement of more reliable predictions, thereby improving decision-making. For this reason, ensemble forecasting is widely used across various domains such as weather forecasting [@Guerra_2020; @gneiting2005weather], financial modeling [@SUN2020101160; @math11041054], and infectious disease outbreak forecasting [@ray_prediction_2018; @reich_accuracy_2019; @lutz_applying_2019; @viboud_rapidd_2018]. For example, throughout the COVID-19 pandemic, the US COVID-19 Forecast Hub collected individual models developed by over 90 different research groups and built a probabilistic ensemble forecasting model for COVID-19 cases, hospitalizations, and deaths in the US based on those modelsâ€™ predictions, which served as the official short-term forecasts for the US Centers for Disease Control and Prevention (CDC) [@cramer2022united].

The quality of forecasts is assessed by evaluating their error, bias, sharpness, and/or calibration using different scoring metrics.
The selection of the scoring metrics depends on the type of forecast: 
point forecasts (e.g., mean, median) and probabilistic forecasts (e.g., quantiles, samples, predictive cumulative distribution functions, probability mass function).
Commonly used assessment tools for point forecasts are the mean absolute error (MAE) and the mean squared error (MSE), which calculate the average magnitude of forecast errors. 
Scoring metrics for probabilistic forecasts consider the uncertainty and variability in predictions and provide concise evaluations through numerical scores [@gneiting_strictly_2007].  Some examples include the weighted interval score (WIS) for the quantile-based forecasts and the continuous ranked probability score (CRPS) for the forecasts taking the form of predictive cumulative distribution functions [@bracher_evaluating_2021]. We note that CRPS is a general scoring rule that can be computed either analytically in closed form or numerically from samples, and WIS is a quantile-based approximation of CRPS.
