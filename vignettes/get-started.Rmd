---
title: "Simple working examples"
output: rmarkdown::html_vignette
bibliography: references.bib
vignette: >
  %\VignetteIndexEntry{Simple working examples}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 4,
  warning = FALSE,
  message = FALSE
)
options(width = 200)
library(dplyr)
library(ggplot2)
library(kableExtra)
```


This vignette demonstrates the usage of the `modelimportance` package for 
evaluating how each component model contributes to ensemble accuracy.
We provide simple working examples to help you get started with the package. 
Detailed descriptions of the model importance metrics, algorithms, key 
functions, and in-depth interpretations of the examples covered here are 
available in the accompanying article titled '`modelimportance`: Evaluating 
model importance within a multi-model ensemble in R' under *Articles*.


## Setup

We first load the necessary packages.
```{r setup}
library(hubExamples)
library(modelimportance)
library(dplyr)
library(ggplot2)
```


We use some example forecast and target data from the `hubExamples` package, 
which provides sample datasets for multiple modeling hubs in the hubverse format. 


## Example data

The forecast data used here contains forecasts of weekly incident influenza 
hospitalizations in the US for Massachusetts (FIPS code 25) and Texas (FIPS code
48), generated on November 19, 2022. 
These forecasts are for two target end dates, November 26, 2022 (horizon 1), and
December 10, 2022 (horizon 3), and were produced by three models: 
'Flusight-baseline', 'MOBS-GLEAM_FLUH', and 'PSI-DICE'. 
The output type is `median` and the `output_type_id` column has `NA`s as no 
further specification is required for this output type. 
We have modified the example data slightly: some forecasts have been removed to 
demonstrate the handling of missing values. 
Therefore, MOBS-GLEAM_FLUH's forecast for Massachusetts on November 26, 2022, 
and PSI-DICE's forecast for Texas on December 10, 2022, are missing.

```{r forecast_data}
forecast_data <- hubExamples::forecast_outputs |>
  dplyr::filter(
    output_type %in% c("median"),
    target_end_date %in% as.Date(c("2022-11-26", "2022-12-10"))
  ) |>
  filter(
    !(model_id == "MOBS-GLEAM_FLUH" & location == "25" &
        target_end_date == as.Date("2022-11-26")),
    !(model_id == "PSI-DICE" & location == "48" &
        target_end_date == as.Date("2022-12-10"))
  )

forecast_data
```

The corresponding target data contains the observed hospitalization counts for 
these dates and locations.

```{r target_data}
target_data <- hubExamples::forecast_target_ts |>
  dplyr::filter(
    target_end_date %in% unique(forecast_data$target_end_date),
    location %in% unique(forecast_data$location),
    target == "wk inc flu hosp"
  ) |>
  # Rename columns to match the oracle output format
  rename(oracle_value = observation)

target_data
```

We visualize the forecasts and the observed values.

```{r fig-example-median-lomo}
forecast_data |>
  ggplot(aes(x = target_end_date)) +
  geom_point(aes(y = value, color = model_id), size = 2) +
  facet_wrap(~location,
    scales = "free_y",
    labeller = labeller(location = function(x) paste0("Location: ", x))
  ) +
  geom_point(
    data = target_data,
    aes(y = oracle_value, group = 1, shape = "Observed"),
    alpha = 1, size = 2
  ) +
  scale_x_date(
    breaks = target_data$target_end_date,
    date_labels = "%Y-%m-%d", expand = expansion(add = c(5, 5))
  ) +
  scale_color_manual(
    name = "model_id/Observed",
    values = c(
      "Flusight-baseline" = "#619CFF",
      "MOBS-GLEAM_FLUH" = "#00BA38", "PSI-DICE" = "#F8766D"
    ),
    limits = c("Flusight-baseline", "MOBS-GLEAM_FLUH", "PSI-DICE")
  ) +
  scale_shape_manual(values = c("Observed" = 1)) +
  labs(
    x = "Date", y = "Weekly Hospitalization",
    title = "Forecasts of incident deaths generated on November 19, 2022"
  )
```

As expected, prediction errors increase at longer horizons due to greater uncertainty, with forecasts for December 10, 2022, showing larger deviations from the observed values compared to those for November 26, 2022.
Additionally, the forecasts for Massachusetts are relatively more accurate compared to those for Texas, which 
tend to have higher errors.


## Evaluation using LOMO algorithm

We quantify the contribution of each model within the ensemble using the 
`model_importance()` function. 
The following code evaluates the importance of each ensemble member in the 
simple mean ensemble using the LOMO algorithm. 

```{r lomo}
scores_lomo <- model_importance(
  forecast_data = forecast_data, oracle_output_data = target_data,
  ensemble_fun = "simple_ensemble",
  importance_algorithm = "lomo"
)
scores_lomo
```

For models that missed forecasts for certain tasks, `NA` values are assigned in 
the importance column for those tasks.

We summarize the importance scores for each model by averaging across all tasks.
`NA` values are removed during the averaging process by setting the `na_action`
argument to `"drop"`.

```{r lomo-NAdrop}
model_importance_summary(
  scores_lomo, by = "model_id", na_action = "drop", fun = mean
)
```
The results show that the model 'PSI-DICE' has the highest importance score, 
followed by 'Flusight-baseline' and 'MOBS-GLEAM_FLUH'.
That is, 'PSI-DICE' contributes the most to improving the ensemble's predictive 
performance, whereas 'MOBS-GLEAM_FLUH', which has a negative score, detracts 
from the ensembleâ€™s performance.

Another approach to handling `NA` values is to use the `"worst"` option for 
`na_action`, which replaces `NA` values with the worst (i.e., minimum) score 
among the other models for the same task.

```{r lomo-NAworst}
model_importance_summary(
  scores_lomo, by = "model_id", na_action = "worst", fun = mean
)
```


The results show that the importance scores of 'Flusight-baseline' is unchanged 
because it has no missing forecast. 
We observe that the importance score of 'PSI-DICE', which was previously 
positive, has now decreased to a negative value when compared to the evaluation 
using the `"drop"` option for `na_action`. Moreover, 'MOBS-GLEAM_FLUH' still 
ranks the lowest, but the importance score has increased. 


It is also possible to impute the missing scores with intermediate values by 
assigning the average importance scores of other models in the same task. 
This strategy may offer a more balanced trade-off by mitigating the influence of
the missing data without overly penalizing or overlooking them. 

```{r lomo-NAaverage}
model_importance_summary(
  scores_lomo, by = "model_id", na_action = "average", fun = mean
)
```

## Evaluation using LASOMO algorithm

Now we demonstrate the use of the LASOMO algorithm in the evaluation of model 
importance. As we explored the difference of `na_action` options in the previous
LOMO example, we focus on options for `subset_wt`, which specifies how weights 
are assigned to subsets of models when calculating importance scores, with 
`na_action` fixed to `"drop"`.

The following code and corresponding outputs illustrate the evaluation using 
each weighting scheme.

```{r lasomo-NAdrop-equal}
#| message: false
# LASOMO - equal weights
scores_lasomo_eq <- model_importance(
  forecast_data = forecast_data, oracle_output_data = target_data,
  ensemble_fun = "simple_ensemble",
  importance_algorithm = "lasomo", subset_wt = "equal"
)
model_importance_summary(
  scores_lasomo_eq, by = "model_id", na_action = "drop", fun = mean
)
```

```{r lasomo-NAdrop-perm}
#| message: false
# LASOMO - perm based weights
scores_lasomo_perm <- model_importance(
  forecast_data = forecast_data,
  oracle_output_data = target_data,
  ensemble_fun = "simple_ensemble",
  importance_algorithm = "lasomo", subset_wt = "perm_based"
)
model_importance_summary(
  scores_lasomo_perm, by = "model_id", na_action = "drop", fun = mean
)
```

In this example, there are only three models ($n = 3$), and the weights do not 
differ significantly between the two weighting schemes. 
Therefore, the resulting outputs show little difference. 
However, in general, with a larger number of models, the two weighting schemes 
may yield different importance scores for each model.

An extensive application in more complex scenarios with a larger number of 
models can be found in @kim2024.

## References

