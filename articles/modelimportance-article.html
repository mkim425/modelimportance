<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>`modelimportance`: Evaluating model importance within a multi-model ensemble in R ‚Ä¢ modelimportance</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet">
<meta property="og:title" content="`modelimportance`: Evaluating model importance within a multi-model ensemble in R">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">modelimportance</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../articles/get-started.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item"><a class="nav-link" href="../articles/index.html">Articles</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">



<script src="modelimportance-article_files/kePrint-0.0.1/kePrint.js"></script><link href="modelimportance-article_files/lightable-0.0.1/lightable.css" rel="stylesheet">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>`modelimportance`: Evaluating model importance within a multi-model ensemble in R</h1>
                        <h4 data-toc-skip class="author">Minsu Kim,
Nicholas Reich, Li Shandross</h4>
            
      

      <div class="d-none name"><code>modelimportance-article.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="abstract">Abstract<a class="anchor" aria-label="anchor" href="#abstract"></a>
</h2>
<p>Ensemble forecasts are commonly used to support decision-making and
policy planning across various fields because they often offer improved
accuracy and stability compared to individual models. As each model has
its own unique characteristics, understanding and measuring the value
each constituent model adds to the overall accuracy of the ensemble can
support the construction of effective ensembles. <span class="citation">Kim, Ray, and Reich (2024)</span> introduced metrics to
quantify how each component model contributes to the accuracy of
ensemble performance and demonstrated their use in the context of
probabilistic forecasting. Building on this work, the
<code>modelimportance</code> package provides tools for extended
applications, embracing both point and probabilistic forecasts. It
supports multiple functionalities, allowing users to specify which
ensemble approach to implement and which model importance metric to use.
Additionally, the software offers customizable options for handling
missing values. These features enable the package to serve as a
versatile tool for researchers and practitioners. It helps not only in
constructing an effective ensemble model across a wide range of
forecasting tasks, but also in understanding the role of each model
within the ensemble and gaining insights into individual models
themselves. This package follows the ‚Äòhubverse‚Äô <span class="citation">(Consortium of Infectious Disease Modeling Hubs
2024)</span> framework, which is a collection of open-source software
and tools developed to promote collaborative modeling hub efforts and
simplify their setup and operation. Accordingly, it depends on several
packages in the hubverse ecosystem, such as <code>hubUtils</code>,
<code>hubEnsembles</code>, <code>hubEvals</code>, and
<code>hubExamples</code>. Doing so enables seamless integration and
flexibility with other forecasting tools and systems, allowing many
analyses to be performed on existing and ongoing hubs.</p>
</div>
<div class="section level2">
<h2 id="sec:intro">Introduction<a class="anchor" aria-label="anchor" href="#sec:intro"></a>
</h2>
<p>Ensemble forecasting is a method to produce a single, consolidated
prediction by combining forecasts generated from different models. While
each model‚Äôs strengths are pronounced, its weaknesses are
counterbalanced, which leads to an ensemble forecast that is more robust
and accurate <span class="citation">(Gneiting and Raftery 2005; Hastie,
Tibshirani, and Friedman 2001)</span>. Specifically, ensembles
effectively mitigate the bias and variance arising from the predictions
of individual models by averaging them out, and aggregating in this way
can reduce prediction errors and improve overall performance. Enhanced
prediction accuracy and robustness enable the achievement of more
reliable predictions, thereby improving decision-making. For this
reason, ensemble forecasting is widely used across various domains such
as weather forecasting <span class="citation">(Jordan A. Guerra et al.
2020; Gneiting and Raftery 2005)</span>, financial modeling <span class="citation">(Sun, Wang, and Wei 2020; He et al. 2023)</span>, and
infectious disease outbreak forecasting <span class="citation">(Evan L.
Ray and Reich 2018; N. G. Reich et al. 2019; Lutz et al. 2019; Viboud et
al. 2018)</span>. For example, throughout the COVID-19 pandemic, the US
COVID-19 Forecast Hub collected individual models developed by over 90
different research groups and built a probabilistic ensemble forecasting
model for COVID-19 cases, hospitalizations, and deaths in the US based
on those models‚Äô predictions, which served as the official short-term
forecasts for the US Centers for Disease Control and Prevention (CDC)
<span class="citation">(Cramer et al. 2022)</span>.</p>
<p>The quality of forecasts is assessed by evaluating their error, bias,
sharpness, and/or calibration using different scoring metrics. The
selection of the scoring metrics depends on the type of forecast: point
forecasts (e.g., mean, median) and probabilistic forecasts (e.g.,
quantiles, samples, predictive cumulative distribution functions,
probability mass function). Commonly used assessment tools for point
forecasts are the mean absolute error (MAE) and the mean squared error
(MSE), which calculate the average magnitude of forecast errors. Scoring
metrics for probabilistic forecasts consider the uncertainty and
variability in predictions and provide concise evaluations through
numerical scores <span class="citation">(Gneiting and Raftery
2007)</span>. Some examples include the weighted interval score (WIS)
for the quantile-based forecasts and the continuous ranked probability
score (CRPS) for the forecasts taking the form of predictive cumulative
distribution functions <span class="citation">(Bracher et al.
2021)</span>. We note that CRPS is a general scoring rule that can be
computed either analytically in closed form or numerically from samples,
and WIS is a quantile-based approximation of CRPS.</p>
<p>Several <span class="proglang">R</span> packages have been developed
for this purpose. To name a few, the <span class="pkg">fable</span>
package <span class="citation">(O‚ÄôHara-Wild, Hyndman, and Wang
2024)</span> is widely used for univariate time series forecasting and
includes functions for accuracy measurement. The <span class="pkg">Metrics</span> <span class="citation">(Hamner and Frasco
2018)</span> and <span class="pkg">MLmetrics</span> <span class="citation">(Yan 2024)</span> provide a wide range of performance
metrics specifically designed for evaluating machine learning models.
The <span class="pkg">scoringRules</span> <span class="citation">(Jordan, Kr√ºger, and Lerch 2019)</span> package offers
a comprehensive set of proper scoring rules for evaluating probabilistic
forecasts and supports both univariate and multivariate settings. The
<span class="pkg">scoringutils</span> <span class="citation">(Bosse et
al. 2022)</span> package offers additional features to the functionality
provided by <span class="pkg">scoringRules</span>, which makes it more
useful for certain tasks, such as summarizing, comparing, and
visualizing forecast performance. These packages have been valuable to
evaluate individual models as independent entities, using performance
metrics selected for each specific situation or problem type. However,
they do not measure the individual models‚Äô contributions to the enhanced
predictive accuracy when used as part of an ensemble. <span class="citation">Kim, Ray, and Reich (2024)</span> demonstrate that a
model‚Äôs individual performance does not necessarily correspond to its
contribution as a component within an ensemble. Our developed package
introduces this capability. The <span class="pkg">modelimportance</span>
package provides tools to evaluate the role of each model as an ensemble
member within an ensemble model, rather than focusing on the individual
predictive performance per se.</p>
<p>In ensemble forecasting, certain models contribute more significantly
to the overall predictions than others. Assessing the impact of each
component model on ensemble predictions is methodologically similar to
determining variable importance in traditional regression and machine
learning models, where variable importance measures evaluate how much
individual variables decrease in accuracy of the model‚Äôs predictive
performance or reduce the average loss. <span class="proglang">R</span>
packages such as <span class="pkg">randomForest</span> <span class="citation">(Liaw and Wiener 2002)</span>, <span class="pkg">caret</span> <span class="citation">(Kuhn 2008)</span>,
<span class="pkg">xgboost</span> <span class="citation">(Chen et al.
2024)</span>, and <span class="pkg">gbm</span> <span class="citation">(Ridgeway and Developers 2024)</span> implement these
functions for different types of models: random forest models, general
machine learning models, extreme gradient boosting models, and
generalized boosted regression models, respectively. These packages
focus on feature-level importance within a single model and do not
measure the contribution of individual models within an ensemble. The
<span class="pkg">modelimportance</span> package addresses this
limitation. The tools in the <span class="pkg">modelimportance</span>
quantify how each component model helps enhance the ensemble model‚Äôs
predictive performance. They assign numerical scores to each model based
on a selected metric that measures forecast accuracy, depending on the
forecast type.</p>
<p>These capabilities provide unique support for hub organizers who
refer to the entity responsible for launching and managing a collective
modeling hub. They coordinate multiple teams to produce forecasts and
integrate their predictions into an ensemble forecast <span class="citation">(Shandross et al. 2024)</span>, which, as mentioned
earlier, is known for having better performance compared to individual
models. Examples include the US CDC and the European Centre for Disease
Prevention and Control. The <span class="pkg">modelimportance</span>
package can even strengthen the benefits of a multi-model ensemble by
helping these organizations create more effective ensemble forecasts
based on the precise evaluation of each model‚Äôs contribution.
Specifically, <span class="pkg">modelimportance</span> follows
‚Äòhubverse‚Äô standards, where ‚Äòhubverse‚Äô offers a set of publicly
available software and data tools developed to promote collaborative
modeling hub efforts and reduce the effort required to set up and
operate them <span class="citation">(Consortium of Infectious Disease
Modeling Hubs 2024)</span>. Adherence to the model output formats
specified by the hubverse convention enables many analyses to be
performed on existing and ongoing hubs by seamless integration and
flexibility with other forecasting tools and systems. We note that there
are over a dozen active hubs running as of fall 2025, with more in
planning stages.</p>
<p>We highlight some strong development practices we employed, such as
unit testing of individual functions, continuous integration testing on
different operating systems, and independent code review. This emphasis
on quality control is a key strength of this work and distinguishes it
from other academic software development projects.</p>
<p>The paper proceeds as follows. describes how the package relates to
the hubverse framework, including its dependencies, the model output
formats defined within hubverse, and the structure of data presentation
for both forecasts and actual observations. presents two algorithms
implemented in for calculating the model importance metric:
leave-one-model-out and leave-all-subsets-of-models-out. We demonstrate
the various functionalities supports in and highlight our quality
assurance measures and its open access in . Some examples are provided
in . We close this paper with concluding remarks and a discussion of
possible extensions.</p>
</div>
<div class="section level2">
<h2 id="sec-data">Data<a class="anchor" aria-label="anchor" href="#sec-data"></a>
</h2>
<div class="section level3">
<h3 id="subsec:dependence_hubverse">Relationship and dependencies on hubverse<a class="anchor" aria-label="anchor" href="#subsec:dependence_hubverse"></a>
</h3>
<p>The <span class="pkg">modelimportance</span> package is designed to
work with the hubverse framework and, accordingly, depends on several
packages in the hubverse ecosystem, such as <span class="pkg">hubUtils</span> (<span class="citation">Krystalli and
Shandross (2025)</span>), <span class="pkg">hubEnsembles</span> (<span class="citation">Shandross, Howerton, and Ray (2025)</span>), <span class="pkg">hubEvals</span> (<span class="citation">N. Reich et al.
(2025)</span>), and <span class="pkg">hubExamples</span> (<span class="citation">Evan L. Ray, Sweger, and Contamin (2025)</span>). <span class="pkg">modelimportance</span> uses a <code>model_out_tbl</code> S3
class as the model output format defined in <span class="pkg">hubUtils</span>, which consists of utility functions to
standardize prediction files and data formats (details in ). Ensembling
predictions from multiple models relies on <span class="pkg">hubEnsembles</span>, which offers a broadly applicable
framework to construct multi-model ensembles using various ensemble
methods. Calculation of forecast accuracy using various metrics is based
on <span class="pkg">hubEvals</span>, which internally leverages <span class="pkg">scoringutils</span>. We use the example datasets from <span class="pkg">hubExamples</span> for testing and demonstration purposes
(see ).</p>
</div>
<div class="section level3">
<h3 id="subsec:model_output_format">Model output format<a class="anchor" aria-label="anchor" href="#subsec:model_output_format"></a>
</h3>
<p>Model outputs are structured in a tabular format designed
specifically for predictions, which is a formal S3 object called
<code>model_out_tbl</code>. In the hubverse standard, each row
represents an individual prediction or a component of a prediction for a
single task, and its details are described in multiple columns through
which one can identify the unique label assigned to each forecasting
model, task characteristics, prediction representation type, and
predicted values <span class="citation">(Shandross et al. 2024)</span>.
To elaborate on the task characteristics, each prediction task means a
specific forecasting problem and it can be described by a set of task ID
variables. Examples of such variables include a date on which forecasts
are generated, the target to predict (e.g., flu-related incident deaths,
cases, or hospitalizations), and the prediction horizon, which is the
length of time into the future from the point when a model generate its
forecast, for a specific location on a certain target date. <span class="citation">(<strong>tbl-example-model_output?</strong>)</span>
illustrates short-term forecasts of weekly incident influenza
hospitalizations in the US for Massachusetts, generated by the model
‚ÄòFlusight-baseline‚Äô on December 17, 2022, in the
<code>model_out_tbl</code> format. The <code>model_id</code> column
lists a uniquely identified model name. All of the
<code>reference_date</code>, <code>target</code>, <code>horizon</code>,
<code>location</code>, and <code>target_end_date</code> columns are all
referred to as the task ID variables, which together defines the task
characteristics. Note that the forecast generation date and the target
date for which the prediction is made are mapped to the
<code>reference_date</code> and <code>target_end_date</code> columns,
respectively, and the location is represented based on the FIPS code
(e.g., ‚Äò25‚Äô for Massachusetts). The time length to the
<code>target_end_date</code>, which is the number of weeks ahead from
the <code>reference_date</code>, is indicated in the
<code>horizon</code> column. The prediction representation is specified
as ‚Äòquantile‚Äô in the <code>output_type</code> column, and details are
represented in the <code>output_type_id</code> column with seven
quantiles of 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, and 0.95 for each target
end date. The predicted value corresponding to each quantile is recorded
in the <code>value</code> column.</p>
<table class="table table table-striped table-hover table-condensed table-responsive" style="font-size: 12px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
Table 1: Example of the model output for incident influenza
hospitalizations (top 10 rows) extracted from
<code>forecast_outputs</code> data in the <code>hubExamples</code>
package.
</caption>
<thead><tr>
<th style="text-align:left;">
model_id
</th>
<th style="text-align:left;">
reference_date
</th>
<th style="text-align:left;">
target
</th>
<th style="text-align:right;">
horizon
</th>
<th style="text-align:left;">
location
</th>
<th style="text-align:left;">
target_end_date
</th>
<th style="text-align:left;">
output_type
</th>
<th style="text-align:left;">
output_type_id
</th>
<th style="text-align:right;">
value
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
Flusight-baseline
</td>
<td style="text-align:left;">
2022-12-17
</td>
<td style="text-align:left;">
wk inc flu hosp
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
25
</td>
<td style="text-align:left;">
2022-12-24
</td>
<td style="text-align:left;">
quantile
</td>
<td style="text-align:left;">
0.05
</td>
<td style="text-align:right;">
496
</td>
</tr>
<tr>
<td style="text-align:left;">
Flusight-baseline
</td>
<td style="text-align:left;">
2022-12-17
</td>
<td style="text-align:left;">
wk inc flu hosp
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
25
</td>
<td style="text-align:left;">
2022-12-24
</td>
<td style="text-align:left;">
quantile
</td>
<td style="text-align:left;">
0.1
</td>
<td style="text-align:right;">
536
</td>
</tr>
<tr>
<td style="text-align:left;">
Flusight-baseline
</td>
<td style="text-align:left;">
2022-12-17
</td>
<td style="text-align:left;">
wk inc flu hosp
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
25
</td>
<td style="text-align:left;">
2022-12-24
</td>
<td style="text-align:left;">
quantile
</td>
<td style="text-align:left;">
0.25
</td>
<td style="text-align:right;">
566
</td>
</tr>
<tr>
<td style="text-align:left;">
Flusight-baseline
</td>
<td style="text-align:left;">
2022-12-17
</td>
<td style="text-align:left;">
wk inc flu hosp
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
25
</td>
<td style="text-align:left;">
2022-12-24
</td>
<td style="text-align:left;">
quantile
</td>
<td style="text-align:left;">
0.5
</td>
<td style="text-align:right;">
582
</td>
</tr>
<tr>
<td style="text-align:left;">
Flusight-baseline
</td>
<td style="text-align:left;">
2022-12-17
</td>
<td style="text-align:left;">
wk inc flu hosp
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
25
</td>
<td style="text-align:left;">
2022-12-24
</td>
<td style="text-align:left;">
quantile
</td>
<td style="text-align:left;">
0.75
</td>
<td style="text-align:right;">
598
</td>
</tr>
<tr>
<td style="text-align:left;">
Flusight-baseline
</td>
<td style="text-align:left;">
2022-12-17
</td>
<td style="text-align:left;">
wk inc flu hosp
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
25
</td>
<td style="text-align:left;">
2022-12-24
</td>
<td style="text-align:left;">
quantile
</td>
<td style="text-align:left;">
0.9
</td>
<td style="text-align:right;">
629
</td>
</tr>
<tr>
<td style="text-align:left;">
Flusight-baseline
</td>
<td style="text-align:left;">
2022-12-17
</td>
<td style="text-align:left;">
wk inc flu hosp
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
25
</td>
<td style="text-align:left;">
2022-12-24
</td>
<td style="text-align:left;">
quantile
</td>
<td style="text-align:left;">
0.95
</td>
<td style="text-align:right;">
668
</td>
</tr>
<tr>
<td style="text-align:left;">
Flusight-baseline
</td>
<td style="text-align:left;">
2022-12-17
</td>
<td style="text-align:left;">
wk inc flu hosp
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
25
</td>
<td style="text-align:left;">
2022-12-31
</td>
<td style="text-align:left;">
quantile
</td>
<td style="text-align:left;">
0.05
</td>
<td style="text-align:right;">
454
</td>
</tr>
<tr>
<td style="text-align:left;">
Flusight-baseline
</td>
<td style="text-align:left;">
2022-12-17
</td>
<td style="text-align:left;">
wk inc flu hosp
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
25
</td>
<td style="text-align:left;">
2022-12-31
</td>
<td style="text-align:left;">
quantile
</td>
<td style="text-align:left;">
0.1
</td>
<td style="text-align:right;">
518
</td>
</tr>
<tr>
<td style="text-align:left;">
Flusight-baseline
</td>
<td style="text-align:left;">
2022-12-17
</td>
<td style="text-align:left;">
wk inc flu hosp
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
25
</td>
<td style="text-align:left;">
2022-12-31
</td>
<td style="text-align:left;">
quantile
</td>
<td style="text-align:left;">
0.25
</td>
<td style="text-align:right;">
558
</td>
</tr>
</tbody>
</table>
<p><span class="citation">(<strong>fig-example-model_output?</strong>)</span>
visualize the information on the prediction task provided by <span class="citation">(<strong>tbl-example-model_output?</strong>)</span> for
three models. For each model, the quantile-based forecasts are shown for
the target end dates of December 24, 2022 (horizon 1), December 31, 2022
(horizon 2), and January 07, 2023 (horizon 3), which were made on
December 17, 2022 based on the historical data available as of that
date. The prediction intervals defined by the lowest and highest
quantiles (0.05 and 0.95) represent the uncertainty of the predictions.
To give brief details in the interpretation, the Flusight-baseline model
under-predicted the outcomes for the first two target dates (horizon 1
and 2), but it over-predicted the outcome for the last target date
(horizon 3). Its prediction intervals are narrow compared to the other
two models, which indicates that it is more confident about its
predictions. However, two of three prediction intervals (horizons 1 and
2) failed to cover the eventually observed values, implying that the
model was apparently overconfident.</p>
<div class="figure">
<img src="modelimportance-article_files/figure-html/fig-example-model_output-1.png" class="r-plt" alt="Figure 1: Example plot of three distributional forecasts corresponding to the model output for incident influenza hospitalizations shown in Table 1. Solid black dots indicate historically available data as of the forecast generation date, and open black circles indicate the eventually observed values. The blue dots represent predictive medians and the blue shaded area represents the corresponding 90% prediction interval defined by the 0.05 and 0.95 quantiles." width="672"><p class="caption">
Figure 1: Example plot of three distributional forecasts corresponding
to the model output for incident influenza hospitalizations shown in
Table 1. Solid black dots indicate historically available data as of the
forecast generation date, and open black circles indicate the eventually
observed values. The blue dots represent predictive medians and the blue
shaded area represents the corresponding 90% prediction interval defined
by the 0.05 and 0.95 quantiles.
</p>
</div>
</div>
<div class="section level3">
<h3 id="subsec:model_output">Forecast data representation<a class="anchor" aria-label="anchor" href="#subsec:model_output"></a>
</h3>
<p>Generally, quantitative forecasts can be categorized into either
point forecasts or probabilistic forecasts. For a specific prediction
task, point forecasts, represented by a single predicted value, provide
a clear and concise prediction, making them easy to interpret and
communicate. Probabilistic forecasts, on the other hand, provide a
probability distribution over possible future values, which inherently
involves uncertainty. They are represented in various ways, such as
probability mass functions (pmf), cumulative distribution functions
(cdf), samples, or probability quantiles (or intervals).</p>
<p>The <code>output_type</code> and <code>output_type_id</code> columns
in the model output format, as defined by the hubverse convention,
specify the forecast structure. Only one <code>output_type</code> is
allowed, and it must be one of the ‚Äòmean‚Äô, ‚Äòmedian‚Äô, ‚Äòquantile‚Äô, or
‚Äòpmf‚Äô in the <span class="pkg">modelimportance</span> package: ‚Äòmean‚Äô or
‚Äòmedian‚Äô for point forecasts and ‚Äòquantile‚Äô or ‚Äòpmf‚Äô for probabilistic
forecasts. As aforementioned, <code>output_type_id</code> column
identifies addtional detailed information, such as specific quantile
levels (e.g., ‚Äú0.1‚Äù, ‚Äú0.25‚Äù, ‚Äú0.5‚Äù, ‚Äú0.75‚Äù, and ‚Äú0.9‚Äù) for the
‚Äòquantile‚Äô output type and categorical values (e.g., ‚Äúlow‚Äù, ‚Äúmoderate‚Äù,
‚Äúhigh‚Äù, and ‚Äúvery high‚Äù) for the ‚Äòpmf‚Äô output type. The predicted values
for <code>pmf</code> are constrained to be between 0 and 1, indicating
the probability at each categorical level, while they are unbounded
numeric otherwise. Different output types correspond to different
scoring rules for evaluating a model‚Äôs prediction performance. <span class="citation">(<strong>tbl-pair-output-scoringrule?</strong>)</span>
presents the output types and their associated scoring rules supported
by the <span class="pkg">modelimportance</span> package.</p>
<table class="table table table-striped table-hover table-condensed table-responsive" style="font-size: 13px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
Table 2: Pairs of output types and their associated scoring rules for
evaluating prediction performance.
</caption>
<thead><tr>
<th style="text-align:left;">
Output Type
</th>
<th style="text-align:left;">
Scoring Rule
</th>
<th style="text-align:left;">
Description
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
mean
</td>
<td style="text-align:left;">
RSE
</td>
<td style="text-align:left;">
Evaluate using the root squared error (RSE)
</td>
</tr>
<tr>
<td style="text-align:left;">
median
</td>
<td style="text-align:left;">
AE
</td>
<td style="text-align:left;">
Evaluate using the absolute error (AE)
</td>
</tr>
<tr>
<td style="text-align:left;">
quantile
</td>
<td style="text-align:left;">
WIS
</td>
<td style="text-align:left;">
Evaluate using the weighted interval score (WIS)
</td>
</tr>
<tr>
<td style="text-align:left;">
pmf
</td>
<td style="text-align:left;">
Log Score
</td>
<td style="text-align:left;">
Evaluate using the logarithm of the probability assigned to the true
outcome (LogScore)
</td>
</tr>
</tbody>
</table>
</div>
<div class="section level3">
<h3 id="subsec:oracle_output_data">Oracle output data<a class="anchor" aria-label="anchor" href="#subsec:oracle_output_data"></a>
</h3>
<p>The <code>oracle_output_data</code> is a data frame that contains the
ground truth values for the variables used to define modeling targets.
It is referred to as ‚Äúoracle‚Äù because it is formatted as if an oracle
made a perfect point prediction equal to the truth. This data must
follow the oracle output format defined in the hubverse standard, which
includes independent task ID columns (e.g., <code>location</code>,
<code>target_date</code>), the <code>output_type</code> column
specifying the output type of the predictions and an
<code>oracle_value</code> column for the observed values. As in the
forecast data, if the <code>output_type</code> is either
<code>"quantile"</code> or <code>"pmf"</code>, the
<code>output_type_id</code> column is often required to provide further
identifying information.</p>
<p>The <code>model_out_tbl</code> and <code>oracle_output_data</code>
must have the same task ID columns and <code>output_type</code>,
including <code>output_type_id</code> if necessary, which are used to
match the predictions with the ground truth data.</p>
</div>
</div>
<div class="section level2">
<h2 id="sec:algorithms">Algorithms<a class="anchor" aria-label="anchor" href="#sec:algorithms"></a>
</h2>
<p>This section provides a brief description of the leave one model out
(LOMO) and leave all subsets of models out (LASOMO) algorithms, which
are used to compute the model importance score. The basic idea of
measuring the importance of each component model is to evaluate the
change in ensemble performance when that model is included or excluded
in the ensemble construction. More specifically, we compare the
performance of an ensemble with and without a specific model for a
specific task, and consider the difference in performance as the
importance of that model for that task. We apply this idea to many tasks
and aggregate the importance scores for that model across all tasks
using averages. (Details can be found in <span class="citation">Kim,
Ray, and Reich (2024)</span>.)</p>
<p>LOMO involves creating an ensemble by excluding one component model
from the entire set of models. Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùíú</mi><annotation encoding="application/x-tex">{\mathcal A}</annotation></semantics></math>
be a set of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
models and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>F</mi><mi>i</mi></msup><annotation encoding="application/x-tex">F^i</annotation></semantics></math>
be a forecast produced by model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><mi>n</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">i = 1,2, \dots, n.</annotation></semantics></math>
Each ensemble excludes exactly one model while including all the others.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>F</mi><msup><mi>ùíú</mi><mrow><mo>‚àí</mo><mi>i</mi></mrow></msup></msup><annotation encoding="application/x-tex">F^{{\mathcal A}^{-i}}</annotation></semantics></math>
denotes the ensemble forecast constructed using all forecasts
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>F</mi><mi>ùíú</mi></msup><annotation encoding="application/x-tex">F^{\mathcal A}</annotation></semantics></math>
except
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>F</mi><mi>i</mi></msup><annotation encoding="application/x-tex">F^i</annotation></semantics></math>.
Model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>‚Äôs
importance score using LOMO is calculated as the difference in accuracy,
as measured by a specific score, between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>F</mi><msup><mi>ùíú</mi><mrow><mo>‚àí</mo><mi>i</mi></mrow></msup></msup><annotation encoding="application/x-tex">F^{{\mathcal A}^{-i}}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>F</mi><mi>ùíú</mi></msup><annotation encoding="application/x-tex">F^{\mathcal A}</annotation></semantics></math>
(). For example, when evaluating model 1 within an ensemble of three
models
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">n=3</annotation></semantics></math>),
LOMO creates an ensemble forecast
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>F</mi><mrow><mo stretchy="false" form="prefix">{</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo stretchy="false" form="postfix">}</mo></mrow></msup><annotation encoding="application/x-tex">F^{\{2,3\}}</annotation></semantics></math>
using only
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>F</mi><mn>2</mn></msup><annotation encoding="application/x-tex">F^2</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>F</mi><mn>3</mn></msup><annotation encoding="application/x-tex">F^3</annotation></semantics></math>.
The performance of this reduced ensemble is then compared to the full
ensemble forecast
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>F</mi><mrow><mo stretchy="false" form="prefix">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo stretchy="false" form="postfix">}</mo></mrow></msup><annotation encoding="application/x-tex">F^{\{1,2,3\}}</annotation></semantics></math>,
which incorporates all three models. We note that a model can make an
ensemble better or worse, and thus the importance score for model 1 can
be positive or negative accordingly.</p>
<div class="figure">
<img src="algorithm-lomo.jpg" class="r-plt" alt="Algorithm 1: Leave one model out (LOMO)" width="100%"><p class="caption">
Algorithm 1: Leave one model out (LOMO)
</p>
</div>
<p>On the other hand, LASOMO involves ensemble constructions from all
possible subsets of models. For each subset
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
that does not contain the model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>‚à™</mo><mo stretchy="false" form="prefix">{</mo><mi>i</mi><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">S \cup \{i\}</annotation></semantics></math>
plays a role of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùíú</mi><annotation encoding="application/x-tex">{\mathcal A}</annotation></semantics></math>
in the LOMO; the score associated with the subset
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
is the difference of measures between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>F</mi><mi>S</mi></msup><annotation encoding="application/x-tex">F^S</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>F</mi><mrow><mi>S</mi><mo>‚à™</mo><mo stretchy="false" form="prefix">{</mo><mi>i</mi><mo stretchy="false" form="postfix">}</mo></mrow></msup><annotation encoding="application/x-tex">F^{S \cup \{i\}}</annotation></semantics></math>.
Then, all scores are aggregated across all possible subsets that the
model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
does not belong to (). For example, using the earlier setup of three
forecast models, LASOMO considers three subsets, which we denote by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mn>1</mn></msub><mo>=</mo><mo stretchy="false" form="prefix">{</mo><mn>2</mn><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">S_1=\{2\}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mn>2</mn></msub><mo>=</mo><mo stretchy="false" form="prefix">{</mo><mn>3</mn><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">S_2=\{3\}</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mn>3</mn></msub><mo>=</mo><mo stretchy="false" form="prefix">{</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">S_3=\{2, 3\}</annotation></semantics></math>,
to calculate the importance score of model 1 (excluding all subsets that
include model 1). The ensemble forecasts
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>F</mi><mrow><mo stretchy="false" form="prefix">{</mo><mn>2</mn><mo stretchy="false" form="postfix">}</mo></mrow></msup><mo>,</mo><msup><mi>F</mi><mrow><mo stretchy="false" form="prefix">{</mo><mn>3</mn><mo stretchy="false" form="postfix">}</mo></mrow></msup></mrow><annotation encoding="application/x-tex">F^{\{2\}}, F^{\{3\}}</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>F</mi><mrow><mo stretchy="false" form="prefix">{</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo stretchy="false" form="postfix">}</mo></mrow></msup><annotation encoding="application/x-tex">F^{\{2,3\}}</annotation></semantics></math>
are then compared to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>F</mi><mrow><mo stretchy="false" form="prefix">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo stretchy="false" form="postfix">}</mo></mrow></msup><mo>,</mo><msup><mi>F</mi><mrow><mo stretchy="false" form="prefix">{</mo><mn>1</mn><mo>,</mo><mn>3</mn><mo stretchy="false" form="postfix">}</mo></mrow></msup></mrow><annotation encoding="application/x-tex">F^{\{1,2\}}, F^{\{1,3\}}</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>F</mi><mrow><mo stretchy="false" form="prefix">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo stretchy="false" form="postfix">}</mo></mrow></msup><annotation encoding="application/x-tex">F^{\{1,2,3\}}</annotation></semantics></math>,
respectively. The performance differences attributable to model 1‚Äôs
inclusion are aggregated, which results in the importance score for
model 1. We note that the subsets (e.g.,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mn>1</mn></msub><mo>,</mo><msub><mi>S</mi><mn>2</mn></msub><mo>,</mo></mrow><annotation encoding="application/x-tex">S_1, S_2,</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>3</mn></msub><annotation encoding="application/x-tex">S_3</annotation></semantics></math>)
may have different weights during the aggregating process.</p>
<div class="figure">
<img src="algorithm-lasomo.jpg" class="r-plt" alt="Algorithm 2: Leave all subsets of models out (LASOMO)" width="100%"><p class="caption">
Algorithm 2: Leave all subsets of models out (LASOMO)
</p>
</div>
<p>The <span class="pkg">modelimportance</span> package offers two
weighting options for subsets: one assigns equal (uniform) weights to
all subsets, and the other assigns weights based on their size, similar
to the concept of Shapley values in cooperative game theory, which
measure a player‚Äôs average contribution to all possible coalitions (or,
equivalently, over all permutations of players) (<span class="citation">Shapley (1953)</span>). Users can choose one to
evaluate the contribution of each model in a manner suited to their
preferred framework.</p>
<p>Algorithms and outline the steps to implement LOMO and LASOMO for a
single prediction task, respectively.</p>
<div class="section level3">
<h3 id="comparison-of-weighting-schemes-in-lasomo">Comparison of weighting schemes in LASOMO<a class="anchor" aria-label="anchor" href="#comparison-of-weighting-schemes-in-lasomo"></a>
</h3>
<p>The differences in how the two weighting schemes influence the
importance scores become more pronounced as the number of models
increases. As described in , the formulas for their subset weights are
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mrow><msup><mn>2</mn><mrow><mi>n</mi><mo>‚àí</mo><mn>1</mn></mrow></msup><mo>‚àí</mo><mn>1</mn></mrow></mfrac><mspace width="1.0em"></mspace><mtext mathvariant="normal">and</mtext><mspace width="1.0em"></mspace><mfrac><mn>1</mn><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac linethickness="0"><mrow><mi>n</mi><mo>‚àí</mo><mn>1</mn></mrow><mi>k</mi></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">\frac{1}{2^{n-1}-1} \quad\text{and}\quad \frac{1}{(n-1)\binom{n-1}{k}},</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
is the size of each subset and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
is the total number of models. The equal scheme treats all subsets
equally, so medium-sized subsets have considerable influence in the
final result, as there are many such subsets. In contrast, the
permutation-based scheme adjusts the weights according to the subset
size, giving the greatest weight to both the smallest and largest
subsets while assigning small weights to the mid-sized subsets.
Moreover, the weights assigned to the mid-sized subsets under the
permutation-based approach decrease much faster with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
than those under the equal weighting scheme (see for details).
Consequently, when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
is large, middle-sized subsets play a dominant role in determining the
importance scores under the equal weighting scheme, whereas
extreme-sized subsets primarily drive the scores under the
permutation-based weighting approach.</p>
<p>Overall, the difference between the two weighting schemes is likely
to arise mainly from the the extreme-sized subsets when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
is large. This is because the weights given to the mid-sized subsets
become increasingly similar, which are very small values on the order of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mn>10</mn><mrow><mo>‚àí</mo><mn>3</mn></mrow></msup><annotation encoding="application/x-tex">10^{-3}</annotation></semantics></math>
even when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">n=8</annotation></semantics></math>,
while the weights assigned to the smallest and largest subsets remain
substantially different (<span class="citation">(<strong>fig-lasomo-weights?</strong>)</span>).</p>
<div class="figure">
<img src="modelimportance-article_files/figure-html/fig-lasomo-weights-1.png" class="r-plt" alt="Figure 2:Comparison of weights assigned to a subset. The plot shows the weights assigned to a subset as the number of models $n$ increases from 2 to 10. The red line represents the weights under the equal weighting scheme , while the blue and green lines represent the minimum and maximum weights, respectively, under the permutation-based weighting scheme. The minimum weight occurs when the subset size is around $(n-1)/2$, and the maximum weight occurs when the subset size is $n-1$. As the number of models increases, the weights assigned by the two schemes become increasingly similar for mid-sized subsets whereas substantial differences remain for extreme-sized subsets." width="672"><p class="caption">
Figure 2:Comparison of weights assigned to a subset. The plot shows the
weights assigned to a subset as the number of models
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
increases from 2 to 10. The red line represents the weights under the
equal weighting scheme , while the blue and green lines represent the
minimum and maximum weights, respectively, under the permutation-based
weighting scheme. The minimum weight occurs when the subset size is
around
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">(n-1)/2</annotation></semantics></math>,
and the maximum weight occurs when the subset size is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>‚àí</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n-1</annotation></semantics></math>.
As the number of models increases, the weights assigned by the two
schemes become increasingly similar for mid-sized subsets whereas
substantial differences remain for extreme-sized subsets.
</p>
</div>
</div>
</div>
<div class="section level2">
<h2 id="sec:main-functions">Main functions<a class="anchor" aria-label="anchor" href="#sec:main-functions"></a>
</h2>
<p>In this section, we describe the usage of the function
<code><a href="../reference/model_importance.html">model_importance()</a></code>, where multiple options are available to
customize the evaluation framework (<span class="citation">(<strong>tbl-arguments1?</strong>)</span>, <span class="citation">(<strong>tbl-arguments2?</strong>)</span>).</p>
<div class="section level3">
<h3 id="model_importance">model_importance( )<a class="anchor" aria-label="anchor" href="#model_importance"></a>
</h3>
<p>The <code><a href="../reference/model_importance.html">model_importance()</a></code> function calculates the
importance scores of ensemble component models based on their
contributions to improving ensemble prediction accuracy for each
prediction task. It returns a single data frame of importance scores
combined across all tasks. If a model missed predictions for a specific
task, an <code>NA</code> value will be assigned for that task.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/model_importance.html">model_importance</a></span><span class="op">(</span><span class="va">forecast_data</span>, <span class="va">oracle_output_data</span>, <span class="va">ensemble_fun</span>,</span>
<span>                 <span class="va">importance_algorithm</span>, <span class="va">subset_wt</span>, <span class="va">min_log_score</span>,</span>
<span>                 <span class="va">...</span><span class="op">)</span></span></code></pre></div>
<p>The <code>forecast_data</code> is a data frame containing predictions
and should be or can be coerced to a <code>model_out_tbl</code> format,
which is the standard S3 class model output format defined by the
hubverse convention. If it fails to be coerced to a
<code>model_out_tbl</code> format, an error message will be returned
from <span class="pkg">hubUtils</span>, which provides the function
<code>as_model_out_tbl()</code> for this purpose. Users may need to
manually transform their data to meet the hubverse standards.</p>
<p>The <code>oracle_output_data</code> is a data frame containing the
actual observed values of the variables used to specify modeling
targets. Details are provided in .</p>
<p>The <code>ensemble_fun</code> argument specifies the ensemble method
to be used for evaluating model importance, which relies on
implementations in the package (<span class="citation">Shandross,
Howerton, and Ray (2025)</span>). The currently supported methods are
<code>"simple_ensemble"</code> and <code>"linear_pool"</code>. The
<code>"simple_ensemble"</code> method returns the average of the
predicted values from all component models per prediction task defined
by task IDs, <code>output_type</code>, and <code>output_type_id</code>
columns. The default aggregation function for this method is
<code>"mean"</code>, but it can be customized by specifying additional
arguments through <code>...</code>, such as
<code>agg_fun="median"</code>. When <code>"linear_pool"</code> is
specified, ensemble model outputs are created as a linear pool of
component model outputs. This method supports only an
<code>output_type</code> of <code>"mean"</code>,
<code>"quantile"</code>, or <code>"pmf"</code>.</p>
<table class="table table table-striped table-condensed table-responsive" style="font-size: 13px; width: auto !important; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
Table 3: Description of the arguments for the
<code><a href="../reference/model_importance.html">model_importance()</a></code> function, including their purpose,
possible values, and default settings.
</caption>
<thead><tr>
<th style="text-align:left;">
Argument
</th>
<th style="text-align:left;">
Description
</th>
<th style="text-align:left;">
Possible Values
</th>
<th style="text-align:left;">
Default
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
<code>forecast_data</code>
</td>
<td style="text-align:left;">
Forecasts
</td>
<td style="text-align:left;">
Must be the model output format
</td>
<td style="text-align:left;">
N/A
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>oracle_output_data</code>
</td>
<td style="text-align:left;">
Ground truth data
</td>
<td style="text-align:left;">
Must be the oracle output format
</td>
<td style="text-align:left;">
N/A
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>ensemble_fun</code>
</td>
<td style="text-align:left;">
Ensemble method
</td>
<td style="text-align:left;">
<code>'simple_ensemble'</code>, <code>'linear_pool'</code>
</td>
<td style="text-align:left;">
<code>'simple_ensemble'</code>
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>importance_algorithm</code>
</td>
<td style="text-align:left;">
Algorithm to calculate importance
</td>
<td style="text-align:left;">
<code>'lomo', 'lasomo'</code>
</td>
<td style="text-align:left;">
<code>'lomo'</code>
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>subset_wt</code>
</td>
<td style="text-align:left;">
Method for assigning weight to subsets when using LASOMO algorithm
</td>
<td style="text-align:left;">
<code>'equal', 'perm_based'</code>
</td>
<td style="text-align:left;">
<code>'equal'</code>
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>min_log_score</code>
</td>
<td style="text-align:left;">
Minimum value to replace for log score
</td>
<td style="text-align:left;">
Non-positive numeric
</td>
<td style="text-align:left;">
-10
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>...</code>
</td>
<td style="text-align:left;">
Optional arguments for <code>'simple_ensemble'</code>
</td>
<td style="text-align:left;">
Varies
</td>
<td style="text-align:left;">
<code>agg_fun='mean'</code>
</td>
</tr>
</tbody>
</table>
<p>The <code>importance_algorithm</code> argument specifies the
algorithm for model importance calculation, which can be either
<code>"lomo"</code> (leave one model out) and <code>"lasomo"</code>
(leave all subsets of models out). The <code>subset_wt</code> argument
is employed only for the <code>"lasomo"</code> algorithm. This argument
has two options: <code>"equal"</code> assigns equal weight to all
subsets and <code>"perm_based"</code> assigns weight averaged over all
possible permutations as in the formula of Shapley values (). The
default values of <code>importance_algorithm</code> and
<code>subset_wt</code> are <code>"lomo"</code> and <code>"equal"</code>,
respectively.</p>
<p>The <code>min_log_score</code> argument is relevant only for the
<code>output_type</code> of <code>"pmf"</code>, which uses Log Score as
a scoring rule. It sets a minimum threshold for log scores to avoid
issues with extremely low probabilities assigned to the true outcome,
which can lead to undefined or negative infinite log scores. Any
probability lower than this threshold will be adjusted to this minimum
value before calculating the importance metric based on the log score.
The default value is set to -10, following the CDC FluSight thresholding
convention <span class="citation">(Brooks et al. 2018; N. G. Reich et
al. 2019)</span>. Users may choose a different value based on their
practical needs.</p>
</div>
<div class="section level3">
<h3 id="model_importance_summary">model_importance_summary( )<a class="anchor" aria-label="anchor" href="#model_importance_summary"></a>
</h3>
<p>The <code><a href="../reference/model_importance_summary.html">model_importance_summary()</a></code> function summarizes the
importance scores produced by <code><a href="../reference/model_importance.html">model_importance()</a></code> across
tasks for each model.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/model_importance_summary.html">model_importance_summary</a></span><span class="op">(</span><span class="va">importance_scores</span>, by <span class="op">=</span> <span class="st">"model_id"</span>,</span>
<span>                         na_action <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"drop"</span>, <span class="st">"worst"</span>, <span class="st">"average"</span><span class="op">)</span>,</span>
<span>                         fun <span class="op">=</span> <span class="va">mean</span>, <span class="va">...</span><span class="op">)</span></span></code></pre></div>
<p>The <code>importance_scores</code> is a data frame containing model
importance scores for individual prediction tasks, as produced by the
<code><a href="../reference/model_importance.html">model_importance()</a></code> function.</p>
<p>The <code>by</code> argument specifies the grouping variable(s) for
summarization. The default is <code>"model_id"</code> to summarize
importance scores for each model. Users can also specify other columns
present in the <code>importance_scores</code> data frame as needed.</p>
<p>The <code>na_action</code> argument allows for specifying how to
handle <code>NA</code> values generated during importance score
calculation for each task, occurring when a model did not contribute to
the ensemble prediction for a given task by missing its forecast
submission. Three options are available: <code>"worst"</code>,
<code>"average"</code>, and <code>"drop"</code>. In each specific
prediction task, if a model has any missing predictions, the
<code>"worst"</code> option replaces the <code>NA</code> values with the
smallest value among other models‚Äô importance metrics, while the
<code>"average"</code> option replaces them with the average of the
other models‚Äô importance metrics in that task. The <code>"drop"</code>
option removes the <code>NA</code> values, which results in the
exclusion of the model from the evaluation for that task.</p>
<p>The <code>fun</code> argument specifies a function used to summarize
importance scores. <code>fun = mean</code> is a default choice, but
other summary functions are also applicable (e.g.,
<code>fun = median</code>). Additional arguments can be passed to the
summary function <code>fun</code> through <code>...</code> if needed
(e.g., <code>fun = quantile, probs = 0.25</code> for a quartile
summary).</p>
<table class="table table table-striped table-condensed table-responsive" style="font-size: 13px; width: auto !important; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
Table 4: Description of the arguments for the
<code><a href="../reference/model_importance_summary.html">model_importance_summary()</a></code> function, including their
purpose, possible values, and default settings.
</caption>
<thead><tr>
<th style="text-align:left;">
Argument
</th>
<th style="text-align:left;">
Description
</th>
<th style="text-align:left;">
Possible Values
</th>
<th style="text-align:left;">
Default
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
<code>importance_scores</code>
</td>
<td style="text-align:left;">
Model importance scores produced by <code><a href="../reference/model_importance.html">model_importance()</a></code>
</td>
<td style="text-align:left;">
data frame
</td>
<td style="text-align:left;">
N/A
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>by</code>
</td>
<td style="text-align:left;">
Grouping variable(s) for summarization
</td>
<td style="text-align:left;">
grouping variable(s)
</td>
<td style="text-align:left;">
<code>'model_id'</code>
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>na_action</code>
</td>
<td style="text-align:left;">
Method to handle <code>NA</code> values
</td>
<td style="text-align:left;">
<code>'drop', 'worst', 'average'</code>
</td>
<td style="text-align:left;">
<code>'drop'</code>
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>fun</code>
</td>
<td style="text-align:left;">
Function to summarize importance scores
</td>
<td style="text-align:left;">
summary function
</td>
<td style="text-align:left;">
<code>mean</code>
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>...</code>
</td>
<td style="text-align:left;">
Optional arguments for <code>fun</code>
</td>
<td style="text-align:left;">
depends on <code>fun</code>
</td>
<td style="text-align:left;">
N/A
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section level2">
<h2 id="sec:implementation-and-availability">Implementation and availability<a class="anchor" aria-label="anchor" href="#sec:implementation-and-availability"></a>
</h2>
<p>This package is implemented in <span class="proglang">R</span> and
distributed via CRAN and GitHhub. We conducted unit tests using the
<span class="pkg">testthat</span> package <span class="citation">(Wickham 2011)</span> to ensure that all functions work
correctly as expected, including those used internally. We also
performed continuous integration testing using GitHub Actions to
maintain functionality across platforms, including Windows, macOS, and
Linux. Integrated GitHub Action, we used package to maintain consistent
code style, code quality, and detection potential issues. All code
changes were systematically reviewed by fellow team members, and this
enhanced reliability.</p>
</div>
<div class="section level2">
<h2 id="sec:examples">Examples<a class="anchor" aria-label="anchor" href="#sec:examples"></a>
</h2>
<p>The examples in this section illustrate the use of the
<code><a href="../reference/model_importance.html">model_importance()</a></code> function to evaluate the importance of
component models within an ensemble, using various combinations of the
arguments described in . We use some example forecast and target data
from the <span class="pkg">hubExamples</span> package, which provides
sample datasets for multiple modeling hubs in the hubverse format.</p>
<div class="section level3">
<h3 id="sec:example-data">Example data<a class="anchor" aria-label="anchor" href="#sec:example-data"></a>
</h3>
<p>The forecast data used here contains forecasts of weekly incident
influenza hospitalizations in the US for Massachusetts (FIPS code 25)
and Texas (FIPS code 48), generated on November 19, 2022. These
forecasts are for two target end dates, November 26, 2022 (horizon 1),
and December 10, 2022 (horizon 3), and were produced by three models:
‚ÄòFlusight-baseline‚Äô, ‚ÄòMOBS-GLEAM_FLUH‚Äô, and ‚ÄòPSI-DICE‚Äô. The output type
is <code>median</code> and the <code>output_type_id</code> column has
<code>NA</code>s as no further specification is required for this output
type. We have modified the example data slightly: some forecasts have
been removed to demonstrate the handling of missing values. Therefore,
MOBS-GLEAM_FLUH‚Äôs forecast for Massachusetts on November 26, 2022, and
PSI-DICE‚Äôs forecast for Texas on December 10, 2022, are missing.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">forecast_data</span></span></code></pre></div>
<pre><code><span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 10 √ó 9</span></span></span>
<span><span class="co">#&gt;    model_id          reference_date target          horizon location target_end_date output_type output_type_id value</span></span>
<span><span class="co">#&gt;    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>             <span style="color: #949494; font-style: italic;">&lt;date&gt;</span>         <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>             <span style="color: #949494; font-style: italic;">&lt;int&gt;</span> <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>    <span style="color: #949494; font-style: italic;">&lt;date&gt;</span>          <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>       <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>          <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 1</span> Flusight-baseline 2022-11-19     wk inc flu hosp       1 25       2022-11-26      median      <span style="color: #BB0000;">NA</span>                51</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 2</span> Flusight-baseline 2022-11-19     wk inc flu hosp       3 25       2022-12-10      median      <span style="color: #BB0000;">NA</span>                51</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 3</span> Flusight-baseline 2022-11-19     wk inc flu hosp       1 48       2022-11-26      median      <span style="color: #BB0000;">NA</span>              <span style="text-decoration: underline;">1</span>052</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 4</span> Flusight-baseline 2022-11-19     wk inc flu hosp       3 48       2022-12-10      median      <span style="color: #BB0000;">NA</span>              <span style="text-decoration: underline;">1</span>052</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 5</span> MOBS-GLEAM_FLUH   2022-11-19     wk inc flu hosp       3 25       2022-12-10      median      <span style="color: #BB0000;">NA</span>                43</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 6</span> MOBS-GLEAM_FLUH   2022-11-19     wk inc flu hosp       1 48       2022-11-26      median      <span style="color: #BB0000;">NA</span>              <span style="text-decoration: underline;">1</span>072</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 7</span> MOBS-GLEAM_FLUH   2022-11-19     wk inc flu hosp       3 48       2022-12-10      median      <span style="color: #BB0000;">NA</span>               688</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 8</span> PSI-DICE          2022-11-19     wk inc flu hosp       1 25       2022-11-26      median      <span style="color: #BB0000;">NA</span>                90</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 9</span> PSI-DICE          2022-11-19     wk inc flu hosp       3 25       2022-12-10      median      <span style="color: #BB0000;">NA</span>               159</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">10</span> PSI-DICE          2022-11-19     wk inc flu hosp       1 48       2022-11-26      median      <span style="color: #BB0000;">NA</span>              <span style="text-decoration: underline;">1</span>226</span></span></code></pre>
<p>The corresponding target data contains the observed hospitalization
counts for these dates and locations.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">target_data</span></span></code></pre></div>
<pre><code><span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 4 √ó 4</span></span></span>
<span><span class="co">#&gt;   target_end_date target          location oracle_value</span></span>
<span><span class="co">#&gt;   <span style="color: #949494; font-style: italic;">&lt;date&gt;</span>          <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>           <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>           <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">1</span> 2022-11-26      wk inc flu hosp 25                221</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">2</span> 2022-11-26      wk inc flu hosp 48               <span style="text-decoration: underline;">1</span>929</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">3</span> 2022-12-10      wk inc flu hosp 25                578</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">4</span> 2022-12-10      wk inc flu hosp 48               <span style="text-decoration: underline;">1</span>781</span></span></code></pre>
<div class="figure">
<img src="modelimportance-article_files/figure-html/fig-example-median-lomo-1.png" class="r-plt" alt="Figure 3: Plot of three point forecasts (median) and the eventually observed values from the `forecast_data` and `target_data`  for weekly incident influenza hospitalizations in Massachusetts (FIPS code 25) and Texas (FIPS code 48). Colored dots indicate the forecasts by three models, generated on November 19, 2022. Open black circles indicate the eventually observed values. MOBS-GLEAM_FLUH's forecast for Massachusetts on November 26, 2022, and PSI-DICE's forecast for Texas on December 10, 2022, are missing." width="672"><p class="caption">
Figure 3: Plot of three point forecasts (median) and the eventually
observed values from the <code>forecast_data</code> and
<code>target_data</code> for weekly incident influenza hospitalizations
in Massachusetts (FIPS code 25) and Texas (FIPS code 48). Colored dots
indicate the forecasts by three models, generated on November 19, 2022.
Open black circles indicate the eventually observed values.
MOBS-GLEAM_FLUH‚Äôs forecast for Massachusetts on November 26, 2022, and
PSI-DICE‚Äôs forecast for Texas on December 10, 2022, are missing.
</p>
</div>
<p>As expected, prediction errors increase at longer horizons due to
greater uncertainty, with forecasts for December 10, 2022, showing
larger deviations from the observed values compared to those for
November 26, 2022. Additionally, the forecasts for Massachusetts are
relatively more accurate compared to those for Texas, which tend to have
higher errors (<span class="citation">(<strong>fig-example-median-lomo?</strong>)</span>).</p>
</div>
<div class="section level3">
<h3 id="sec:example-lomo">Evaluation using LOMO algorithm<a class="anchor" aria-label="anchor" href="#sec:example-lomo"></a>
</h3>
<p>We quantify the contribution of each model within the ensemble using
the <code><a href="../reference/model_importance.html">model_importance()</a></code> function. The following code
evaluates the importance of each ensemble member in the simple mean
ensemble using the LOMO algorithm.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/model_importance.html">model_importance</a></span><span class="op">(</span></span>
<span>  forecast_data <span class="op">=</span> <span class="va">forecast_data</span>,</span>
<span>  oracle_output_data <span class="op">=</span> <span class="va">target_data</span>,</span>
<span>  ensemble_fun <span class="op">=</span> <span class="st">"simple_ensemble"</span>,</span>
<span>  importance_algorithm <span class="op">=</span> <span class="st">"lomo"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>This call generates both the result and informative messages,
summarizing the input data, including the number of dates on which
forecasts were produced and the number of models with ids as
follows.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>Forecasts from <span class="dv">2022-11-19</span> to <span class="dv">2022-11-19</span> (a total of <span class="dv">1</span> forecast <span class="fu">date</span>(s)).</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>The available model IDs are<span class="sc">:</span></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>     Flusight<span class="sc">-</span>baseline</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>     MOBS<span class="sc">-</span>GLEAM_FLUH</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>     PSI<span class="sc">-</span>DICE </span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>(a total of <span class="dv">3</span> models)</span></code></pre></div>
<p>The function output is a data frame containing model ids and their
corresponding importance scores for each prediction task, along with
task id columns.</p>
<pre><code><span><span class="co">#&gt;             model_id reference_date          target horizon location target_end_date output_type importance</span></span>
<span><span class="co">#&gt; 1  Flusight-baseline     2022-11-19 wk inc flu hosp       1       25      2022-11-26      median  -19.50000</span></span>
<span><span class="co">#&gt; 2    MOBS-GLEAM_FLUH     2022-11-19 wk inc flu hosp       1       25      2022-11-26      median         NA</span></span>
<span><span class="co">#&gt; 3           PSI-DICE     2022-11-19 wk inc flu hosp       1       25      2022-11-26      median   19.50000</span></span>
<span><span class="co">#&gt; 4  Flusight-baseline     2022-11-19 wk inc flu hosp       1       48      2022-11-26      median  -32.33333</span></span>
<span><span class="co">#&gt; 5    MOBS-GLEAM_FLUH     2022-11-19 wk inc flu hosp       1       48      2022-11-26      median  -22.33333</span></span>
<span><span class="co">#&gt; 6           PSI-DICE     2022-11-19 wk inc flu hosp       1       48      2022-11-26      median   54.66667</span></span>
<span><span class="co">#&gt; 7  Flusight-baseline     2022-11-19 wk inc flu hosp       3       25      2022-12-10      median  -16.66667</span></span>
<span><span class="co">#&gt; 8    MOBS-GLEAM_FLUH     2022-11-19 wk inc flu hosp       3       25      2022-12-10      median  -20.66667</span></span>
<span><span class="co">#&gt; 9           PSI-DICE     2022-11-19 wk inc flu hosp       3       25      2022-12-10      median   37.33333</span></span>
<span><span class="co">#&gt; 10 Flusight-baseline     2022-11-19 wk inc flu hosp       3       48      2022-12-10      median  182.00000</span></span>
<span><span class="co">#&gt; 11   MOBS-GLEAM_FLUH     2022-11-19 wk inc flu hosp       3       48      2022-12-10      median -182.00000</span></span>
<span><span class="co">#&gt; 12          PSI-DICE     2022-11-19 wk inc flu hosp       3       48      2022-12-10      median         NA</span></span></code></pre>
<p>For models that missed forecasts for certain tasks, <code>NA</code>
values were assigned in the importance column for those tasks.</p>
<p>We summarize the importance scores for each model by averaging across
all tasks. <code>NA</code> values are removed during the averaging
process by setting the <code>na_action</code> argument to
<code>"drop"</code>.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">scores_lomo</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/model_importance.html">model_importance</a></span><span class="op">(</span></span>
<span>  forecast_data <span class="op">=</span> <span class="va">forecast_data</span>,</span>
<span>  oracle_output_data <span class="op">=</span> <span class="va">target_data</span>,</span>
<span>  ensemble_fun <span class="op">=</span> <span class="st">"simple_ensemble"</span>,</span>
<span>  importance_algorithm <span class="op">=</span> <span class="st">"lomo"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="../reference/model_importance_summary.html">model_importance_summary</a></span><span class="op">(</span></span>
<span>  importance_scores <span class="op">=</span> <span class="va">scores_lomo</span>,</span>
<span>  by <span class="op">=</span> <span class="st">"model_id"</span>,</span>
<span>  na_action <span class="op">=</span> <span class="st">"drop"</span>,</span>
<span>  fun <span class="op">=</span> <span class="va">mean</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 3 √ó 2</span></span></span>
<span><span class="co">#&gt;   model_id          importance_score_mean</span></span>
<span><span class="co">#&gt;   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>                             <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">1</span> PSI-DICE                           37.2</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">2</span> Flusight-baseline                  28.4</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">3</span> MOBS-GLEAM_FLUH                   -<span style="color: #BB0000;">75</span></span></span></code></pre></div>
<p>The results show that the model ‚ÄòPSI-DICE‚Äô has the highest importance
score, followed by ‚ÄòFlusight-baseline‚Äô and ‚ÄòMOBS-GLEAM_FLUH‚Äô. That is,
‚ÄòPSI-DICE‚Äô contributes the most to improving the ensemble‚Äôs predictive
performance, whereas ‚ÄòMOBS-GLEAM_FLUH‚Äô, which has a negative score,
detracts from the ensemble‚Äôs performance. The low importance score of
‚ÄòMOBS-GLEAM_FLUH‚Äô is mainly due to a substantially larger prediction
error for Texas on the target end date of December 10, 2022, compared to
other models, while its missing forecast for Massachusetts for November
26, 2022, was not factored into the evaluation. This single large error
significantly affected its contribution score.</p>
<p>Another approach to handling missing values is to use the
<code>"worst"</code> option for <code>na_action</code>, which replaces
missing values with the worst (i.e., minimum) score among the other
models for the same task.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/model_importance_summary.html">model_importance_summary</a></span><span class="op">(</span></span>
<span>  importance_scores <span class="op">=</span> <span class="va">scores_lomo</span>,</span>
<span>  by <span class="op">=</span> <span class="st">"model_id"</span>,</span>
<span>  na_action <span class="op">=</span> <span class="st">"worst"</span>,</span>
<span>  fun <span class="op">=</span> <span class="va">mean</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 3 √ó 2</span></span></span>
<span><span class="co">#&gt;   model_id          importance_score_mean</span></span>
<span><span class="co">#&gt;   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>                             <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">1</span> Flusight-baseline                  28.4</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">2</span> PSI-DICE                          -<span style="color: #BB0000;">17.6</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">3</span> MOBS-GLEAM_FLUH                   -<span style="color: #BB0000;">61.1</span></span></span></code></pre></div>
<p>The results show that the importance scores of ‚ÄòFlusight-baseline‚Äô is
unchanged because it has no missing forecast. We observe that the
importance score of ‚ÄòPSI-DICE‚Äô, which was previously positive, has now
decreased to a negative value when compared to the evaluation using the
<code>"drop"</code> option for <code>na_action</code>. Moreover,
‚ÄòMOBS-GLEAM_FLUH‚Äô still ranks the lowest, but the importance score has
increased. This change is related to the varying forecast accuracy
across different tasks. For the target end date of November 26, 2022, in
Massachusetts, most forecasts are relatively accurate. Thus, even if the
‚ÄòMOBS-GLEAM_FLUH‚Äô is assigned the worst value of importance score for
its missing forecast, including this value in the averaging is not
detrimental to the overall importance metric; rather, it is more
beneficial than excluding it. In contrast, for the target end date of
December 10, 2022, in Texas, the forecasts have much larger errors
across the board, and assigning the worst value of importance score to
the missing forecast of ‚ÄòPSI-DICE‚Äô in this task has a detrimental effect
on averaging importance scores. This is because the scale of the
importance scores is influenced by the magnitude of the prediction
errors: for tasks with small errors, the scores remain moderate, while
tasks with large errors can yield importance scores of much greater
magnitude.</p>
<p>It is also possible to impute the missing scores with intermediate
values by assigning the average importance scores of other models in the
same task. This strategy may offer a more balanced trade-off by
mitigating the influence of the missing data without overly penalizing
or overlooking them.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/model_importance_summary.html">model_importance_summary</a></span><span class="op">(</span></span>
<span>  <span class="va">scores_lomo</span>, by <span class="op">=</span> <span class="st">"model_id"</span>, na_action <span class="op">=</span> <span class="st">"average"</span>, fun <span class="op">=</span> <span class="va">mean</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 3 √ó 2</span></span></span>
<span><span class="co">#&gt;   model_id          importance_score_mean</span></span>
<span><span class="co">#&gt;   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>                             <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">1</span> Flusight-baseline                  28.4</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">2</span> PSI-DICE                           27.9</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">3</span> MOBS-GLEAM_FLUH                   -<span style="color: #BB0000;">56.2</span></span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="sec:example-lasomo">Evaluation using LASOMO algorithm<a class="anchor" aria-label="anchor" href="#sec:example-lasomo"></a>
</h3>
<p>Now we demonstrate the use of the LASOMO algorithm in the evaluation
of model importance. As we explored the difference of
<code>na_action</code> options in the previous LOMO example (), we focus
on options for <code>subset_wt</code>, which specifies how weights are
assigned to subsets of models when calculating importance scores, with
<code>na_action</code> fixed to <code>"drop"</code>.</p>
<p>The following code and corresponding outputs illustrate the
evaluation using each weighting scheme.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">scores_lasomo_eq</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/model_importance.html">model_importance</a></span><span class="op">(</span></span>
<span>  forecast_data <span class="op">=</span> <span class="va">forecast_data</span>,</span>
<span>  oracle_output_data <span class="op">=</span> <span class="va">target_data</span>,</span>
<span>  ensemble_fun <span class="op">=</span> <span class="st">"simple_ensemble"</span>,</span>
<span>  importance_algorithm <span class="op">=</span> <span class="st">"lasomo"</span>,</span>
<span>  subset_wt <span class="op">=</span> <span class="st">"equal"</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/model_importance_summary.html">model_importance_summary</a></span><span class="op">(</span></span>
<span>  <span class="va">scores_lasomo_eq</span>, by <span class="op">=</span> <span class="st">"model_id"</span>, na_action <span class="op">=</span> <span class="st">"drop"</span>, fun <span class="op">=</span> <span class="va">mean</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 3 √ó 2</span></span></span>
<span><span class="co">#&gt;   model_id          importance_score_mean</span></span>
<span><span class="co">#&gt;   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>                             <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">1</span> PSI-DICE                           47.4</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">2</span> Flusight-baseline                  24.3</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">3</span> MOBS-GLEAM_FLUH                   -<span style="color: #BB0000;">79.8</span></span></span></code></pre></div>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">scores_lasomo_perm</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/model_importance.html">model_importance</a></span><span class="op">(</span></span>
<span>  forecast_data <span class="op">=</span> <span class="va">forecast_data</span>,</span>
<span>  oracle_output_data <span class="op">=</span> <span class="va">target_data</span>,</span>
<span>  ensemble_fun <span class="op">=</span> <span class="st">"simple_ensemble"</span>,</span>
<span>  importance_algorithm <span class="op">=</span> <span class="st">"lasomo"</span>,</span>
<span>  subset_wt <span class="op">=</span> <span class="st">"perm_based"</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/model_importance_summary.html">model_importance_summary</a></span><span class="op">(</span></span>
<span>  <span class="va">scores_lasomo_perm</span>, by <span class="op">=</span> <span class="st">"model_id"</span>, na_action <span class="op">=</span> <span class="st">"drop"</span>, fun <span class="op">=</span> <span class="va">mean</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 3 √ó 2</span></span></span>
<span><span class="co">#&gt;   model_id          importance_score_mean</span></span>
<span><span class="co">#&gt;   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>                             <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">1</span> PSI-DICE                           44.8</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">2</span> Flusight-baseline                  25.3</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">3</span> MOBS-GLEAM_FLUH                   -<span style="color: #BB0000;">78.6</span></span></span></code></pre></div>
<p>In this example, there are only three models
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">n = 3</annotation></semantics></math>),
and the weights do not differ significantly between the two weighting
schemes. Therefore, the resulting outputs show little difference.
However, in general, with a larger number of models, the two weighting
schemes may yield quite different importance scores for each model.</p>
<p>In this section, we explored each component model‚Äôs contribution to
the ensemble accuracy with only three models. An extensive application
in more complex scenarios with a larger number of models can be found in
<span class="citation">Kim, Ray, and Reich (2024)</span>.</p>
</div>
</div>
<div class="section level2">
<h2 id="sec:discussion">Summary and discussion<a class="anchor" aria-label="anchor" href="#sec:discussion"></a>
</h2>
<p>Multi-model ensemble forecasts often provide better accuracy and
robustness than single models, and are widely used in decision-making
and policy planning across various domains. The contribution of each
component model to the accuracy of the ensemble depends on its own
unique characteristics. The <span class="pkg">modelimportance</span>
package enables the quantification of the value that each component
model adds to the ensemble performance in different evaluation
contexts.</p>
<p>The primary function of the package is
<code><a href="../reference/model_importance.html">model_importance()</a></code>, which returns a data frame with
component models and their importance metrics. Users can choose the
various ensemble methods to apply and model importance algorithm between
LOMO and LASOMO. Additionally, customizable options are available for
handling missing values. These features enable the package to serve as a
versatile tool to aid collaborative efforts to construct an effective
ensemble model across a wide range of forecasting tasks. We note that
unit testing with continuous integration ensures the reliability of all
functions and the overall quality of code across multiple platforms.</p>
<p>There is a room to enhance the current version of this package.
Although this package supports four different output types (‚Äòmean‚Äô,
‚Äòmedian‚Äô, ‚Äòquantile, and ‚Äôpmf‚Äô), other output types are widely used in
practice. For example, ‚Äòsample‚Äô output type is commonly used in the US
Flu Scenario Modeling Hub <span class="citation">(Flu Scenario Modeling
Hub 2024)</span>. This format includes multiple simulated values
(samples) from the forecast distribution. The
<code>output_type_id</code> is specified for each sample, which
typically indexes the samples or indicates their source, depending on
the context. The package can be extended to support this output type,
which is under consideration for future releases. These extensions would
aim to broaden the scope of applications in real-world forecasting
tasks.</p>
</div>
<div class="section level2 unnumbered">
<h2 class="unnumbered" id="acknowledgements">Acknowledgements<a class="anchor" aria-label="anchor" href="#acknowledgements"></a>
</h2>
<p>We acknowledge Zhian N. Kamvar for debugging and resolving coding
issues while developing the package. We are also grateful to Mattew
Cornell for his advice on unit testing, which greatly helped us improve
the structure and testing our code with a solid understanding of unit
testing practices.</p>
</div>
<div class="section level2 unnumbered">
<h2 class="unnumbered" id="appendix">Appendix<a class="anchor" aria-label="anchor" href="#appendix"></a>
</h2>
<div class="section level3">
<h3 id="sec:appendix">Weights for subsets in LASOMO<a class="anchor" aria-label="anchor" href="#sec:appendix"></a>
</h3>
<p>In the LASOMO algorithm, two weighting schemes are available for
subsets of models in the calculation of model importance scores: equal
weights and permutation-based weights.</p>
<p>Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
be the total number of models and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
be the size of a subset that does not include the model being evaluated.
The formulas for the weights under each scheme are as follows:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msup><mi>w</mi><mtext mathvariant="normal">eq</mtext></msup></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mn>1</mn><mrow><msup><mn>2</mn><mrow><mi>n</mi><mo>‚àí</mo><mn>1</mn></mrow></msup><mo>‚àí</mo><mn>1</mn></mrow></mfrac><mo>,</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msup><mi>w</mi><mtext mathvariant="normal">perm</mtext></msup></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mn>1</mn><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac linethickness="0"><mrow><mi>n</mi><mo>‚àí</mo><mn>1</mn></mrow><mi>k</mi></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>,</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
w^{\text{eq}} &amp;= \frac{1}{2^{n-1}-1}, \\
w^{\text{perm}} &amp;= \frac{1}{(n-1)\binom{n-1}{k}},
\end{align*}</annotation></semantics></math> where the superscripts ‚Äúeq‚Äù
and ‚Äúperm‚Äù denote the equal and permutation-based weighting schemes,
respectively. The maximum weight under the permutation-based scheme
occurs when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mi>n</mi><mo>‚àí</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k=n-1</annotation></semantics></math>,
which is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>/</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">{1}/{(n-1)}</annotation></semantics></math>.
The minimum weight occurs when the subset size is around
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">{(n-1)}/{2}</annotation></semantics></math>
(i.e.,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mo stretchy="false" form="prefix">‚åä</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mn>2</mn><mo stretchy="false" form="postfix">‚åã</mo></mrow><annotation encoding="application/x-tex">k=\lfloor (n-1)/2 \rfloor</annotation></semantics></math>),
which is approximately
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><msqrt><mrow><mi>œÄ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mn>2</mn></mrow></msqrt><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><msup><mn>2</mn><mrow><mi>n</mi><mo>‚àí</mo><mn>1</mn></mrow></msup></mrow></mfrac><annotation encoding="application/x-tex">\displaystyle\frac{\sqrt{\pi(n-1)/2}}{(n-1)2^{n-1}}</annotation></semantics></math>
by Stirling‚Äôs approximation.</p>
<p>Given a fixed mid-sized subset, as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
increases, the weight assigned to this subset under the equal weighting
scheme decreases at a rate of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mi>/</mi><msup><mn>2</mn><mi>n</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O({1}/{2^n})</annotation></semantics></math>,
while under the permutation-based scheme, it decreases at a much faster
rate of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mi>/</mi><mrow><mo stretchy="true" form="prefix">(</mo><msqrt><mi>n</mi></msqrt><mspace width="0.167em"></mspace><msup><mn>2</mn><mi>n</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O({1}/({\sqrt{n}\,2^n}))</annotation></semantics></math>.
This indicates that as the number of models grows, that mid-sized subset
becomes significantly less influential in determining model importance
scores when using the permutation-based weighting scheme compared to the
equal weighting scheme.</p>
<p>On the other hand, for subsets of extreme sizes (e.g.,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k=1</annotation></semantics></math>
or
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mi>n</mi><mo>‚àí</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k=n-1</annotation></semantics></math>),
the weights under permutation-based weighting scheme decrease only at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mi>/</mi><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O({1}/{n})</annotation></semantics></math>,
much slower under the equal weighting scheme. This implies that in
scenarios with a large number of models, the contributions of these
extreme-sized subsets play a relatively larger role in the calculation
of model importance scores when using permutation-based weights compared
to the equal weighting approach.</p>
</div>
</div>
<div class="section level2 unnumbered">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-bosse2022evaluating" class="csl-entry">
Bosse, Nikos I, Hugo Gruson, Anne Cori, Edwin van Leeuwen, Sebastian
Funk, and Sam Abbott. 2022. <span>‚Äú<span class="nocase">Evaluating
forecasts with scoringutils in R</span>.‚Äù</span> <em>arXiv Preprint
arXiv:2205.07090</em>.
</div>
<div id="ref-bracher_evaluating_2021" class="csl-entry">
Bracher, Johannes, Evan L. Ray, Tilmann Gneiting, and Nicholas G. Reich.
2021. <span>‚ÄúEvaluating Epidemic Forecasts in an Interval
Format.‚Äù</span> <em>PLOS Computational Biology</em> 17 (2): e1008618. <a href="https://doi.org/10.1371/journal.pcbi.1008618" class="external-link">https://doi.org/10.1371/journal.pcbi.1008618</a>.
</div>
<div id="ref-brooks2018nonmechanistic" class="csl-entry">
Brooks, Logan C, David C Farrow, Sangwon Hyun, Ryan J Tibshirani, and
Roni Rosenfeld. 2018. <span>‚ÄúNonmechanistic Forecasts of Seasonal
Influenza with Iterative One-Week-Ahead Distributions.‚Äù</span> <em>PLOS
Computational Biology</em> 14 (6): e1006134.
</div>
<div id="ref-Rpackage-xgboost" class="csl-entry">
Chen, Tianqi, Tong He, Michael Benesty, Vadim Khotilovich, Yuan Tang,
Hyunsu Cho, Kailong Chen, et al. 2024. <em><span class="nocase">xgboost:
Extreme Gradient Boosting</span></em>. <a href="https://CRAN.R-project.org/package=xgboost" class="external-link">https://CRAN.R-project.org/package=xgboost</a>.
</div>
<div id="ref-hubverse_docs" class="csl-entry">
Consortium of Infectious Disease Modeling Hubs. 2024. <span>‚ÄúThe
Hubverse: Open Tools for Collaborative Forecasting.‚Äù</span> <a href="https://hubverse.io" class="external-link">https://hubverse.io</a>.
</div>
<div id="ref-cramer2022united" class="csl-entry">
Cramer, Estee Y, Yuxin Huang, Yijin Wang, Evan L Ray, Matthew Cornell,
Johannes Bracher, Andrea Brennen, et al. 2022. <span>‚Äú<span class="nocase">The United States COVID-19 forecast hub
dataset</span>.‚Äù</span> <em>Scientific Data</em> 9 (1): 462.
</div>
<div id="ref-FluSMH" class="csl-entry">
Flu Scenario Modeling Hub. 2024. <span>‚ÄúFlu Scenario Modeling
Hub.‚Äù</span> <a href="https://fluscenariomodelinghub.org/index.html" class="external-link">https://fluscenariomodelinghub.org/index.html</a>.
</div>
<div id="ref-gneiting2005weather" class="csl-entry">
Gneiting, Tilmann, and Adrian E Raftery. 2005. <span>‚ÄúWeather
Forecasting with Ensemble Methods.‚Äù</span> <em>Science</em> 310 (5746):
248‚Äì49.
</div>
<div id="ref-gneiting_strictly_2007" class="csl-entry">
‚Äî‚Äî‚Äî. 2007. <span>‚Äú<span class="nocase">Strictly Proper Scoring Rules,
Prediction, and Estimation</span>.‚Äù</span> <em>Journal of the American
Statistical Association</em> 102 (477): 359‚Äì78. <a href="https://doi.org/10.1198/016214506000001437" class="external-link">https://doi.org/10.1198/016214506000001437</a>.
</div>
<div id="ref-Rpackage-Metrics" class="csl-entry">
Hamner, Ben, and Michael Frasco. 2018. <em><span class="nocase">Metrics:
Evaluation Metrics for Machine Learning</span></em>. <a href="https://CRAN.R-project.org/package=Metrics" class="external-link">https://CRAN.R-project.org/package=Metrics</a>.
</div>
<div id="ref-hastie01statisticallearning" class="csl-entry">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2001. <em>The
Elements of Statistical Learning</em>. Springer Series in Statistics.
New York, NY, USA: Springer New York Inc.
</div>
<div id="ref-math11041054" class="csl-entry">
He, Kaijian, Qian Yang, Lei Ji, Jingcheng Pan, and Yingchao Zou. 2023.
<span>‚Äú<span class="nocase">Financial Time Series Forecasting with the
Deep Learning Ensemble Model</span>.‚Äù</span> <em>Mathematics</em> 11
(4). <a href="https://doi.org/10.3390/math11041054" class="external-link">https://doi.org/10.3390/math11041054</a>.
</div>
<div id="ref-Guerra_2020" class="csl-entry">
Jordan A. Guerra, Sophie A. Murray, D. Shaun Bloomfield, and Peter T.
Gallagher. 2020. <span>‚ÄúEnsemble Forecasting of Major Solar Flares:
Methods for Combining Models.‚Äù</span> <em>J. Space Weather Space
Clim.</em> 10: 38. <a href="https://doi.org/10.1051/swsc/2020042" class="external-link">https://doi.org/10.1051/swsc/2020042</a>.
</div>
<div id="ref-Rpackage-scoringRules" class="csl-entry">
Jordan, Alexander, Fabian Kr√ºger, and Sebastian Lerch. 2019.
<span>‚ÄúEvaluating Probabilistic Forecasts with <span class="nocase">scoringRules</span>.‚Äù</span> <em>Journal of Statistical
Software</em> 90 (12): 1‚Äì37. <a href="https://doi.org/10.18637/jss.v090.i12" class="external-link">https://doi.org/10.18637/jss.v090.i12</a>.
</div>
<div id="ref-kim2024" class="csl-entry">
Kim, Minsu, Evan L. Ray, and Nicholas G. Reich. 2024. <span>‚Äú<span class="nocase">Beyond forecast leaderboards: Measuring individual model
importance based on contribution to ensemble accuracy</span>.‚Äù</span> <a href="https://arxiv.org/abs/2412.08916" class="external-link">https://arxiv.org/abs/2412.08916</a>.
</div>
<div id="ref-Rpackage-hubUtils" class="csl-entry">
Krystalli, Anna, and Li Shandross. 2025. <em>hubUtils: Core ‚ÄôHubverse‚Äô
Utilities</em>. <a href="https://github.com/hubverse-org/hubUtils" class="external-link">https://github.com/hubverse-org/hubUtils</a>.
</div>
<div id="ref-Rpackage-caret" class="csl-entry">
Kuhn, Max. 2008. <span>‚Äú<span class="nocase">Building Predictive Models
in <span>R</span> Using the caret Package</span>.‚Äù</span> <em>Journal of
Statistical Software</em> 28 (5): 1‚Äì26. <a href="https://doi.org/10.18637/jss.v028.i05" class="external-link">https://doi.org/10.18637/jss.v028.i05</a>.
</div>
<div id="ref-Rpackage-randomForest" class="csl-entry">
Liaw, Andy, and Matthew Wiener. 2002. <span>‚Äú<span class="nocase">Classification and Regression by
randomForest</span>.‚Äù</span> <em>R News</em> 2 (3): 18‚Äì22. <a href="https://CRAN.R-project.org/doc/Rnews/" class="external-link">https://CRAN.R-project.org/doc/Rnews/</a>.
</div>
<div id="ref-lutz_applying_2019" class="csl-entry">
Lutz, Chelsea S., Mimi P. Huynh, Monica Schroeder, Sophia Anyatonwu, F.
Scott Dahlgren, Gregory Danyluk, Danielle Fernandez, et al. 2019.
<span>‚ÄúApplying Infectious Disease Forecasting to Public Health: A Path
Forward Using Influenza Forecasting Examples.‚Äù</span>
<em><span>BMC</span> Public Health</em> 19 (1): 1659. <a href="https://doi.org/10.1186/s12889-019-7966-8" class="external-link">https://doi.org/10.1186/s12889-019-7966-8</a>.
</div>
<div id="ref-Rpackage-fable" class="csl-entry">
O‚ÄôHara-Wild, Mitchell, Rob Hyndman, and Earo Wang. 2024. <em><span class="nocase">fable: Forecasting Models for Tidy Time
Series</span></em>. <a href="https://fable.tidyverts.org" class="external-link">https://fable.tidyverts.org</a>.
</div>
<div id="ref-ray_prediction_2018" class="csl-entry">
Ray, Evan L., and Nicholas G. Reich. 2018. <span>‚ÄúPrediction of
Infectious Disease Epidemics via Weighted Density Ensembles.‚Äù</span>
<em>PLOS Computational Biology</em> 14 (2): e1005910. <a href="https://doi.org/10.1371/journal.pcbi.1005910" class="external-link">https://doi.org/10.1371/journal.pcbi.1005910</a>.
</div>
<div id="ref-Rpackage-hubExamples" class="csl-entry">
Ray, Evan L, Becky Sweger, and Lucie Contamin. 2025. <em>hubExamples:
Example Hub Data</em>. <a href="https://github.com/hubverse-org/hubExamples" class="external-link">https://github.com/hubverse-org/hubExamples</a>.
</div>
<div id="ref-reich_accuracy_2019" class="csl-entry">
Reich, Nicholas G., Craig J. McGowan, Teresa K. Yamana, Abhinav Tushar,
Evan L. Ray, Dave Osthus, Sasikiran Kandula, et al. 2019.
<span>‚ÄúAccuracy of Real-Time Multi-Model Ensemble Forecasts for Seasonal
Influenza in the <span>U</span>.<span>S</span>.‚Äù</span> <em>PLOS
Computational Biology</em> 15 (11): e1007486. <a href="https://doi.org/10.1371/journal.pcbi.1007486" class="external-link">https://doi.org/10.1371/journal.pcbi.1007486</a>.
</div>
<div id="ref-Rpackage-hubEvals" class="csl-entry">
Reich, Nicholas, Evan Ray, Nikos Bosse, Matthew Cornell, Becky Sweger,
and Kimberlyn Roosa. 2025. <em>hubEvals: Basic Tools for Scoring
Hubverse Forecasts</em>. <a href="https://github.com/hubverse-org/hubEvals" class="external-link">https://github.com/hubverse-org/hubEvals</a>.
</div>
<div id="ref-Rpackage-gbm" class="csl-entry">
Ridgeway, Greg, and GBM Developers. 2024. <em><span class="nocase">gbm:
Generalized Boosted Regression Models</span></em>. <a href="https://CRAN.R-project.org/package=gbm" class="external-link">https://CRAN.R-project.org/package=gbm</a>.
</div>
<div id="ref-Shandross2024" class="csl-entry">
Shandross, Li, Emily Howerton, Lucie Contamin, Harry Hochheiser, Anna
Krystalli, Consortium of Infectious Disease Modeling Hubs, Nicholas G
Reich, and Evan L Ray. 2024. <span>‚Äú<span class="nocase">Multi-model
ensembles in infectious disease and public health: Methods,
interpretation, and implementation in R</span>.‚Äù</span>
<em>medRxiv</em>.
</div>
<div id="ref-Rpackage-hubEnsembles" class="csl-entry">
Shandross, Li, Emily Howerton, and Evan L Ray. 2025. <em>hubEnsembles:
Ensemble Methods for Combining Hub Model Outputs</em>. <a href="https://github.com/hubverse-org/hubEnsembles" class="external-link">https://github.com/hubverse-org/hubEnsembles</a>.
</div>
<div id="ref-Shapley1953" class="csl-entry">
Shapley, Lloyd S. 1953. <span>‚Äú<span class="nocase">A value for n-person
games</span>.‚Äù</span> <em>Contribution to the Theory of Games</em> 2.
</div>
<div id="ref-SUN2020101160" class="csl-entry">
Sun, Shaolong, Shouyang Wang, and Yunjie Wei. 2020. <span>‚ÄúA New
Ensemble Deep Learning Approach for Exchange Rates Forecasting and
Trading.‚Äù</span> <em>Advanced Engineering Informatics</em> 46: 101160.
https://doi.org/<a href="https://doi.org/10.1016/j.aei.2020.101160" class="external-link">https://doi.org/10.1016/j.aei.2020.101160</a>.
</div>
<div id="ref-viboud_rapidd_2018" class="csl-entry">
Viboud, C√©cile, Kaiyuan Sun, Robert Gaffey, Marco Ajelli, Laura
Fumanelli, Stefano Merler, Qian Zhang, Gerardo Chowell, Lone Simonsen,
and Alessandro Vespignani. 2018. <span>‚ÄúThe <span>RAPIDD</span> Ebola
Forecasting Challenge: Synthesis and Lessons Learnt.‚Äù</span>
<em>Epidemics</em>, The <span>RAPIDD</span> ebola forecasting challenge,
22 (March): 13‚Äì21. <a href="https://doi.org/10.1016/j.epidem.2017.08.002" class="external-link">https://doi.org/10.1016/j.epidem.2017.08.002</a>.
</div>
<div id="ref-Rpackage-testthat" class="csl-entry">
Wickham, Hadley. 2011. <span>‚ÄúTestthat: Get Started with
Testing.‚Äù</span> <em>The R Journal</em> 3: 5‚Äì10. <a href="https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf" class="external-link">https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf</a>.
</div>
<div id="ref-Rpackage-MLmetrics" class="csl-entry">
Yan, Yachen. 2024. <em><span>MLmetrics: Machine Learning Evaluation
Metrics</span></em>. <a href="https://CRAN.R-project.org/package=MLmetrics" class="external-link">https://CRAN.R-project.org/package=MLmetrics</a>.
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Minsu Kim, Li Shandross, Nicholas Reich, Evan Ray.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.2.0.</p>
</div>

    </footer>
</div>





  </body>
</html>
