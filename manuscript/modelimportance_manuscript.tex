% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  article,
  shortnames,
  notitle]{jss}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{orcidlink,thumbpdf,lmodern}

\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}
\usepackage{algorithm, algcompatible, setspace}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}{}
\makeatother
\makeatletter
\makeatother
\makeatletter
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[borderline west={3pt}{0pt}{shadecolor}, sharp corners, boxrule=0pt, interior hidden, frame hidden, breakable, enhanced]}{\end{tcolorbox}}\fi
\makeatother

\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={: Evaluating model importance within a multi-model ensemble in },
  pdfauthor={Minsu Kim; Li Shandross; Evan L. Ray; Nicholas G. Reich},
  pdfkeywords={ensemble, forecast, prediction, model importance, Shapley
value, R},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


%% -- Article metainformation (author, title, ...) -----------------------------

%% Author information
\author{Minsu Kim~\orcidlink{0009-0008-4637-3589}\\University of
Massachusetts Amherst \And Li
Shandross~\orcidlink{0009-0008-1348-1954}\\University of Massachusetts
Amherst \AND Evan L. Ray~\orcidlink{0000-0003-4035-0243}\\University of
Massachusetts Amherst \\ CVS Health \And Nicholas G.
Reich~\orcidlink{0000-0003-3503-9899}\\University of Massachusetts
Amherst}
\Plainauthor{Minsu Kim, Li Shandross, Evan L. Ray, Nicholas G.
Reich} %% comma-separated

\title{\pkg{modelimportance}: Evaluating model importance within a
multi-model ensemble in \proglang{R}}
\Plaintitle{: Evaluating model importance within a multi-model ensemble
in} %% without formatting

%% an abstract and keywords
\Abstract{Ensemble forecasts are commonly used to support
decision-making and policy planning across various fields because they
often offer improved accuracy and stability compared to individual
models. As each model has its own unique characteristics, understanding
and measuring the value each constituent model adds to the overall
accuracy of the ensemble can support the construction of effective
ensembles. The \proglang{R} package \pkg{modelimportance} provides tools
to quantify how each component model contributes to the acccuracy of
ensemble performance for both point and probabilistic forecasts. It
supports multiple functionalities; it allows users to specify which
ensemble approach to implement and which model importance metric to use.
Additionally, the software offers customizable options for handling
missing values. These features enable the package to serve as a
versatile tool for researchers and practitioners. It helps not only in
constructing an effective ensemble model across a wide range of
forecasting tasks, but also in understanding the role of each model
within the ensemble and gaining insights into individual models
themselves. This package follows the `hubverse' framework, which is a
collection of open-source software and tools developed to promote
collaborative modeling hub efforts and simplify their setup and
operation. Doing so enables seamless integration and flexibility with
other forecasting tools and systems, allowing many analyses to be
performed on existing and ongoing hubs.}

%% at least one keyword must be supplied
\Keywords{ensemble, forecast, prediction, model importance, Shapley
value, R}
\Plainkeywords{ensemble, forecast, prediction, model importance, Shapley
value, R}

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}
%% \setcounter{page}{1}
%% \Pages{1--xx}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
Minsu Kim\\
E-mail: \email{minsu@umass.edu}\\
\\~
Li Shandross\\
\\~
Evan L. Ray\\
\\~
Nicholas G. Reich\\
\\~

}

\begin{document}
\maketitle


\section{Introduction}\label{sec:intro}

Ensemble forecasting is a method to produce a single, consolidated
prediction by combining forecasts generated from different models. While
each model's strengths are pronounced, its weaknesses are
counterbalanced, which leads to an ensemble forecast that is more robust
and accurate \citep{gneiting2005weather, hastie01statisticallearning}.
Specifically, ensembles effectively mitigate the bias and variance
arising from the predictions of individual models by averaging them out,
and aggregating in this way can reduce prediction errors and improve
overall performance. Enhanced prediction accuracy and robustness enable
the achievement of more reliable predictions, thereby improving
decision-making. For this reason, ensemble forecasting is widely used
across various domains such as weather forecasting
\citep{Guerra_2020, gneiting2005weather}, financial modeling
\citep{SUN2020101160, math11041054}, and infectious disease outbreak
forecasting
\citep{ray_prediction_2018, reich_accuracy_2019, lutz_applying_2019, viboud_rapidd_2018}.
For example, throughout the COVID-19 pandemic, the US COVID-19 Forecast
Hub collected individual models developed by over 90 different research
groups and built a probabilistic ensemble forecasting model for COVID-19
cases, hospitalizations, and deaths in the US based on those models'
predictions, which served as the official short-term forecasts for the
US Centers for Disease Control and Prevention (CDC)
\citep{cramer2022united}.

The quality of forecasts is assessed by evaluating their error, bias,
sharpness, and/or calibration using different scoring metrics. The
selection of the scoring metrics depends on the type of forecast: point
forecasts (e.g., mean, median) and probabilistic forecasts (e.g.,
quantiles, samples, predictive cumulative distribution functions,
probability mass function). Commonly used assessment tools for point
forecasts are the mean absolute error (MAE) and the mean squared error
(MSE), which calculate the average magnitude of forecast errors. Scoring
metrics for probabilistic forecasts consider the uncertainty and
variability in predictions and provide concise evaluations through
numerical scores \citep{gneiting_strictly_2007}. Some examples include
the weighted interval score (WIS) for the quantile-based forecasts and
the continuous ranked probability score (CRPS) for the forecasts taking
the form of predictive cumulative distribution functions
\citep{bracher_evaluating_2021}. We note that CRPS is a general scoring
rule that can be computed either analytically in closed form or
numerically from samples, and WIS is a quantile-based approximation of
CRPS.

Several \proglang{R} packages have been developed for this purpose. To
name a few, the \pkg{fable} package \citep{Rpackage-fable} is widely
used for univariate time series forecasting and includes functions for
accuracy measurement. The \pkg{Metrics} \citep{Rpackage-Metrics} and
\pkg{MLmetrics} \citep{Rpackage-MLmetrics} provide a wide range of
performance metrics specifically designed for evaluating machine
learning models. The \pkg{scoringRules} \citep{Rpackage-scoringRules}
package offers a comprehensive set of proper scoring rules for
evaluating probabilistic forecasts and supports both univariate and
multivariate settings. The \pkg{scoringutils}
\citep{bosse2022evaluating} package offers additional features to the
functionality provided by \pkg{scoringRules}, which makes it more useful
for certain tasks, such as summarizing, comparing, and visualizing
forecast performance. These packages have been valuable to evaluate
individual models as independent entities, using performance metrics
selected for each specific situation or problem type. However, they do
not measure the individual models' contributions to the enhanced
predictive accuracy when used as part of an ensemble. \citet{kim2024}
demonstrate that a model's individual performance does not necessarily
correspond to its contribution as a component within an ensemble. Our
developed package introduces this capability. The \pkg{modelimportance}
package provides tools to evaluate the role of each model as an ensemble
member within an ensemble model, rather than focusing on the individual
predictive performance per se.

In ensemble forecasting, certain models contribute more significantly to
the overall predictions than others. Assessing the impact of each
component model on ensemble predictions is methodologically similar to
determining variable importance in traditional regression and machine
learning models, where variable importance measures evaluate how much
individual variables decrease in accuracy of the model's predictive
performance or reduce the average loss. \proglang{R} packages such as
\pkg{randomForest} \citep{Rpackage-randomForest}, \pkg{caret}
\citep{Rpackage-caret}, \pkg{xgboost} \citep{Rpackage-xgboost}, and
\pkg{gbm} \citep{Rpackage-gbm} implement these functions for different
types of models: random forest models, general machine learning models,
extreme gradient boosting models, and generalized boosted regression
models, respectively. These packages focus on feature-level importance
within a single model and do not measure the contribution of individual
models within an ensemble. The \pkg{modelimportance} package addresses
this limitation. The tools in the \pkg{modelimportance} quantify how
each component model helps enhance the ensemble model's predictive
performance. They assign numerical scores to each model based on a
selected metric that measures forecast accuracy, depending on the
forecast type.

These capabilities provide unique support for hub organizers who refer
to the entity responsible for launching and managing a collective
modeling hub. They coordinate multiple teams to produce forecasts and
integrate their predictions into an ensemble forecast
\citep{Shandross2024}, which, as mentioned earlier, is known for having
better performance compared to individual models. Examples include the
US CDC and the European Centre for Disease Prevention and Control. The
\pkg{modelimportance} package can even strengthen the benefits of a
multi-model ensemble by helping these organizations create more
effective ensemble forecasts based on the precise evaluation of each
model's contribution. Specifically, \pkg{modelimportance} follows
`hubverse' standards, where `hubverse' offers a set of publicly
available software and data tools developed to promote collaborative
modeling hub efforts and reduce the effort required to set up and
operate them \citep{hubverse_docs}. Adherence to the model output
formats specified by the hubverse convention enables many analyses to be
performed on existing and ongoing hubs by seamless integration and
flexibility with other forecasting tools and systems. We note that there
are over a dozen active hubs running as of fall 2025, with more in
planning stages.

We highlight some strong development practices we employed, such as unit
testing of individual functions, continuous integration testing on
different operating systems, and independent code review. This emphasis
on quality control is a key strength of this work and distinguishes it
from other academic software development projects.

The paper proceeds as follows. Section \ref{sec:data} describes how the
\pkg{modelimportance} package relates to the hubverse framework,
including its dependencies, the model output formats defined within
hubverse, and the structure of data presentation for both forecasts and
actual observations. Section \ref{sec:algorithms} presents two
algorithms implemented in \pkg{modelimportance} for calculating the
model importance metric: leave-one-model-out and
leave-all-subsets-of-models-out. We demonstrate the various
functionalities \pkg{modelimportance} supports in Section
\ref{sec:main-function} and highlight our quality assurance measures and
its open access in Section \ref{sec:implementation-and-availability}.
Some examples are provided in Section \ref{sec:examples}. We close this
paper with some concluding remarks and a discussion of possible
extensions.

\section{Data}\label{sec:data}

\subsection{Relationship and dependencies on
hubverse}\label{subsec:dependence_hubverse}

The \pkg{modelimportance} package is designed to work with the hubverse
framework and, accordingly, depends on several packages in the hubverse
ecosystem, such as \pkg{hubUtils} (\citet{Rpackage-hubUtils}),
\pkg{hubEnsembles} (\citet{Rpackage-hubEnsembles}), \pkg{hubEvals}
(\citet{Rpackage-hubEvals}), and \pkg{hubExamples}
(\citet{Rpackage-hubExamples}). \pkg{modelimportance} uses a
\texttt{model\_out\_tbl} S3 class as the model output format defined in
\pkg{hubUtils}, which consists of utility functions to standardize
prediction files and data formats (details in Section
\ref{subsec:model_output_format}). Ensembling predictions from multiple
models relies on \pkg{hubEnsembles}, which offers a broadly applicable
framework to construct multi-model ensembles using various ensemble
methods. Calculation of forecast accuracy using various metrics is based
on \pkg{hubEvals}, which internally leverages \pkg{scoringutils}. We use
the example datasets from \pkg{hubExamples} for testing and
demonstration purposes (see Section \ref{sec:examples}).

\subsection{Model output format}\label{subsec:model_output_format}

Model outputs are structured in a tabular format designed specifically
for predictions, which is a formal S3 object called
\texttt{model\_out\_tbl}. In the hubverse standard, each row represents
an individual prediction or a component of a prediction for a single
task, and its details are described in multiple columns through which
one can identify the unique label assigned to each forecasting model,
task characteristics, prediction representation type, and predicted
values \citep{Shandross2024}. To elaborate on the task characteristics,
each prediction task means a specific forecasting problem and it can be
described by a set of task ID variables. Examples of such variables
include a date on which forecasts are generated, the target to predict
(e.g., flu-related incident deaths, cases, or hospitalizations), and the
prediction horizon, which is the length of time into the future from the
point when a model generate its forecast, for a specific location on a
certain target date. Table~\ref{tbl-example-model_output} illustrates
short-term forecasts of weekly incident influenza hospitalizations in
the US for Massachusetts, generated by the model `Flusight-baseline' on
December 17, 2022, in the \texttt{model\_out\_tbl} format. The
\texttt{model\_id} column lists a uniquely identified model name. All of
the \texttt{reference\_date}, \texttt{target}, \texttt{horizon},
\texttt{location}, and \texttt{target\_end\_date} columns are all
referred to as the task ID variables, which together defines the task
characteristics. Note that the forecast generation date and the target
date for which the prediction is made are mapped to the
\texttt{reference\_date} and \texttt{target\_end\_date} columns,
respectively, and the location is represented based on the FIPS code
(e.g., `25' for Massachusetts). The time length to the
\texttt{target\_end\_date}, which is the number of weeks ahead from the
\texttt{reference\_date}, is indicated in the \texttt{horizon} column.
The prediction representation is specified as `quantile' in the
\texttt{output\_type} column, and details are represented in the
\texttt{output\_type\_id} column with seven quantiles of 0.05, 0.1,
0.25, 0.5, 0.75, 0.9, and 0.95 for each target end date. The predicted
value corresponding to each quantile is recorded in the \texttt{value}
column.

\begin{table}

\centering{

\centering
\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\begin{tabular}{lllrllllr}
\toprule
model\_id & reference\_date & target & horizon & location & target\_end\_date & output\_type & output\_type\_id & value\\
\midrule
Flusight-baseline & 2022-12-17 & wk inc flu hosp & 1 & 25 & 2022-12-24 & quantile & 0.05 & 496\\
Flusight-baseline & 2022-12-17 & wk inc flu hosp & 1 & 25 & 2022-12-24 & quantile & 0.1 & 536\\
Flusight-baseline & 2022-12-17 & wk inc flu hosp & 1 & 25 & 2022-12-24 & quantile & 0.25 & 566\\
Flusight-baseline & 2022-12-17 & wk inc flu hosp & 1 & 25 & 2022-12-24 & quantile & 0.5 & 582\\
Flusight-baseline & 2022-12-17 & wk inc flu hosp & 1 & 25 & 2022-12-24 & quantile & 0.75 & 598\\
\addlinespace
Flusight-baseline & 2022-12-17 & wk inc flu hosp & 1 & 25 & 2022-12-24 & quantile & 0.9 & 629\\
Flusight-baseline & 2022-12-17 & wk inc flu hosp & 1 & 25 & 2022-12-24 & quantile & 0.95 & 668\\
Flusight-baseline & 2022-12-17 & wk inc flu hosp & 2 & 25 & 2022-12-31 & quantile & 0.05 & 454\\
Flusight-baseline & 2022-12-17 & wk inc flu hosp & 2 & 25 & 2022-12-31 & quantile & 0.1 & 518\\
Flusight-baseline & 2022-12-17 & wk inc flu hosp & 2 & 25 & 2022-12-31 & quantile & 0.25 & 558\\
\bottomrule
\end{tabular}}

}

\caption{\label{tbl-example-model_output}Example of the model output for
incident influenza hospitalizations (top 10 rows) extracted from
\texttt{forecast\_outputs} data in the \pkg{hubExamples} package.}

\end{table}%

Figure~\ref{fig-example-model_output} visualize the information on the
prediction task provided by Table~\ref{tbl-example-model_output} for
three models. For each model, the quantile-based forecasts are shown for
the target end dates of December 24, 2022 (horizon 1), December 31, 2022
(horizon 2), and January 07, 2023 (horizon 3), which were made on
December 17, 2022 based on the historical data available as of that
date. The prediction intervals defined by the lowest and highest
quantiles (0.05 and 0.95) represent the uncertainty of the predictions.
To give brief details in the interpretation, the Flusight-baseline model
under-predicted the outcomes for the first two target dates (horizon 1
and 2), but it over-predicted the outcome for the last target date
(horizon 3). Its prediction intervals are narrow compared to the other
two models, which indicates that it is more confident about its
predictions. However, two of three prediction intervals (horizons 1 and
2) failed to cover the eventually observed values, implying that the
model was apparently overconfident.

\begin{figure}[t]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{modelimportance_manuscript_files/figure-pdf/fig-example-model_output-1.pdf}}

}

\caption{\label{fig-example-model_output}Example plot of three
distributional forecasts corresponding to the model output for incident
influenza hospitalizations shown in
Table~\ref{tbl-example-model_output}. Solid black dots indicate
historically available data as of the forecast generation date, and open
black circles indicate the eventually observed values. The blue dots
represent predictive medians and the blue shaded area represents the
corresponding 90\% prediction interval defined by the 0.05 and 0.95
quantiles.}

\end{figure}%

\subsection{Forecast data representation}\label{subsec:model_output}

Generally, quantitative forecasts can be categorized into either point
forecasts or probabilistic forecasts. For a specific prediction task,
point forecasts, represented by a single predicted value, provide a
clear and concise prediction, making them easy to interpret and
communicate. Probabilistic forecasts, on the other hand, provide a
probability distribution over possible future values, which inherently
involves uncertainty. They are represented in various ways, such as
probability mass functions (pmf), cumulative distribution functions
(cdf), samples, or probability quantiles (or intervals).

The \texttt{output\_type} and \texttt{output\_type\_id} columns in the
model output format, as defined by the hubverse convention, specify the
forecast structure. Only one \texttt{output\_type} is allowed, and it
must be one of the `mean', `median', `quantile', or `pmf' in the
\pkg{modelimportance} package: `mean' or `median' for point forecasts
and `quantile' or `pmf' for probabilistic forecasts. As aforementioned,
\texttt{output\_type\_id} column identifies addtional detailed
information, such as specific quantile levels (e.g., ``0.1'', ``0.25'',
``0.5'', ``0.75'', and ``0.9'') for the `quantile' output type and
categorical values (e.g., ``low'', ``moderate'', ``high'', and ``very
high'') for the `pmf' output type. The predicted values for \texttt{pmf}
are constrained to be between 0 and 1, indicating the probability at
each categorical level, while they are unbounded numeric otherwise.
Different output types correspond to different scoring rules for
evaluating a model's prediction performance.
Table~\ref{tbl-pair-output-scoringrule} presents the output types and
their associated scoring rules supported by the \pkg{modelimportance}
package.

\begin{table}

\centering{

\begin{tabular}{ll>{\raggedright\arraybackslash}p{10cm}}
\toprule
Output Type & Scoring Rule & Description\\
\midrule
mean & MSE & Evaluate using the mean squared error (MSE)\\
median & MAE & Evaluate using the mean absolute error (MAE)\\
quantile & WIS & Evaluate using the weighted interval score (WIS)\\
pmf & Log Score & Evaluate using the logarithm of the probability assigned to the true outcome (LogScore)\\
\bottomrule
\end{tabular}

}

\caption{\label{tbl-pair-output-scoringrule}Pairs of output types and
their associated scoring rules for evaluating prediction performance.}

\end{table}%

\subsection{Oracle output data}\label{subsec:oracle_output_data}

The \texttt{oracle\_output\_data} is a data frame that contains the
ground truth values for the variables used to define modeling targets.
It is referred to as
`\texttt{oracle"\ because\ it\ is\ formatted\ as\ if\ an\ oracle\ made\ a\ perfect\ point\ prediction\ equal\ to\ the\ truth.\ This\ data\ must\ follow\ the\ oracle\ output\ format\ defined\ in\ the\ hubverse\ standard,\ which\ includes\ independent\ task\ ID\ columns\ (e.g.,}location\texttt{,}target\_date\texttt{),\ the}output\_type\texttt{column\ specifying\ the\ output\ type\ of\ the\ predictions\ and\ an}oracle\_value\texttt{column\ for\ the\ observed\ values.\ \ As\ in\ the\ forecast\ data,\ if\ the}output\_type\texttt{is\ either}``quantile''\texttt{or}``pmf''\texttt{,\ the}output\_type\_id`
column is often required to provide further identifying information.

The \texttt{model\_out\_tbl} and \texttt{oracle\_output\_data} must have
the same task ID columns and \texttt{output\_type}, including
\texttt{output\_type\_id} if necessary, which are used to match the
predictions with the ground truth data.

\section{Algorithms}\label{sec:algorithms}

This section provides a brief description of the leave one model out
(LOMO) and leave all subsets of models out (LASOMO) algorithms, which
are used to compute the model importance score. The basic idea of
measuring the importance of each component model is to evaluate the
change in ensemble performance when that model is included or excluded
in the ensemble construction. More specifically, we compare the
performance of an ensemble with and without a specific model for a
specific task, and consider the difference in performance as the
importance of that model for that task. We apply this idea to many tasks
and aggregate the importance scores for that model across all tasks
using averages. (Details can be found in \citet{kim2024}.)

LOMO involves creating an ensemble by excluding one component model from
the entire set of models. Let \({\cal A}\) be a set of \(n\) models and
\(F^i\) be a forecast produced by model \(i\), where
\(i = 1,2, \dots, n.\) Each ensemble excludes exactly one model while
including all the others. Denoting by \(F^{{\cal A}^{-i}}\) an ensemble
forecast constructed without \(F^i\) and by \(F^{\cal A}\) the ensemble
forecast built from the entire set of models, the importance score using
LOMO is calculated as the difference in accuracy, as measured by a
specific score, between \(F^{{\cal A}^{-i}}\) and \(F^{\cal A}\)
(Algorithm \ref{alg:lomo}). For example, when evaluating model 1 within
an ensemble of three models (\(n=3\)), LOMO creates an ensemble forecast
\(F^{\{2,3\}}\) using only \(F^2\) and \(F^3\). The performance of this
reduced ensemble is then compared to the full ensemble forecast
\(F^{\{1,2,3\}}\), which incorporates all three models. We note that a
model can make an ensemble better or worse, and thus the importance
score for model 1 can be positive or negative accordingly.

\begin{algorithm}[b!]
\caption{Importance score calculation for one prediction task using leave one model out (LOMO) algorithm} 
\label{alg:lomo}
 \begin{spacing}{1.2}
  \begin{algorithmic}[1]
   \REQUIRE{
    \Statex $\bullet$ Set of $n$ individual models, ${\cal A}=\{1,2,\dots, n\},$ and their forecasts $\{F^1,F^2,...,F^n\}$
    \Statex $\bullet$ Truth value, $y$
    \Statex $\bullet$ Function $g$ to build an ensemble
    \Statex $\bullet$ Scoring rule $\psi$ to evaluate forecast skills}
   \ENSURE importance metric of model $i$, $\phi^{i,\text{lomo}}$
    \STATE{Create an ensemble forecast $F^{\cal A}$ using $g$: $F^{\cal A} \gets g(\{F^1,F^2,...,F^n\})$}
    \STATE{Evaluate $F^{\cal A}$ using $\psi$: $\psi(F^{\cal A},y).$}
    \FOR{each $i, (i = 1,2,..., n)$}
      \STATE {$F^{{\cal A}^{-i}} \gets g(\{F^j|\, j\ne i, j\in {\cal A}\})$}
      \STATE{Compute $\psi(F^{{\cal A}^{-i}},y).$}
      \STATE $\phi^{i,\text{lomo}} \gets   \psi(F^{{\cal A}^{-i}},y) - \psi(F^{\cal A},y)$
    \ENDFOR
  \end{algorithmic}
 \end{spacing}
\end{algorithm}

On the other hand, LASOMO involves ensemble constructions from all
possible subsets of models. For each subset \(S\) that does not contain
the model \(i\), \(S \cup \{i\}\) plays a role of \({\cal A}\) in the
LOMO; the score associated with the subset \(S\) is the difference of
measures between \(F^S\) and \(F^{S \cup \{i\}}\). Then, all scores are
aggregated across all possible subsets that the model \(i\) does not
belong to (Algorithm \ref{alg:lasomo}). For example, using the earlier
setup of three forecast models, LASOMO considers three subsets, which we
denote by \(S_1=\{2\}\), \(S_2=\{3\}\), and \(S_3=\{2, 3\}\), to
calculate the importance score of model 1 (excluding all subsets that
include model 1). The ensemble forecasts \(F^{\{2\}}, F^{\{3\}}\), and
\(F^{\{2,3\}}\) are then compared to \(F^{\{1,2\}}, F^{\{1,3\}}\), and
\(F^{\{1,2,3\}}\), respectively. The performance differences
attributable to model 1's inclusion are aggregated, which results in the
importance score for model 1. We note that the subsets (e.g.,
\(S_1, S_2,\) and \(S_3\)) may have different weights during the
aggregating process. The \pkg{modelimportance} package offers two
weighting options for subsets: one assigns equal (uniform) weights to
all subsets, and the other assigns weights based on their size, similar
to the concept of Shapley values in cooperative game theory, which
measure a player's average contribution to all possible coalitions
(\citet{Shapley1953}). Users can choose one to evaluate the contribution
of each model in a manner suited to their preferred framework.

Algorithms \ref{alg:lomo} and \ref{alg:lasomo} outline the steps to
implement LOMO and LASOMO for a single prediction task, respectively.

\begin{algorithm}[t!]
\caption{Importance score calculation for one prediction task using leave all subsets of models out (LASOMO) algorithm} 
\label{alg:lasomo}
 \begin{spacing}{1.2}
  \begin{algorithmic}[1]
   \REQUIRE{
    \Statex $\bullet$ Set of $n$ individual models, ${\cal A}=\{1,2,\dots, n\},$ and their forecasts $\{F^1,F^2,...,F^n\}$
    \Statex $\bullet$ Truth value, $y$
    \Statex $\bullet$ Function $g$ to build an ensemble
    \Statex $\bullet$ Scoring rule $\psi$ to evaluate forecast skills}
   \ENSURE Importance metric of model $i$, $\phi^{i,\text{lasomo}}$
   \FOR{$i = 1 \text{ to } n$}
     \STATE $\phi^{i,\text{lasomo}} \gets 0$
     \STATE Make a list of non-empty subsets of $\cal A$ that does not contain $i$: $S_1, S_2, \cdots, S_{2^{n-1}-1}$
     \FOR{$j = 1 \text{ to } 2^{n-1}-1$}
       \STATE{Assign a weight to the subset $S_j$: }
         \IF{all subsets have uniform weights}
           \STATE $\gamma_{S_j} \gets \displaystyle\frac{1}{2^{n-1}-1}$
         \ELSE{ (\textit{subset's weight depends on its size})}
           \STATE {$\gamma_{S_j} \gets \displaystyle\frac{1}{(n-1)\binom{n-1}{|S_j|}}$}
         \ENDIF
       \STATE{$F^{S_j}\gets g(\{F^j|\, j\in S_j\})$}
       \STATE{$F^{{S_j}\cup \{i\}}\gets g(\{F^i, F^j|\, j\in S_j\})$}
       \STATE{Compute $\psi(F^{S_j},y)$ and
       $\psi(F^{{S_j}\cup\{i\}},y)$}
       \STATE {$\phi^{i,\text{lasomo}} \gets \phi^{i,\text{lasomo}} + \gamma_{S_j}\times\Big[ \psi(F^{S_j},y) - \psi(F^{{S_j}\cup\{i\}},y) \Big]$}
     \ENDFOR
   \ENDFOR
  \end{algorithmic}
 \end{spacing}
\end{algorithm}

\section{Main function: Evaluating ensemble
members}\label{sec:main-function}

In this section, we describe the usage of the function
\texttt{model\_importance()}, where multiple options are available to
customize the evaluation framework (Table~\ref{tbl-arguments}).

\begin{verbatim}
R> model_importance(forecast_data, oracle_output_data, ensemble_fun, weighted,
                    training_window_length, importance_algorithm, subset_wt, 
                    na_action, ...)
\end{verbatim}

\texttt{forecast\_data} is a data frame containing predictions and
should be or can be coerced to a \texttt{model\_out\_tbl} format, which
is the standard S3 class model output format defined by the hubverse
convention. If it fails to be coerced to a \texttt{model\_out\_tbl}
format, an error message will be returned from \pkg{hubUtils}, which
provides the function \texttt{as\_model\_out\_tbl()} for this purpose.
Users may need to manually transform their data to meet the hubverse
standards.

The \texttt{ensemble\_fun} argument specifies the ensemble method to be
used for evaluating model importance, which relies on implementations in
the \pkg{hubEnsembles} package (\citet{Rpackage-hubEnsembles}). The
currently supported methods are \texttt{"simple\_ensemble"} and
\texttt{"linear\_pool"}. The \texttt{"simple\_ensemble"} method returns
the average of the predicted values from all component models per
prediction task defined by task IDs, \texttt{output\_type}, and
\texttt{output\_type\_id} columns. The default aggregation function for
this method is \texttt{"mean"}, but it can be customized by specifying
additional arguments through \texttt{...}, such as
\texttt{agg\_fun="median"}. When \texttt{"linear\_pool"} is specified,
ensemble model outputs are created as a linear pool of component model
outputs. This method supports only an \texttt{output\_type} of
\texttt{"mean"}, \texttt{"quantile"}, or \texttt{"pmf"}.

\begin{table}

\centering{

\centering\begingroup\fontsize{10.5}{12.5}\selectfont

\begin{tabular}{l>{\raggedright\arraybackslash}p{3.8cm}>{\raggedright\arraybackslash}p{3.5cm}>{\raggedright\arraybackslash}p{2.7cm}}
\toprule
Argument & Description & Possible Values & Default\\
\midrule
\cellcolor{gray!10}{\texttt{forecast\_data}} & \cellcolor{gray!10}{Forecasts} & \cellcolor{gray!10}{Must be the model output format} & \cellcolor{gray!10}{N/A}\\
\texttt{oracle\_output\_data} & Ground truth data & Must be the oracle output format & N/A\\
\cellcolor{gray!10}{\texttt{ensemble\_fun}} & \cellcolor{gray!10}{Ensemble method} & \cellcolor{gray!10}{\texttt{"simple\_ensemble"}, \texttt{"linear\_pool"}} & \cellcolor{gray!10}{\texttt{"simple\_ensemble"}}\\
\texttt{training\_window\_length} & Time interval of historical data used during the training process & Non-negative integer & 0\\
\cellcolor{gray!10}{\texttt{importance\_algorithm}} & \cellcolor{gray!10}{Algorithm to calculate importance} & \cellcolor{gray!10}{\texttt{"lomo", "lasomo"}} & \cellcolor{gray!10}{\texttt{"lomo"}}\\
\addlinespace
\texttt{subset\_wt} & Method for assigning weight to subsets when using LASOMO algorithm & \texttt{"equal", "perm\_based"} & \texttt{"equal"}\\
\cellcolor{gray!10}{\texttt{na\_action}} & \cellcolor{gray!10}{Method to handle missing data} & \cellcolor{gray!10}{\texttt{"worst", "average", "drop"}} & \cellcolor{gray!10}{\texttt{"worst"}}\\
\texttt{...} & Optional arguments for \texttt{"simple\_ensemble"} & Varies & \texttt{agg\_fun="mean"}\\
\bottomrule
\end{tabular}
\endgroup{}

}

\caption{\label{tbl-arguments}Description of the arguments for the
\texttt{model\_importance()} function, including their purpose, possible
values, and default settings.}

\end{table}%

The \texttt{weighted} argument is a logical value that indicates whether
model weighting should be done when building an ensemble using the
specified \texttt{ensemble\_fun}. If it is set to \texttt{TRUE}, model
weights are estimated based on the previous performance of each model,
and these weights are used to build the ensemble.

The \texttt{importance\_algorithm} argument specifies the algorithm for
model importance calculation, which can be either \texttt{"lomo"} (leave
one model out) and \texttt{"lasomo"} (leave all subsets of models out).
The \texttt{subset\_wt} argument is employed only for the
\texttt{"lasomo"} algorithm. This argument has two options:
\texttt{"equal"} assigns equal weight to all subsets and
\texttt{"perm\_based"} assigns weight averaged over all possible
permutations as in the formula of Shapley values (Algorithm
\ref{alg:lasomo}). The default values of \texttt{importance\_algorithm}
and \texttt{subset\_wt} are \texttt{"lomo"} and \texttt{"equal"},
respectively.

The \texttt{na\_action} argument allows for specifying how to handle
missing values in the \texttt{forecast\_data}. Three options are
available: \texttt{"worst"}, \texttt{"average"}, and \texttt{"drop"}. In
each specific prediction task, if a model has any missing predictions,
the \texttt{"worst"} option replaces those missing values with the
smallest value from the other models, while the \texttt{"average"}
option replaces them with the average of the other models' predictions
in that task. The \texttt{"drop"} option removes missing values, which
results in the exclusion of the model from the evaluation for that task.

\section{Implementation and
availability}\label{sec:implementation-and-availability}

This package is implemented in \proglang{R} and distributed via CRAN and
GitHhub. We conducted unit tests using the \pkg{testthat} package
\citep{Rpackage-testthat} to ensure that all functions work correctly as
expected, including those used internally. We also performed continuous
integration testing using GitHub Actions to maintain functionality
across platforms, including Windows, macOS, and Linux. Integrated GitHub
Action, we used \pkg{lintr} package to maintain consistent code style,
code quality, and detection potential issues. All code changes were
systematically reviewed by fellow team members, and this enhanced
reliability.

\section{Examples}\label{sec:examples}

The examples in this section illustrate the use of the
\texttt{model\_importance()} function to evaluate the importance of
component models within an ensemble, using various combinations of the
arguments described in Section \ref{sec:main-function}. We use some
example forecast and target data from the \pkg{hubExamples} package,
which provides sample datasets for multiple modeling hubs in the
hubverse format.

\subsection{Evaluation using untrained ensemble in LOMO algorithm for
mean forecasts}\label{sec:untrained-lomo-mean}

The forecast data used here contains forecasts of weekly incident
influenza hospitalizations in the US for Massachusetts (FIPS code 25)
and Texas (FIPS code 48), generated on November 19, 2022. These
forecasts are for two target end dates, November 26, 2022 (horizon 1),
and December 10, 2022 (horizon 3), and were produced by three models:
`Flusight-baseline', `MOBS-GLEAM\_FLUH', and `PSI-DICE'. The output type
is \texttt{mean} and the \texttt{output\_type\_id} column has
\texttt{NA}s as no further specification is required for this output
type. We have modified the example data slightly: the forecast values
were rounded to the nearest integer and some forecasts have been removed
to demonstrate the handling of missing values. Therefore,
MOBS-GLEAM\_FLUH's forecast for Massachusetts on November 26, 2022, and
PSI-DICE's forecast for Texas on December 10, 2022, are missing.

\begin{verbatim}
R> forecast_data
\end{verbatim}

\small

\begin{verbatim}
# A tibble: 10 x 9
   model_id          reference_date target          horizon location
   <chr>             <date>         <chr>             <int> <chr>   
 1 Flusight-baseline 2022-11-19     wk inc flu hosp       1 25      
 2 Flusight-baseline 2022-11-19     wk inc flu hosp       3 25      
 3 Flusight-baseline 2022-11-19     wk inc flu hosp       1 48      
 4 Flusight-baseline 2022-11-19     wk inc flu hosp       3 48      
 5 MOBS-GLEAM_FLUH   2022-11-19     wk inc flu hosp       3 25      
 6 MOBS-GLEAM_FLUH   2022-11-19     wk inc flu hosp       1 48      
 7 MOBS-GLEAM_FLUH   2022-11-19     wk inc flu hosp       3 48      
 8 PSI-DICE          2022-11-19     wk inc flu hosp       1 25      
 9 PSI-DICE          2022-11-19     wk inc flu hosp       3 25      
10 PSI-DICE          2022-11-19     wk inc flu hosp       1 48      
   target_end_date output_type output_type_id value
   <date>          <chr>       <chr>          <dbl>
 1 2022-11-26      mean        <NA>              51
 2 2022-12-10      mean        <NA>              53
 3 2022-11-26      mean        <NA>            1052
 4 2022-12-10      mean        <NA>            1053
 5 2022-12-10      mean        <NA>              47
 6 2022-11-26      mean        <NA>            1073
 7 2022-12-10      mean        <NA>             701
 8 2022-11-26      mean        <NA>              92
 9 2022-12-10      mean        <NA>             159
10 2022-11-26      mean        <NA>            1222
\end{verbatim}

\normalsize

The corresponding target data contains the observed hospitalization
counts for these dates and locations.

\begin{verbatim}
R> target_data
\end{verbatim}

\small

\begin{verbatim}
# A tibble: 4 x 3
  target_end_date location oracle_value
  <date>          <chr>           <dbl>
1 2022-11-26      25                221
2 2022-11-26      48               1929
3 2022-12-10      25                578
4 2022-12-10      48               1781
\end{verbatim}

\normalsize

\begin{figure}[t]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{modelimportance_manuscript_files/figure-pdf/fig-example-mean-lomo-1.pdf}}

}

\caption{\label{fig-example-mean-lomo}Plot of three point forecasts
(mean) and the eventually observed values from the
\texttt{forecast\_data} and \texttt{target\_data} for weekly incident
influenza hospitalizations in Massachusetts (FIPS code 25) and Texas
(FIPS code 48). Colored dots indicate the forecasts by three models,
generated on November 19, 2022. Open black circles indicate the
eventually observed values. MOBS-GLEAM\_FLUH's forecast for
Massachusetts on November 26, 2022, and PSI-DICE's forecast for Texas on
December 10, 2022, are missing.}

\end{figure}%

Overall, the forecasts tend to have larger prediction errors for the
target end date of December 10, 2022, compared to November 26, 2022,
which is expected due to increased uncertainty at longer horizons.
Additionally, the forecasts for Massachusetts are relatively more
accurate compared to those for Texas, which tend to have higher errors
(Figure~\ref{fig-example-mean-lomo}).

We can quantify the contribution of each model within the ensemble using
the \texttt{model\_importance()} function. The following code evaluates
the importance of each model in the simple mean ensemble using the LOMO
algorithm, without training the ensemble (\texttt{weighted\ =\ FALSE});
training is infeasible in this example as the data available for
training is limited. The \texttt{na\_action} argument is set to
\texttt{"drop"}, which represents that any missing values in the
forecasts will be excluded from the evaluation.

\begin{verbatim}
R> model_importance(
                    forecast_data = forecast_data, 
                    oracle_output_data = target_data,
                    ensemble_fun = "simple_ensemble", 
                    weighted = FALSE,
                    importance_algorithm = "lomo", 
                    na_action = "drop"
                   )
\end{verbatim}

This call generates both the result and informative messages,
summarizing the input data, including the number of dates on which
forecasts were produced and the number of models with ids as follows.

\small

\begin{verbatim}
Forecasts from 2022-11-19 to 2022-11-19 (a total of 1 forecast date(s)).
The available model IDs are:
     Flusight-baseline
     MOBS-GLEAM_FLUH
     PSI-DICE 
(a total of 3 models)
\end{verbatim}

\normalsize

The function output is a data frame containing model IDs and their
corresponding importance scores in the \texttt{mean\_importance} column,
ordered from most to least importance. The column name
\texttt{mean\_importance} indicates the average of task-specific
importance scores.

\small

\begin{verbatim}
# A tibble: 3 x 2
  model_id          mean_importance
  <chr>                       <dbl>
1 Flusight-baseline          69149.
2 PSI-DICE                   44303.
3 MOBS-GLEAM_FLUH          -113477.
\end{verbatim}

\normalsize

The results show that the model `Flusight-baseline' has the highest
importance score, followed by `PSI-DICE' and `MOBS-GLEAM\_FLUH'. That
is, `Flusight-baseline' contributes the most to improving the ensemble's
predictive performance, whereas MOBS-GLEAM\_FLUH, which has a negative
score, detracts from the ensemble's performance. The low importance
score of `MOBS-GLEAM\_FLUH' is mainly due to a substantially larger
prediction error for Texas on the target end date of December 10, 2022,
compared to other models, while its missing forecast for Massachusetts
for November 26, 2022, was not factored into the evaluation. This single
large error significantly affected its contribution score.

Another approach to handling missing values is to use the
\texttt{"worst"} option for \texttt{na\_action}, which replaces missing
values with the worst (i.e., minimum) score among the other models for
the same task.

\begin{verbatim}
R> model_importance(
                    forecast_data = forecast_data, 
                    oracle_output_data = target_data,
                    ensemble_fun = "simple_ensemble", 
                    weighted = FALSE,
                    importance_algorithm = "lomo", 
                    na_action = "worst"
                   )
\end{verbatim}

\small

\begin{verbatim}
# A tibble: 3 x 2
  model_id          mean_importance
  <chr>                       <dbl>
1 Flusight-baseline          69149.
2 PSI-DICE                  -38581.
3 MOBS-GLEAM_FLUH           -86535.
\end{verbatim}

\normalsize

The results show that the importance scores of `Flusight-baseline' is
unchanged because it has no missing forecast. We observe that the
importance score of `PSI-DICE', which was previously positive, has now
decreased to a negative value when compared to the evaluation using the
\texttt{"drop"} option for \texttt{na\_action}. Moreover,
`MOBS-GLEAM\_FLUH' still ranks the lowest, but the importance score has
increased. This change is related to the varying forecast accuracy
across different tasks. For the target end date of November 26, 2022, in
Massachusetts, most forecasts are relatively accurate. Thus, even if the
`MOBS-GLEAM\_FLUH' is assigned the worst value of importance score for
its missing forecast, including this value in the averaging is not
detrimental to the overall importance metric; rather, it is more
beneficial than excluding it. In contrast, for the target end date of
December 10, 2022, in Texas, the forecasts have much larger errors
across the board, and assigning the worst value of importance score to
the missing forecast of `PSI-DICE' in this task has a detrimental effect
on averaging importance scores. This is because the scale of the
importance scores is influenced by the magnitude of the prediction
errors: for tasks with small errors, the scores remain moderate, while
tasks with large errors can yield importance scores of much greater
magnitude.

It is also possible to impute the missing scores with intermediate
values by assigning the average importance scores of other models in the
same task. This strategy may offer a more balanced trade-off by
mitigating the influence of the missing data without overly penalizing
or overlooking them.

\begin{verbatim}
R> model_importance(
                    forecast_data = forecast_data, 
                    oracle_output_data = target_data,
                    ensemble_fun = "simple_ensemble", 
                    weighted = FALSE,
                    importance_algorithm = "lomo", 
                    na_action = "average"
                   )
\end{verbatim}

\small

\begin{verbatim}
# A tibble: 3 x 2
  model_id          mean_importance
  <chr>                       <dbl>
1 Flusight-baseline          69149.
2 PSI-DICE                   40971.
3 MOBS-GLEAM_FLUH           -85003.
\end{verbatim}

\normalsize

In this simple example with only three models, the ranking of the
importance scores remains unchanged. However, in more complex scenarios
with a larger number of models, the choice of \texttt{na\_action} can
impact the importance scores and their interpretation. A more extensive
application can be found in \citet{kim2024}.

\subsection{Evaluation using trained ensemble in LASOMO algorithm for
quantile-based forecasts}\label{sec:trained-lomo-qntl}

TO DO

\section{Summary and discussion}\label{sec:discussion}

Multi-model ensemble forecasts often provide better accuracy and
robustness than single models, and are widely used in decision-making
and policy planning across various domains. The contribution of each
component model to the accuracy of the ensemble depends on its own
unique characteristics. The \pkg{modelimportance} package enables the
quantification of the value that each component model adds to the
ensemble performance in different evaluation contexts.

The primary function of the package is \texttt{model\_importance()},
which returns a data frame with component models and their importance
metrics. Users can choose the various ensemble methods to apply and
model importance algorithm between LOMO and LASOMO. Additionally,
customizable options are available for handling missing values. These
features enable the package to serve as a versatile tool to aid
collaborative efforts to construct an effective ensemble model across a
wide range of forecasting tasks. We note that unit testing with
continuous integration ensures the reliability of all functions and the
overall quality of code across multiple platforms.

There is a room to enhance the current version of this package. Although
this package supports four different output types (`mean', `median',
`quantile, and 'pmf'), other output types are widely used in practice.
For example, `sample' output type is commonly used in the US Flu
Scenario Modeling Hub \citep{FluSMH}. This format includes multiple
simulated values (samples) from the forecast distribution. The
\texttt{output\_type\_id} is specified for each sample, which typically
indexes the samples or indicates their source, depending on the context.
The package can be extended to support this output type, which is under
consideration for future releases. These extensions would aim to broaden
the scope of applications in real-world forecasting tasks.

\section*{Acknowledgements}\label{acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}

We acknowledge Zhian N. Kamvar for debugging and resolving coding issues
while developing the package.


\renewcommand\refname{References}
  \bibliography{references.bib}



\end{document}
