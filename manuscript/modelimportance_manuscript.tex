% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  article,
  shortnames,
  notitle]{jss}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{orcidlink,thumbpdf,lmodern}

\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}
\usepackage{algorithm, algcompatible, setspace}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}{}
\makeatother
\makeatletter
\makeatother
\makeatletter
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[boxrule=0pt, sharp corners, breakable, interior hidden, enhanced, frame hidden, borderline west={3pt}{0pt}{shadecolor}]}{\end{tcolorbox}}\fi
\makeatother

\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={: An  package for evaluating model importance within an ensemble},
  pdfauthor={Minsu Kim; Evan Rays; Nicholas G. Reich},
  pdfkeywords={ensemble, forecast, prediction, model importance, Shapley
value, R},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


%% -- Article metainformation (author, title, ...) -----------------------------

%% Author information
\author{Minsu Kim~\orcidlink{0009-0008-4637-3589}\\University of
Massachusetts Amherst \And Evan
Rays~\orcidlink{0000-0003-4035-0243}\\CVS \AND Nicholas G.
Reich~\orcidlink{0000-0003-3503-9899}\\University of Massachusetts
Amherst}
\Plainauthor{Minsu Kim, Evan Rays, Nicholas G. Reich} %% comma-separated

\title{\pkg{modelimportance}: An \proglang{R} package for evaluating
model importance within an ensemble}
\Plaintitle{: An package for evaluating model importance within an
ensemble} %% without formatting

%% an abstract and keywords
\Abstract{Ensemble forecasts are commonly used to support
decision-making and policy planning across various fields because they
often offer improved accuracy and stability compared to individual
models. As each model has its own unique characteristics, understanding
and measuring the value each constituent model adds to the overall
accuracy of the ensemble is of great interest to building effective
ensembles. The \proglang{R} package \pkg{modelimportance} provides tools
to quantify how each component model contributes to the acccuracy of
ensemble performance for different forecast types. It supports multiple
functionalities; it allows users to specify which ensemble approach to
implement and which model importance metric to use. Additionally, the
software offers customizable options for handling missing values. These
features enable the package to serve as a versatile tool for researchers
and practitioners aiming to construct an effective ensemble model across
a wide range of forecasting tasks.}

%% at least one keyword must be supplied
\Keywords{ensemble, forecast, prediction, model importance, Shapley
value, R}
\Plainkeywords{ensemble, forecast, prediction, model importance, Shapley
value, R}

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}
%% \setcounter{page}{1}
%% \Pages{1--xx}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
Minsu Kim\\
E-mail: \email{minsu@umass.edu}\\
\\~
Evan Rays\\
\\~
Nicholas G. Reich\\
\\~

}

\begin{document}
\maketitle


\section{Introduction}\label{sec:intro}

Ensemble forecasting is a method to produce a single, consolidated
prediction by combining forecasts generated from different models. This
technique it utilizes each model's strengths to counterbalance their
individual weaknesses, leading to more robust and accurate predictions
\citep{gneiting2005weather, hastie01statisticallearning, lutz_applying_2019, viboud_rapidd_2018}.
Specifically, ensembles aggregate diverse insights from various models
and effectively handle individual models models biases and variances by
averaging them out, which can reduce prediction errors and improve
overall model performance. These enhanced prediction accuracy and
robustness enable ensemble forecasting to be widely used across various
domains such as weather forecasting
\citep{Guerra_2020, gneiting2005weather}, financial modeling
\citep{SUN2020101160, math11041054}, and infectious disease outbreak
forecasting \citep{ray_prediction_2018, reich_accuracy_2019} to improve
decision-making and achieve more reliable predictions. For example,
throughout the COVID-19 pandemic, the US COVID-19 Forecast Hub collected
individual models developed by over 90 different research groups and
built a probabilistic ensemble forecasting model for COVID-19 cases,
hospitalizations, and deaths in the US based on those models'
predictions, which served as the official short-term forecasts for the
US Centers for Disease Control and Prevention (CDC) \citep{kim2024}.

The quality of forecasts is assessed by evaluating their error, bias,
sharpness, and/or calibration using different scoring metrics. The
selection of the scoring metrics depends on the type of forecast, such
as point forecasts and probabilistic forecasts, and their corresponding
formats, such as median/mean, quantiles, and predictive cumulative
distribution functions. In the case of point forecasts, accuracy is
commonly measured by the mean absolute error (MAE) and the mean squared
error (MSE), which are direct assessment tools that calculate the
average magnitude of forecast errors. Probabilistic forecasts are
typically assessed using proper scoring rules, which consider the
uncertainty and variability in predictions, providing concise
evaluations through numerical scores \citep{gneiting_strictly_2007}.
Some examples include the weighted interval score (WIS) for the
quantile-based forecasts and the continuous ranked probability score
(CRPS) for the forecasts taking the form of predictive cumulative
distribution functions \citep{bracher_evaluating_2021}.

Several \proglang{R} packages have been developed for this purpose. To
name a few, the \pkg{forecast} package \citep{Rpackage-forecast} is
widely used for univariate time series forecasting and includes
functions for accuracy measurement. The \pkg{Metrics}
\citep{Rpackage-Metrics} and \pkg{MLmetrics} \citep{Rpackage-MLmetrics}
provide a wide range of performance metrics specifically designed for
evaluating machine learning models. The \pkg{scoringRules}
\citep{Rpackage-scoringRules} package offers a comprehensive set of
proper scoring rules for evaluating probabilistic forecasts and supports
both univariate and multivariate settings. The \pkg{scoringutils}
\citep{bosse2022evaluating} package offers additional features to the
functionality provided by \pkg{scoringRules}, which makes it more useful
for certain tasks, such as summarizing, comparing, and visualizing
forecast performance. Additionally, the \pkg{scoringutils} supports
forecasts represented by predictive samples or quantiles of predictive
distributions, allowing for the assessment of any forecast type, even
when a closed-form expression for a parametric distribution is not
available. These packages have been valuable to evaluate individual
models as independent entities, using performance metrics selected for
each specific situation or problem type. However, they do not measure
the individual models' contributions to the enhanced predictive accuracy
when used as part of an ensemble. Kim et al. \citep{kim2024} demonstrate
that a model's individual performance does not necessarily correspond to
its contribution as a component within an ensemble. Our developed
package introduces this capability. The \pkg{modelimportance} package
provides tools to evaluate the role of each model as an ensemble member
within an ensemble model, rather than focusing on the individual
predictive performance per se.

In ensemble forecasting, certain models contribute more significantly to
the overall predictions than others. Assessing the impact of each
component model on ensemble predictions is methodologically similar to
determining variable importance in traditional regression and machine
learning models, where variable importance measures evaluate how much
individual variables enhance the model's predictive performance.
\proglang{R} packages that implement these functions include
\pkg{randomForest} \citep{Rpackage-randomForest}, \pkg{caret}
\citep{Rpackage-caret}, \pkg{xgboost} \citep{Rpackage-xgboost}, and
\pkg{gbm} \citep{Rpackage-gbm}, each providing variable importance
measures for different types of models: random forest models, general
machine learning models, extreme gradient boosting models, and
generalized boosted regression models, respectively. Similarly, the
tools in the \pkg{modelimportance} package quantify the contribution of
each component model within an ensemble to the ensemble model's
predictive performance. They assign numerical scores to each model based
on a selected metric that measures forecast accuracy, depending on the
forecast type.

These capabilities provide unique support for hub organizers, such as
the CDC in the US and the European Centre for Disease Prevention and
Control in the EU. These organizations can create more effective
ensemble forecasts by gathering and utilizing forecasts from various
models based on the precise evaluation of each model's contribution via
the \pkg{modelimportance} package. This, in turn, facilitates better
communication with the public and decision-makers and enhances the
overall decision-making process. Specifically, \pkg{modelimportance} is
incorporated into the `hubverse', which is a collection of open-source
software and data tools developed to promote collaborative modeling hub
efforts and reduce the effort required to set up and operate them
\citep{hubverse_docs}. This package adheres to the model output formats
specified by the hubverse convention, which enables seamless integration
and interoperability with other forecasting tools and systems.

The paper proceeds as follows. In Section 2, we address the model output
formats proposed within the hubverse framework and the structure of
forecasts, followed by a motivating example. Section 3 presents two
algorithms implemented in \pkg{modelimportance} for calculating the
model importance metric: leave-one-model-out and
leave-all-subsets-of-models-out. We demonstrate the various
functionalities \pkg{modelimportance} supports in Section 4 and give
some examples in Section 5. We close this paper with some concluding
remarks and a discussion of possible extensions.

\section{Data}\label{sec:data}

\subsection{Model output format}\label{subsec:model_output_format}

Model outputs are structured in a tabular format designed specifically
for predictions. In the hubverse standard, each row represents an
individual prediction for a single task, and its details are described
in multiple columns through which one can identify the model IDs, task
characteristics, prediction representation type, and predicted values
\citep{Shandross2024}. In Table~\ref{tbl-example-model_output}, for
example, the \texttt{model\_id} column contains the uniquely identified
name of the model that produced the prediction in each row. Task
characteristics are represented by \texttt{reference\_date},
\texttt{target}, \texttt{horizon}, \texttt{location}, and
\texttt{target\_end\_date} columns, collectively referred to as the task
ID columns. The prediction representation type is specified in the
\texttt{output\_type} and \texttt{output\_type\_id} columns. This
example illustrates short-term forecasts of incident influenza
hospitalizations in the US for Massachusetts (FIPS code 25), generated
by the model `Flusight-baseline' on November 19, 2022. The forecasts are
provided in seven quantiles (0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95) for
each target end date.

\begin{table}

\centering{

\centering
\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\begin{tabular}{lllrllllr}
\toprule
model\_id & reference\_date & target & horizon & location & target\_end\_date & output\_type & output\_type\_id & value\\
\midrule
Flusight-baseline & 2022-11-19 & wk inc flu hosp & 0 & 25 & 2022-11-19 & quantile & 0.05 & 22\\
Flusight-baseline & 2022-11-19 & wk inc flu hosp & 0 & 25 & 2022-11-19 & quantile & 0.1 & 31\\
Flusight-baseline & 2022-11-19 & wk inc flu hosp & 0 & 25 & 2022-11-19 & quantile & 0.25 & 45\\
Flusight-baseline & 2022-11-19 & wk inc flu hosp & 0 & 25 & 2022-11-19 & quantile & 0.5 & 51\\
Flusight-baseline & 2022-11-19 & wk inc flu hosp & 0 & 25 & 2022-11-19 & quantile & 0.75 & 57\\
\addlinespace
Flusight-baseline & 2022-11-19 & wk inc flu hosp & 0 & 25 & 2022-11-19 & quantile & 0.9 & 71\\
Flusight-baseline & 2022-11-19 & wk inc flu hosp & 0 & 25 & 2022-11-19 & quantile & 0.95 & 80\\
Flusight-baseline & 2022-11-19 & wk inc flu hosp & 1 & 25 & 2022-11-26 & quantile & 0.05 & 5\\
Flusight-baseline & 2022-11-19 & wk inc flu hosp & 1 & 25 & 2022-11-26 & quantile & 0.1 & 21\\
Flusight-baseline & 2022-11-19 & wk inc flu hosp & 1 & 25 & 2022-11-26 & quantile & 0.25 & 38\\
\bottomrule
\end{tabular}}

}

\caption{\label{tbl-example-model_output}Example of the model output for
incident influenza hospitalizations extracted from
\texttt{forecast\_outputs} data in the \pkg{hubExamples} package.}

\end{table}%

\subsection{Structure of forecasts}\label{subsec:structure_of_forecasts}

The forecasting model supports multiple output types. Generally,
quantitative forecasts can be categorized into either point forecasts or
probabilistic forecasts. For a specific task, point forecasts,
represented by a single predicted value, provide a clear and concise
prediction, making them easy to interpret and communicate. Probabilistic
forecasts, on the other hand, describe the likelihood of different
outcomes, conveying the uncertainty inherent in the prediction. These
forecasts are represented by a probability distribution over possible
future values in various ways, such as probability mass functions (pmf),
cumulative distribution functions (cdf), or probability quantiles (or
intervals).

The \texttt{output\_type} and \texttt{output\_type\_id} columns in the
model output format, as defined by the hubverse convention, specify the
forecast structure. The \texttt{output\_type} column takes the character
value `mean' or `median' for point forecasts, and `pmf', `cdf', or
`quantile' for probabilistic forecasts. The \texttt{output\_type\_id}
column provides additional details for probabilistic forecasts; for
instance, when the \texttt{output\_type} is `quantile', the
\texttt{output\_type\_id} identifies specific quantile levels, as
illustrated in Table~\ref{tbl-example-model_output}. In the case of
point forecasts, \texttt{output\_type\_id} column has `NA'. Different
output types correspond to different scoring rules for evaluating a
model's prediction performance. Table~\ref{tbl-pair-output-scoringrule}
presents the output types and their associated scoring rules supported
by the \pkg{modelimportance} package.

\begin{table}

\centering{

\begin{tabular}{ll>{\raggedright\arraybackslash}p{10cm}}
\toprule
Output Type & Scoring Rule & Description\\
\midrule
mean & MSE & Evaluate using the mean squared error (MSE)\\
median & MAE & Evaluate using the mean absolute error (MAE)\\
quantile & WIS & Evaluate using the weighted interval score (WIS)\\
pmf & Log Score & Evaluate using the logarithm of the probability assigned to the true outcome (LogScore)\\
\bottomrule
\end{tabular}

}

\caption{\label{tbl-pair-output-scoringrule}Pairs of output types and
their associated scoring rules for evaluating prediction performance.}

\end{table}%

\section{Algorithms}\label{sec:algorithms}

This section provides a brief description of the leave one model out
(LOMO) and leave all subsets of models out (LASOMO) algorithms. (Details
can be found in \citet{kim2024})

LOMO involves creating an ensemble by excluding one component model from
the entire set of models. Let \({\cal A}\) be a set of \(n\) models and
\(F^i\) be a forecast produced by model \(i\), where
\(i = 1,2, \dots, n.\) Each ensemble excludes exactly one model while
including all the others. Denoting by \(F^{{\cal A}^{-i}}\) an ensemble
forecast constructed without \(F^i\) and by \(F^{\cal A}\) the ensemble
forecast built from the entire set of models, the importance score using
LOMO is calculated by the difference of measures of these two ensemble
performances, \(F^{{\cal A}^{-i}}\) and \(F^{\cal A}\). For example,
when evaluating model 1 within an ensemble of three models (\(n=3\)),
LOMO creates an ensemble forecast \(F^{\{2,3\}}\) using only \(F^2\) and
\(F^3\). The performance of this reduced ensemble is then compared to
the full ensemble forecast \(F^{\{1,2,3\}}\), which incorporates all
three models.

On the other hand, LASOMO involves ensemble constructions from all
possible subsets of models. For each subset \(S\) that does not contain
the model \(i\), \(S \cup \{i\}\) plays a role of \({\cal A}\) in the
LOMO; the score associated with the subset \(S\) is the difference of
measures between \(F^S\) and \(F^{S \cup \{i\}}\). Then, all scores are
aggregated across all possible subsets that the model \(i\) does not
belong to. For example, using the earlier setup of three forecast
models, LASOMO considers three subsets, \(\{2\}\), \(\{3\}\), and
\(\{2, 3\}\), to calculate the importance score of model 1 (excluding
all subsets that include model 1). The ensemble forecasts
\(F^{\{2\}}, F^{\{3\}}\), and \(F^{\{2,3\}}\) are then compared to
\(F^{\{1,2\}}, F^{\{1,3\}}\), and \(F^{\{1,2,3\}}\), respectively. The
performance differences attributable to model 1's inclusion are
aggregated, which results in the importance score of model 1. We note
that the subsets may have different weights during the aggregating
process. The \pkg{modelimportance} package offers two weighting options
for subsets: one assigns equal (uniform) weights to all subsets, and the
other assigns weights based on their size, similar to the concept of
Shapley values. Users can choose one to evaluate the contribution of
each model in a manner suited to their preferred framework.

\begin{algorithm}[bh!]
\caption{Importance score using leave one model out (LOMO) algorithm} 
\label{alg:lomo}
 \begin{spacing}{1.2}
  \begin{algorithmic}[1]
   \REQUIRE{
    \Statex $\bullet$ Set of $n$ individual models, ${\cal A}=\{1,2,\dots, n\},$ and their forecasts $\{F^1,F^2,...,F^n\}$
    \Statex $\bullet$ Truth value, $y$
    \Statex $\bullet$ Function $g$ to build an ensemble
    \Statex $\bullet$ Scoring rule $\psi$ to evaluate forecast skills}
   \ENSURE importance metric of model $i$, $\phi^{i,\text{lomo}}$
    \STATE{Create an ensemble forecast $F^{\cal A}$ using $g$: $F^{\cal A} \gets g(\{F^1,F^2,...,F^n\})$}
    \STATE{Evaluate $F^{\cal A}$ using $\psi$: $\psi(F^{\cal A},y).$}
    \FOR{each $i, (i = 1,2,..., n)$}
      \STATE {$F^{{\cal A}^{-i}} \gets g(\{F^j|\, j\ne i, j\in {\cal A}\})$}
      \STATE{Compute $\psi(F^{{\cal A}^{-i}},y).$}
      \STATE $\phi^{i,\text{lomo}} \gets   \psi(F^{{\cal A}^{-i}},y) - \psi(F^{\cal A},y)$
    \ENDFOR
  \end{algorithmic}
 \end{spacing}
\end{algorithm}

Algorithms \ref{alg:lomo} and \ref{alg:lasomo} outline the steps to
implement LOMO and LASOMO for a single prediction task, respectively.

\begin{algorithm}[bh!]
\caption{Importance score using leave all subsets of models out (LASOMO) algorithm} 
\label{alg:lasomo}
 \begin{spacing}{1.2}
  \begin{algorithmic}[1]
   \REQUIRE{
    \Statex $\bullet$ Set of $n$ individual models, ${\cal A}=\{1,2,\dots, n\},$ and their forecasts $\{F^1,F^2,...,F^n\}$
    \Statex $\bullet$ Truth value, $y$
    \Statex $\bullet$ Function $g$ to build an ensemble
    \Statex $\bullet$ Scoring rule $\psi$ to evaluate forecast skills}
   \ENSURE Importance metric of model $i$, $\phi^{i,\text{lasomo}}$
   \FOR{$i = 1 \text{ to } n$}
     \STATE $\phi^{i,\text{lasomo}} \gets 0$
     \STATE Make a list of non-empty subsets of $\cal A$ that does not contain $i$: $S_1, S_2, \cdots, S_{2^{n-1}-1}$
     \FOR{$j = 1 \text{ to } 2^{n-1}-1$}
       \STATE{Assign a weight to the subset $S_j$: }
         \IF{all subsets have uniform weights}
           \STATE $\gamma_{S_j} \gets \displaystyle\frac{1}{2^{n-1}-1}$
         \ELSE{ (\textit{subset's weight depends on its size})}
           \STATE {$\gamma_{S_j} \gets \displaystyle\frac{1}{(n-1)\binom{n-1}{|S_j|}}$}
         \ENDIF
       \STATE{$F^{S_j}\gets g(\{F^j|\, j\in S_j\})$}
       \STATE{$F^{{S_j}\cup \{i\}}\gets g(\{F^i, F^j|\, j\in S_j\})$}
       \STATE{Compute $\psi(F^{S_j},y)$ and
       $\psi(F^{{S_j}\cup\{i\}},y)$}
       \STATE {$\phi^{i,\text{lasomo}} \gets \phi^{i,\text{lasomo}} + \gamma_{S_j}\times\Big[ \psi(F^{S_j},y) - \psi(F^{{S_j}\cup\{i\}},y) \Big]$}
     \ENDFOR
   \ENDFOR
  \end{algorithmic}
 \end{spacing}
\end{algorithm}

\section{Main funtion: Evaluating ensemble
members}\label{sec:main-function}

In this section, we describe the functionalities of the main function
\texttt{model\_importance()}, where multiple options are available to
customize the evaluation framework (Table~\ref{tbl-arguments}).

\begin{verbatim}
> model_importance(forecast_data, oracle_output_data, ensemble_fun, weighted,
                   training_window_length, importance_algorithm, subset_wt, 
                   na_action, ...)
\end{verbatim}

\begin{table}

\centering{

\centering\begingroup\fontsize{10.5}{12.5}\selectfont

\begin{tabular}{l>{\raggedright\arraybackslash}p{3.8cm}>{\raggedright\arraybackslash}p{3.5cm}>{\raggedright\arraybackslash}p{2.7cm}}
\toprule
Argument & Description & Possible Values & Default\\
\midrule
\cellcolor{gray!10}{\texttt{forecast\_data}} & \cellcolor{gray!10}{Forecasts} & \cellcolor{gray!10}{Must be the model output format} & \cellcolor{gray!10}{N/A}\\
\texttt{oracle\_output\_data} & Ground truth data & Must be the oracle output format & N/A\\
\cellcolor{gray!10}{\texttt{ensemble\_fun}} & \cellcolor{gray!10}{Ensemble method} & \cellcolor{gray!10}{\texttt{simple\_ensemble}, \texttt{linear\_pool}} & \cellcolor{gray!10}{\texttt{simple\_ensemble}}\\
\texttt{training\_window\_length} & Time interval of historical data used during the training process & Non-negative integer & 0\\
\cellcolor{gray!10}{\texttt{importance\_algorithm}} & \cellcolor{gray!10}{Algorithm to calculate importance} & \cellcolor{gray!10}{\texttt{lomo, lasomo}} & \cellcolor{gray!10}{\texttt{lomo}}\\
\addlinespace
\texttt{subset\_wt} & Method for assigning weight to subsets when using lasomo algorithm & \texttt{equal, perm\_based} & \texttt{equal}\\
\cellcolor{gray!10}{\texttt{na\_action}} & \cellcolor{gray!10}{Method to handle missing data} & \cellcolor{gray!10}{\texttt{worst, average, drop}} & \cellcolor{gray!10}{\texttt{worst}}\\
\texttt{...} & Optional arguments for \texttt{simple\_ensemble} & Varies & \texttt{agg\_fun = mean}\\
\bottomrule
\end{tabular}
\endgroup{}

}

\caption{\label{tbl-arguments}Description of the arguments for the
\texttt{model\_importance()} function, including their purpose, possible
values, and default settings.}

\end{table}%

\texttt{forecast\_data} is a data frame containing predictions and
should be or can be coerced to a \texttt{model\_out\_tbl} format, which
is the standard S3 class model output format defined by the hubverse
convention. Only one \texttt{output\_type} is allowed in the data frame,
and it must be one of the \texttt{mean}, \texttt{median},
\texttt{quantile}, or \texttt{pmf}.

\texttt{oracle\_output\_data} is a data frame containing the ground
truth data for the variables that are used to define modeling targets.
This data must follow the oracle output format, which includes
independent task ID columns (e.g., \texttt{location},
\texttt{target\_date}, and \texttt{age\_group}), the
\texttt{output\_type} column specifying the output type of the
predictions and an \texttt{oracle\_value} column for the observed
values. If the \texttt{output\_type} is either \texttt{"quantile"} or
\texttt{"pmf"}, the \texttt{output\_type\_id} column is required to
provide further identifying information. For \texttt{"quantile"}, it
should contain numeric values between 0 and 1 indicating quantile levels
(e.g., ``0.1'', ``0.25'', ``0.5'', ``0.75'', ``0.9''). For
\texttt{"pmf"}, it should contain categorical values such as ``low'',
``moderate'', ``high'', and ``very high''.

The \texttt{forecast\_data} and \texttt{oracle\_output\_data} must have
the same task ID columns and \texttt{output\_type}, including
\texttt{output\_type\_id} if necessary, which are used to match the
predictions with the ground truth data. As aforementioned, these data
frames should follow the model output format and the oracle output
format, respectively, as defined by the hubverse convention.

The \texttt{ensemble\_fun} argument specifies the ensemble method to be
used for evaluating model importance. The currently supported methods
are \texttt{"simple\_ensemble"} and \texttt{"linear\_pool"}. The
\texttt{"simple\_ensemble"} method returns the average of the predicted
values from all component models per prediction task defined by task
IDs, \texttt{output\_type}, and \texttt{output\_type\_id} columns. The
default aggregation function for this method is \texttt{"mean"}, but it
can be customized by specifying additional arguments through
\texttt{...}, such as \texttt{agg\_fun="median"}. When
\texttt{"linear\_pool"} is specified, ensemble model outputs are created
as a linear pool of component model outputs. This method supports only
an \texttt{output\_type} of \texttt{"mean"}, \texttt{"quantile"}, or
\texttt{"pmf"}.

The \texttt{weighted} argument is a logical value that indicates whether
model weighting should be done when building an ensemble using the
specified \texttt{ensemble\_fun}. If it is set to \texttt{TRUE}, model
weights are estimated based on the previous performance of each model,
and these weights are used to build the ensemble.

The \texttt{importance\_algorithm} argument specifies the algorithm for
model importance calculation, which can be either \texttt{"lomo"}
(leave-one-model-out) and \texttt{"lasomo"} (leave all subsets of models
out). The \texttt{subset\_wt} argument is employed only for the
\texttt{"lasomo"} algorithm. This argument has two options:
\texttt{"equal"} assigns equal weight to all subsets and
\texttt{"perm\_based"} assigns weight averaged over all possible
permutations as in the formula of Shapley values (Algorithm
\ref{alg:lasomo}). The default values of \texttt{importance\_algorithm}
and \texttt{subset\_wt} are \texttt{"lomo"} and \texttt{"equal"},
respectively.

The \texttt{na\_action} argument allows for specifying how to handle
missing values in the \texttt{forecast\_data}. Three options are
available: \texttt{"worst"}, \texttt{"average"}, and \texttt{"drop"}. In
each specific prediction task, if a model has any missing predictions,
the \texttt{"worst"} option replaces those missing values with the
smallest value from the other models, while the \texttt{"average"}
option replaces them with the average of the other models' predictions
in that task. The \texttt{"drop"} option removes missing values, which
results in the exclusion of the model from the evaluation for that task.

\section{Examples}\label{sec:examples}

The examples in this section illustrate the use of the
\texttt{model\_importance()} function to evaluate the importance of
component models within an ensemble, using various combinations of the
arguments described in Section \ref{sec:main-function}. We use some
example forecast and target data from the \pkg{hubExamples} package,
which provides sample datasets for multiple modeling hubs in the
hubverse format.

\subsection{Evaluation using untrained ensemble in LOMO algorithm for
mean forecasts}\label{sec:untrained-lomo-mean}

The forecast data used here contains forecasts of weekly incident
influenza hospitalizations in the US for Massachusetts (FIPS code 25)
and Texas (FIPS code 48), generated on November 19, 2022. These
forecasts are for two target end dates, November 26, 2022 (horizon 1),
and December 10, 2022 (horizon 3), and were produced by three models:
`Flusight-baseline', `MOBS-GLEAM\_FLUH', and `PSI-DICE'. The output type
is \texttt{mean} and the \texttt{output\_type\_id} column has
\texttt{NA}s as no further specification is required for this output
type. We have modified the example data slightly: the forecast values
were rounded to the nearest integer and some forecasts have been removed
to demonstrate the handling of missing values. Therefore,
`MOBS-GLEAM\_FLUH'\,'s forecast for Massachusetts on November 26, 2022,
and 'PSI-DICE's forecast for Texas on December 10, 2022, are missing.

\begin{verbatim}
> forecast_data
\end{verbatim}

\small

\begin{verbatim}
# A tibble: 10 x 9
   model_id          reference_date target          horizon location
   <chr>             <date>         <chr>             <int> <chr>   
 1 Flusight-baseline 2022-11-19     wk inc flu hosp       1 25      
 2 Flusight-baseline 2022-11-19     wk inc flu hosp       3 25      
 3 Flusight-baseline 2022-11-19     wk inc flu hosp       1 48      
 4 Flusight-baseline 2022-11-19     wk inc flu hosp       3 48      
 5 MOBS-GLEAM_FLUH   2022-11-19     wk inc flu hosp       3 25      
 6 MOBS-GLEAM_FLUH   2022-11-19     wk inc flu hosp       1 48      
 7 MOBS-GLEAM_FLUH   2022-11-19     wk inc flu hosp       3 48      
 8 PSI-DICE          2022-11-19     wk inc flu hosp       1 25      
 9 PSI-DICE          2022-11-19     wk inc flu hosp       3 25      
10 PSI-DICE          2022-11-19     wk inc flu hosp       1 48      
   target_end_date output_type output_type_id value
   <date>          <chr>       <chr>          <dbl>
 1 2022-11-26      mean        <NA>              51
 2 2022-12-10      mean        <NA>              53
 3 2022-11-26      mean        <NA>            1052
 4 2022-12-10      mean        <NA>            1053
 5 2022-12-10      mean        <NA>              47
 6 2022-11-26      mean        <NA>            1073
 7 2022-12-10      mean        <NA>             701
 8 2022-11-26      mean        <NA>              92
 9 2022-12-10      mean        <NA>             159
10 2022-11-26      mean        <NA>            1222
\end{verbatim}

\normalsize

The corresponding target data contains the observed hospitalization
counts for these dates and locations.

\begin{verbatim}
> target_data
\end{verbatim}

\small

\begin{verbatim}
# A tibble: 4 x 3
  target_end_date location oracle_value
  <date>          <chr>           <dbl>
1 2022-11-26      25                221
2 2022-11-26      48               1929
3 2022-12-10      25                578
4 2022-12-10      48               1781
\end{verbatim}

Overall, the forecasts tend to have larger prediction errors for the
target end date of December 10, 2022, compared to November 26, 2022,
which is expected due to increased uncertainty at longer horizons.
Additionally, the forecasts for Massachusetts are relatively more
accurate compared to those for Texas, which tend to have higher errors.

\normalsize

We can evaluate the importance of each model in the ensemble using the
\texttt{model\_importance()} function. The following code evaluates the
importance of each model in the simple mean ensemble using the LOMO
algorithm, without training the ensemble (\texttt{weighted\ =\ FALSE}).
The \texttt{na\_action} argument is set to \texttt{"drop"}, which
represents that any missing values in the forecasts will be excluded
from the evaluation.

\begin{verbatim}
> model_importance(
        forecast_data = forecast_data, oracle_output_data = target_data,
        ensemble_fun = "simple_ensemble", weighted = FALSE,
        importance_algorithm = "lomo", na_action = "drop"
        )
\end{verbatim}

This call generates both the result and informative messages,
summarizing the input data, including the number of dates on which
forecasts were produced and the number of models with ids as follows.

\small

\begin{verbatim}
Forecasts from 2022-11-19 to 2022-11-19 (a total of 1 forecast date(s)).
The available model IDs are:
     Flusight-baseline
     MOBS-GLEAM_FLUH
     PSI-DICE 
(a total of 3 models)
\end{verbatim}

\normalsize

The function output is a data frame containing model IDs and their
corresponding importance scores in the \texttt{mean\_importance} column,
ordered from most to least importance. The column name
\texttt{mean\_importance} indicates the average of task-specific
importance scores.

\small

\begin{verbatim}
# A tibble: 3 x 2
  model_id          mean_importance
  <chr>                       <dbl>
1 Flusight-baseline          69149.
2 PSI-DICE                   44303.
3 MOBS-GLEAM_FLUH          -113477.
\end{verbatim}

\normalsize

The results show that the model `Flusight-baseline' has the highest
importance score, followed by `PSI-DICE' and `MOBS-GLEAM\_FLUH'. That
is, `Flusight-baseline' contributes the most to improving the ensemble's
predictive performance, whereas MOBS-GLEAM\_FLUH, which has a negative
score, detracts from the ensemble's performance. The low importance
score of `MOBS-GLEAM\_FLUH' is mainly due to a substantially larger
prediction error for Texas on the target end date of December 10, 2022,
compared to other models, while its missing forecast for Massachusetts
for November 26, 2022, was not factored into the evaluation. This single
large error significantly affected its contribution score.

Another approach to handling missing values is to use the
\texttt{"worst"} option for \texttt{na\_action}, which replaces missing
values with the worst (i.e., minimum) score among the other models for
the same task.

\begin{verbatim}
> model_importance(
        forecast_data = forecast_data, oracle_output_data = target_data,
        ensemble_fun = "simple_ensemble", weighted = FALSE,
        importance_algorithm = "lomo", na_action = "worst"
        )
\end{verbatim}

\small

\begin{verbatim}
# A tibble: 3 x 2
  model_id          mean_importance
  <chr>                       <dbl>
1 Flusight-baseline          69149.
2 PSI-DICE                  -38581.
3 MOBS-GLEAM_FLUH           -86535.
\end{verbatim}

\normalsize

The results show that the importance scores of `Flusight-baseline' is
unchanged because it has no missing forecast. We observe that the
importance score of `PSI-DICE', which was previously positive, has now
decreased to a negative value when compared to the evaluation using the
\texttt{"drop"} option for \texttt{na\_action}. Moreover,
`MOBS-GLEAM\_FLUH' still ranks the lowest, but the importance score has
increased. This change is related to the varying forecast accuracy
across different tasks. For the target end date of November 26, 2022, in
Massachusetts, most forecasts are relatively accurate. Thus, even if the
`MOBS-GLEAM\_FLUH' is assigned the worst value of importance score for
its missing forecast, including this value in the averaging is not
detrimental to the overall importance metric; rather, it is more
beneficial than excluding it. In contrast, for the target end date of
December 10, 2022, in Texas, the forecasts have much larger errors
across the board, and assigning the worst value of importance score to
the missing forecast of `PSI-DICE' in this task has a detrimental effect
on averaging importance scores. This is because the scale of the
importance scores is influenced by the magnitude of the prediction
errors: for tasks with small errors, the scores remain moderate, while
tasks with large errors can yield importance scores of much greater
magnitude.

It is also possible to impute the missing scores with intermediate
values by assigning the average importance scores of other models in the
same task. This strategy may offer a more balanced trade-off by
mitigating the influence of the missing data without overly penalizing
or overlooking them.

\begin{verbatim}
> model_importance(
        forecast_data = forecast_data, oracle_output_data = target_data,
        ensemble_fun = "simple_ensemble", weighted = FALSE,
        importance_algorithm = "lomo", na_action = "average"
        )
\end{verbatim}

\small

\begin{verbatim}
# A tibble: 3 x 2
  model_id          mean_importance
  <chr>                       <dbl>
1 Flusight-baseline          69149.
2 PSI-DICE                   40971.
3 MOBS-GLEAM_FLUH           -85003.
\end{verbatim}

\normalsize

In this simple example with only three models, the ranking of the
importance scores remains unchanged. However, in more complex scenarios
with a larger number of models, the choice of \texttt{na\_action} can
impact the importance scores and their interpretation. More extensive
application can be found in \citet{kim2024}.

\subsection{Evaluation using trained ensemble in LASOMO algorithm for
quantile-based forecasts}\label{sec:trained-lomo-qntl}

TO DO

\section{Summary and discussion}\label{sec:discussion}

Multi-model ensemble forecasts often provide better accuracy and
robustness than single models, and are widely used in decision-making
and policy planning across various domains. The contribution of each
component model to the accuracy of the ensemble depends on its own
unique characteristics. The \pkg{modelimportance} package enables the
quantification of the value that each component model adds to the
ensemble performance in different evaluation contexts.

The primary function of the package is \texttt{model\_importance()},
which returns a data frame with component models and their importance
metrics. Users can choose the various ensemble methods to apply and
model importance algorithm between LOMO and LASOMO. Additionally,
customizable options are available for handling missing values. These
features enable the package to serve as a versatile tool to aid
collaborative efforts to construct an effective ensemble model across a
wide range of forecasting tasks.

There is a room to enhance the current version of this package. Although
this package supports four different output types (`mean', `median',
`quantile, and 'pmf'), other output types are widely used in practice.
For example, `sample' output type is commonly used in the US Flu
Scenario Modeling Hub \citep{FluSMH}. This format includes multiple
simulated values (samples) from the forecast distribution. The
\texttt{output\_type\_id} is specified for each sample, which typically
indexes the samples or indicates their source, depending on the context.
The package can be extended to support this output type, which is under
consideration for future releases. These extensions would aim to broaden
the scope of applications in real-world forecasting tasks.


  \bibliography{references.bib}



\end{document}
