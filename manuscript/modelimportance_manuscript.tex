% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  article,
  shortnames,
  notitle]{jss}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{orcidlink,thumbpdf,lmodern}

\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}
\usepackage{algorithm, algcompatible, setspace}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}{}
\makeatother
\makeatletter
\makeatother
\makeatletter
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[borderline west={3pt}{0pt}{shadecolor}, enhanced, boxrule=0pt, sharp corners, interior hidden, breakable, frame hidden]}{\end{tcolorbox}}\fi
\makeatother

\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={: An  package for evaluating model importance within an ensemble},
  pdfauthor={Minsu Kim; Evan Rays; Nicholas G. Reich},
  pdfkeywords={ensemble, forecast, prediction, model importance, Shapley
value, R},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


%% -- Article metainformation (author, title, ...) -----------------------------

%% Author information
\author{Minsu Kim~\orcidlink{0009-0008-4637-3589}\\University of
Massachusetts Amherst \And Evan
Rays~\orcidlink{0000-0003-4035-0243}\\CVS \AND Nicholas G.
Reich~\orcidlink{0000-0003-3503-9899}\\University of Massachusetts
Amherst}
\Plainauthor{Minsu Kim, Evan Rays, Nicholas G. Reich} %% comma-separated

\title{\pkg{modelimportance}: An \proglang{R} package for evaluating
model importance within an ensemble}
\Plaintitle{: An package for evaluating model importance within an
ensemble} %% without formatting

%% an abstract and keywords
\Abstract{Ensemble forecasts are commonly used to support
decision-making and policy planning across various fields because they
often offer improved accuracy and stability compared to individual
models. As each model has its own unique characteristics, understanding
and measuring the value each constituent model adds to the overall
accuracy of the ensemble is of great interest to building effective
ensembles. The \proglang{R} package \pkg{modelimportance} provides tools
to quantify how each component model contributes to the acccuracy of
ensemble performance for different forecast types. It supports multiple
functionalities; it allows users to specify which ensemble approach to
implement and which model importance metric to use. Additionally, the
software offers customizable options for handling missing values. These
features enable the package to serve as a versatile tool for researchers
and practitioners aiming to construct an effective ensemble model across
a wide range of forecasting tasks.}

%% at least one keyword must be supplied
\Keywords{ensemble, forecast, prediction, model importance, Shapley
value, R}
\Plainkeywords{ensemble, forecast, prediction, model importance, Shapley
value, R}

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}
%% \setcounter{page}{1}
%% \Pages{1--xx}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
Minsu Kim\\
E-mail: \email{minsu@umass.edu}\\
\\~
Evan Rays\\
\\~
Nicholas G. Reich\\
\\~

}

\begin{document}
\maketitle


\section{Introduction}\label{sec:intro}

Ensemble forecasting is a method to produce a single, consolidated
prediction by combining forecasts generated from different models. This
technique leverages the strengths of individual models while mitigating
their weaknesses, leading to more robust and accurate predictions
\citep{gneiting2005weather, hastie01statisticallearning, lutz_applying_2019, viboud_rapidd_2018}.
Specifically, ensembles aggregate diverse insights from various models
and handle variance and bias more effectively by averaging out the
biases and variations of individual models, which can reduce prediction
errors and improve overall model performance. Due to these enhanced
prediction accuracy and robustness, it is widely used across various
domains such as weather forecasting
\citep{Guerra_2020, gneiting2005weather}, financial modeling
\citep{SUN2020101160, math11041054}, and infectious disease outbreak
forecasting \citep{ray_prediction_2018, reich_accuracy_2019} to improve
decision-making and achieve more reliable predictions. For example,
throughout the COVID-19 pandemic, the US COVID-19 Forecast Hub collected
individual models developed by over 90 different research groups and
built a probabilistic ensemble forecasting model for COVID-19 cases,
hospitalizations, and deaths in the US based on those models'
predictions, which served as the official short-term forecasts for the
US Centers for Disease Control and Prevention (CDC) \citep{kim2024}.

The quality of forecasts is assessed by evaluating their error, bias,
sharpness, and/or calibration using different scoring metrics. The
selection of the scoring metrics depends on the type of forecast, such
as point forecasts and probabilistic forecasts, and their corresponding
formats, such as median/mean, quantiles, and predictive cumulative
distribution functions. The mean absolute error (MAE) and the mean
squared error (MSE) are the commonly used measures for point forecast
accuracy. They provide a direct assessment of the accuracy of specific
forecasted values by calculating the average magnitude of errors.
Probabilistic forecasts are typically assessed using proper scoring
rules, which consider the uncertainty and variability in predictions,
providing concise evaluations through numerical scores
\citep{gneiting_strictly_2007}. Some examples include the weighted
interval score (WIS) for the quantile-based forecasts and the continuous
ranked probability score (CRPS) for the forecasts taking the form of
predictive cumulative distribution functions
\citep{bracher_evaluating_2021}.

Several \proglang{R} packages have been developed for this purpose. To
name a few, the \pkg{forecast} package \citep{Rpackage-forecast} is
widely used for univariate time series forecasting and includes
functions for accuracy measurement. The \pkg{Metrics}
\citep{Rpackage-Metrics} and \pkg{MLmetrics} \citep{Rpackage-MLmetrics}
provide a wide range of performance metrics specifically designed for
evaluating machine learning models. The \pkg{scoringRules}
\citep{Rpackage-scoringRules} package offers a comprehensive set of
proper scoring rules for evaluating probabilistic forecasts and supports
both univariate and multivariate settings. The \pkg{scoringutils}
\citep{bosse2022evaluating} package adds to the functionality provided
by \pkg{scoringRules}, offering additional features that make it more
useful for certain tasks, such as summarizing, comparing, and
visualizing forecast performance. Additionally, the \pkg{scoringutils}
supports forecasts represented by predictive samples or quantiles of
predictive distributions, allowing for the assessment of any forecast
type, even when a closed-form expression for a parametric distribution
is not available. These packages have been valuable to evaluate
individual models as independent entities, using performance metrics
selected for each specific situation or problem type. However, they do
not measure the individual models' contributions to the enhanced
predictive accuracy when used as part of an ensemble. Kim et al.
\citep{kim2024} demonstrate that a model's individual performance does
not necessarily correspond to its contribution as a component within an
ensemble. Our developed package introduces this capability. The
\pkg{modelimportance} package provides tools to evaluate the role of
each model as an ensemble member within an ensemble model, rather than
focusing on the individual predictive performance per se.

In ensemble forecasting, certain models contribute more significantly to
the overall predictions than others. Assessing the impact of each
component model on ensemble predictions is methodologically similar to
determining variable importance in traditional regression and machine
learning models, where variable importance measures evaluate how much
individual variables enhance the model's predictive performance.
\proglang{R} packages that implement these functions include
\pkg{randomForest} \citep{Rpackage-randomForest}, \pkg{caret}
\citep{Rpackage-caret}, \pkg{xgboost} \citep{Rpackage-xgboost}, and
\pkg{gbm} \citep{Rpackage-gbm}, each providing variable importance
measures for different types of models: random forest models, general
machine learning models, extreme gradient boosting models, and
generalized boosted regression models, respectively. Similarly, the
tools in the \pkg{modelimportance} package quantify the contribution of
each component model within an ensemble to the ensemble model's
predictive performance. They assign numerical scores to each model based
on a selected metric that measures forecast accuracy, depending on the
forecast type.

These capabilities provide unique support for hub organizers, such as
the CDC in the US and the European Centre for Disease Prevention and
Control in the EU. By enabling precise evaluation of each model's
contribution, the \pkg{modelimportance} package helps these
organizations gather and utilize forecasts from various models to create
more effective ensemble forecasts. This, in turn, facilitates better
communication with the public and decision-makers and enhances the
overall decision-making process. Specifically, \pkg{modelimportance} is
incorporated into the `hubverse', which is a collection of open-source
software and data tools developed to promote collaborative modeling hub
efforts and reduce the effort required to set up and operate them
\citep{hubverse_docs}. This package adheres to the model output formats
specified by the hubverse convention, which enables seamless integration
and interoperability with other forecasting tools and systems.

The paper proceeds as follows. In Section 2, we address the model output
formats proposed within the hubverse framework and the structure of
forecasts, followed by a motivating example. Section 3 presents two
algorithms implemented in \pkg{modelimportance} for calculating the
model importance metric: leave-one-model-out and
leave-all-subsets-of-models-out. We demonstrate the various
functionalities \pkg{modelimportance} supports in Section 4 and give
some examples in Section 5. We close this paper with some concluding
remarks and a discussion of possible extensions.

\section{Data}\label{sec:data}

\subsection{Model output format}\label{subsec:model_output_format}

Model outputs are structured in a tabular format designed specifically
for predictions. In the hubverse standard, each row represents an
individual prediction for a single task, and its details are described
in multiple columns through which one can identify the model IDs, task
characteristics, prediction representation type, and predicted values
\citep{Shandross2024}. In Table~\ref{tbl-example-model_output}, for
example, the \texttt{model\_id} column contains the uniquely identified
name of the model that produced the prediction in each row. Task
characteristics are represented by \texttt{reference\_date},
\texttt{target}, \texttt{horizon}, \texttt{location}, and
\texttt{target\_end\_date} columns, collectively referred to as the task
ID columns. The prediction representation type is specified in the
\texttt{output\_type} and \texttt{output\_type\_id} columns. This
example illustrates short-term forecasts of incident influenza
hospitalizations in the US for Massachusetts (FIPS code 25), generated
by the model `Flusight-baseline' on November 19, 2022. The forecasts are
provided in seven quantiles (0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95) for
each target end date.

\begin{table}

\centering{

\centering
\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\begin{tabular}{lllrllllr}
\toprule
model\_id & reference\_date & target & horizon & location & target\_end\_date & output\_type & output\_type\_id & value\\
\midrule
Flusight-baseline & 2022-11-19 & wk inc flu hosp & 0 & 25 & 2022-11-19 & quantile & 0.05 & 22\\
Flusight-baseline & 2022-11-19 & wk inc flu hosp & 0 & 25 & 2022-11-19 & quantile & 0.1 & 31\\
Flusight-baseline & 2022-11-19 & wk inc flu hosp & 0 & 25 & 2022-11-19 & quantile & 0.25 & 45\\
Flusight-baseline & 2022-11-19 & wk inc flu hosp & 0 & 25 & 2022-11-19 & quantile & 0.5 & 51\\
Flusight-baseline & 2022-11-19 & wk inc flu hosp & 0 & 25 & 2022-11-19 & quantile & 0.75 & 57\\
\addlinespace
Flusight-baseline & 2022-11-19 & wk inc flu hosp & 0 & 25 & 2022-11-19 & quantile & 0.9 & 71\\
Flusight-baseline & 2022-11-19 & wk inc flu hosp & 0 & 25 & 2022-11-19 & quantile & 0.95 & 80\\
Flusight-baseline & 2022-11-19 & wk inc flu hosp & 1 & 25 & 2022-11-26 & quantile & 0.05 & 5\\
Flusight-baseline & 2022-11-19 & wk inc flu hosp & 1 & 25 & 2022-11-26 & quantile & 0.1 & 21\\
Flusight-baseline & 2022-11-19 & wk inc flu hosp & 1 & 25 & 2022-11-26 & quantile & 0.25 & 38\\
\bottomrule
\end{tabular}}

}

\caption{\label{tbl-example-model_output}Example of the model output for
incident influenza hospitalizations extracted from
\texttt{forecast\_outputs} data in the \pkg{hubExamples} package.}

\end{table}%

\subsection{Structure of forecasts}\label{subsec:structure_of_forecasts}

The output of the forecasting model can be represented in various types.
Generally, quantitative forecasts can be categorized into either point
forecasts or probabilistic forecasts. For a specific task, point
forecasts, represented by a single predicted value, provide a clear and
concise prediction, making them easy to interpret and communicate.
Probabilistic forecasts, on the other hand, describe the likelihood of
different outcomes, conveying the uncertainty inherent in the
prediction. These forecasts are represented by a probability
distribution over possible future values in various ways, such as
probability mass functions (pmf), cumulative distribution functions
(cdf), or probability quantiles (or intervals).

The \texttt{output\_type} and \texttt{output\_type\_id} columns in the
model output format, as defined by the hubverse convention, specify the
forecast structure. For example, \texttt{output\_type} may be `mean' or
`median' for point forecasts, and `pmf', `cdf', or `quantile' for
probabilistic forecasts. The \texttt{output\_type\_id} column provides
additional details for probabilistic forecasts; for instance, when the
\texttt{output\_type} is `quantile', the \texttt{output\_type\_id}
identifies specific quantile levels, as illustrated in
Table~\ref{tbl-example-model_output}. Different output types correspond
to different scoring rules for evaluating a model's prediction
performance. Table~\ref{tbl-pair-output-scoringrule} presents the output
types and their associated scoring rules supported by the
\pkg{modelimportance} package.

\begin{table}

\centering{

\begin{tabular}{ll>{\raggedright\arraybackslash}p{10cm}}
\toprule
Output Type & Scoring Rule & Description\\
\midrule
mean & MSE & Evaluate using the mean squared error (MSE)\\
median & MAE & Evaluate using the mean absolute error (MAE)\\
quantile & WIS & Evaluate using the weighted interval score (WIS)\\
pmf & Log Score & Evaluate using the logarithm of the probability assigned to the true outcome (LogScore)\\
\bottomrule
\end{tabular}

}

\caption{\label{tbl-pair-output-scoringrule}Pairs of output types and
their associated scoring rules for evaluating prediction performance.}

\end{table}%

\section{Algorithms}\label{sec:algorithms}

This section provides a brief description of the leave one model out
(LOMO) and leave all subsets of models out (LASOMO) algorithms. (Details
can be found in \citet{kim2024})

LOMO involves creating an ensemble by excluding one component model from
the entire set of models. Let \({\cal A}\) be a set of \(n\) models and
\(F^i\) be a forecast produced by model \(i\), where
\(i = 1,2, \dots, n.\) Each ensemble excludes exactly one model while
including all the others. Denoting by \(F^{{\cal A}^{-i}}\) an ensemble
forecast constructed without \(F^i\) and by \(F^{\cal A}\) the ensemble
forecast built from the entire set of models, the importance score using
LOMO is calculated by the difference of measures of these two ensemble
performances, \(F^{{\cal A}^{-i}}\) and \(F^{\cal A}\). For example,
when evaluating model 1 within an ensemble of three models (\(n=3\)),
LOMO creates an ensemble forecast \(F^{\{2,3\}}\) using only \(F^2\) and
\(F^3\). The performance of this reduced ensemble is then compared to
the full ensemble forecast \(F^{\{1,2,3\}}\), which incorporates all
three models.

On the other hand, LASOMO involves ensemble constructions from all
possible subsets of models. For each subset \(S\) that does not contain
the model \(i\), \(S \cup \{i\}\) plays a role of \({\cal A}\) in the
LOMO; the score associated with the subset \(S\) is the difference of
measures between \(F^S\) and \(F^{S \cup \{i\}}\). Then, all scores are
aggregated across all possible subsets that the model \(i\) does not
belong to. For example, using the earlier setup of three forecast
models, LASOMO considers three subsets, \(\{2\}\), \(\{3\}\), and
\(\{2, 3\}\), to calculate the importance score of model 1 (excluding
all subsets that include model 1). The ensemble forecasts
\(F^{\{2\}}, F^{\{3\}}\), and \(F^{\{2,3\}}\) are then compared to
\(F^{\{1,2\}}, F^{\{1,3\}}\), and \(F^{\{1,2,3\}}\), respectively. The
performance differences attributable to model 1's inclusion are
aggregated, which results in the importance score of model 1. We note
that the subsets may have different weights during the aggregating
process. The \pkg{modelimportance} package considers two cases: all
subsets may have uniform weights, or weights may be assigned based on
the size of the subsets, similar to the concept of Shapley values. This
flexibility allows for different approaches to account for the
contribution of each model depending on the desired evaluation
framework.

\begin{algorithm}[bh!]
\caption{Importance score using leave one model out (LOMO) algorithm} 
\label{alg:lomo}
 \begin{spacing}{1.2}
  \begin{algorithmic}[1]
   \REQUIRE{
    \Statex $\bullet$ Set of $n$ individual models, ${\cal A}=\{1,2,\dots, n\},$ and their forecasts $\{F^1,F^2,...,F^n\}$
    \Statex $\bullet$ Truth value, $y$
    \Statex $\bullet$ Function $g$ to build an ensemble
    \Statex $\bullet$ Scoring rule $\psi$ to evaluate forecast skills}
   \ENSURE importance metric of model $i$, $\phi^{i,\text{lomo}}$
    \STATE{Create an ensemble forecast $F^{\cal A}$ using $g$: $F^{\cal A} \gets g(\{F^1,F^2,...,F^n\})$}
    \STATE{Evaluate $F^{\cal A}$ using $\psi$: $\psi(F^{\cal A},y).$}
    \FOR{each $i, (i = 1,2,..., n)$}
      \STATE {$F^{{\cal A}^{-i}} \gets g(\{F^j|\, j\ne i, j\in {\cal A}\})$}
      \STATE{Compute $\psi(F^{{\cal A}^{-i}},y).$}
      \STATE $\phi^{i,\text{lomo}} \gets   \psi(F^{{\cal A}^{-i}},y) - \psi(F^{\cal A},y)$
    \ENDFOR
  \end{algorithmic}
 \end{spacing}
\end{algorithm}

Algorithms \ref{alg:lomo} and \ref{alg:lasomo} outline the steps to
implement LOMO and LASOMO for a single prediction task, respectively.

\begin{algorithm}[bh!]
\caption{Importance score using leave all subsets of models out (LASOMO) algorithm} 
\label{alg:lasomo}
 \begin{spacing}{1.2}
  \begin{algorithmic}[1]
   \REQUIRE{
    \Statex $\bullet$ Set of $n$ individual models, ${\cal A}=\{1,2,\dots, n\},$ and their forecasts $\{F^1,F^2,...,F^n\}$
    \Statex $\bullet$ Truth value, $y$
    \Statex $\bullet$ Function $g$ to build an ensemble
    \Statex $\bullet$ Scoring rule $\psi$ to evaluate forecast skills}
   \ENSURE Importance metric of model $i$, $\phi^{i,\text{lasomo}}$
   \FOR{$i = 1 \text{ to } n$}
     \STATE $\phi^{i,\text{lasomo}} \gets 0$
     \STATE Make a list of non-empty subsets of $\cal A$ that does not contain $i$: $S_1, S_2, \cdots, S_{2^{n-1}-1}$
     \FOR{$j = 1 \text{ to } 2^{n-1}-1$}
       \STATE{Assign a weight to the subset $S_j$: }
         \IF{all subsets have uniform weights}
           \STATE $\gamma_{S_j} \gets \displaystyle\frac{1}{2^{n-1}-1}$
         \ELSE{ (\textit{subset's weight depends on its size})}
           \STATE {$\gamma_{S_j} \gets \displaystyle\frac{1}{(n-1)\binom{n-1}{|S_j|}}$}
         \ENDIF
       \STATE{$F^{S_j}\gets g(\{F^j|\, j\in S_j\})$}
       \STATE{$F^{{S_j}\cup \{i\}}\gets g(\{F^i, F^j|\, j\in S_j\})$}
       \STATE{Compute $\psi(F^{S_j},y)$ and
       $\psi(F^{{S_j}\cup\{i\}},y)$}
       \STATE {$\phi^{i,\text{lasomo}} \gets \phi^{i,\text{lasomo}} + \gamma_{S_j}\times\Big[ \psi(F^{S_j},y) - \psi(F^{{S_j}\cup\{i\}},y) \Big]$}
     \ENDFOR
   \ENDFOR
  \end{algorithmic}
 \end{spacing}
\end{algorithm}

\section{Main funtion: Evaluating ensemble
members}\label{sec:main-function}

In this section, we describe the functionalities of the main function
\texttt{model\_importance()}, where multiple options are available to
customize the evaluation framework (Table~\ref{tbl-arguments}).

\begin{verbatim}
model_importance(forecast_data, oracle_output_data, ensemble_fun, weighted,
                 training_window_length, importance_algorithm, subset_wt, 
                 na_action, ...)
\end{verbatim}

\begin{table}

\centering{

\centering\begingroup\fontsize{10.5}{12.5}\selectfont

\begin{tabular}{l>{\raggedright\arraybackslash}p{3.8cm}>{\raggedright\arraybackslash}p{3.5cm}>{\raggedright\arraybackslash}p{2.7cm}}
\toprule
Argument & Description & Possible Values & Default\\
\midrule
\cellcolor{gray!10}{\texttt{forecast\_data}} & \cellcolor{gray!10}{Forecasts} & \cellcolor{gray!10}{Must be the model output format} & \cellcolor{gray!10}{N/A}\\
\texttt{oracle\_output\_data} & Ground truth data & Must be the oracle output format & N/A\\
\cellcolor{gray!10}{\texttt{ensemble\_fun}} & \cellcolor{gray!10}{Ensemble method} & \cellcolor{gray!10}{\texttt{simple\_ensemble}, \texttt{linear\_pool}} & \cellcolor{gray!10}{\texttt{simple\_ensemble}}\\
\texttt{training\_window\_length} & Time interval of historical data used during the training process & Non-negative integer & 0\\
\cellcolor{gray!10}{\texttt{importance\_algorithm}} & \cellcolor{gray!10}{Algorithm to calculate importance} & \cellcolor{gray!10}{\texttt{lomo, lasomo}} & \cellcolor{gray!10}{\texttt{lomo}}\\
\addlinespace
\texttt{subset\_wt} & Method for assigning weight to subsets when using lasomo algorithm & \texttt{equal, perm\_based} & \texttt{equal}\\
\cellcolor{gray!10}{\texttt{na\_action}} & \cellcolor{gray!10}{Method to handle missing data} & \cellcolor{gray!10}{\texttt{worst, average, drop}} & \cellcolor{gray!10}{\texttt{worst}}\\
\texttt{...} & Optional arguments for \texttt{simple\_ensemble} & Varies & \texttt{agg\_fun = mean}\\
\bottomrule
\end{tabular}
\endgroup{}

}

\caption{\label{tbl-arguments}Description of the arguments for the
\texttt{model\_importance()} function, including their purpose, possible
values, and default settings.}

\end{table}%

\texttt{forecast\_data} is a data frame containing predictions and
should be or can be coerced to a \texttt{model\_out\_tbl} format, which
is the standard S3 class model output format defined by the hubverse
convention. Only one \texttt{output\_type} is allowed in the data frame,
and it must be one of the \texttt{mean}, \texttt{median},
\texttt{quantile}, or \texttt{pmf}.

\texttt{oracle\_output\_data} is a data frame containing the ground
truth data for the variables that are used to define modeling targets.
This data must follow the oracle output format, which includes
independent task ID columns (e.g., \texttt{location},
\texttt{target\_date}, and \texttt{age\_group}), the
\texttt{output\_type} column specifying the output type of the
predictions and an \texttt{oracle\_value} column for the observed
values. If the \texttt{output\_type} is either \texttt{"quantile"} or
\texttt{"pmf"}, the \texttt{output\_type\_id} column is required to
provide further identifying information. For \texttt{"quantile"}, it
should contain numeric values between 0 and 1 indicating quantile levels
(e.g., ``0.1'', ``0.25'', ``0.5'', ``0.75'', ``0.9''). For
\texttt{"pmf"}, it should contain categorical values such as ``low'',
``moderate'', ``high'', and ``very high''.

The \texttt{forecast\_data} and \texttt{oracle\_output\_data} must have
the same task ID columns and \texttt{output\_type}, including
\texttt{output\_type\_id} if necessary, which are used to match the
predictions with the ground truth data. As aforementioned, these data
frames should follow the model output format and the oracle output
format, respectively, as defined by the hubverse convention.

The \texttt{ensemble\_fun} argument specifies the ensemble method to be
used for evaluating model importance. The currently supported methods
are \texttt{"simple\_ensemble"} and \texttt{"linear\_pool"}. The
\texttt{"simple\_ensemble"} method returns the average of the predicted
values from all component models per prediction task defined by task
IDs, \texttt{output\_type}, and \texttt{output\_type\_id} columns. The
default aggregation function for this method is \texttt{"mean"}, but it
can be customized by specifying additional arguments through
\texttt{...}, such as \texttt{agg\_fun="median"}. When
\texttt{"linear\_pool"} is specified, ensemble model outputs are created
as a linear pool of component model outputs. This method supports only
an \texttt{output\_type} of \texttt{"mean"}, \texttt{"quantile"}, or
\texttt{"pmf"}.

The \texttt{weighted} argument is a logical value that indicates whether
model weighting should be done when building an ensemble using the
specified \texttt{ensemble\_fun}. If it is set to \texttt{TRUE}, model
weights are estimated based on the previous performance of each model,
and these weights are used to build the ensemble.

The \texttt{importance\_algorithm} argument specifies the algorithm for
model importance calculation, which can be either \texttt{"lomo"}
(leave-one-model-out) and \texttt{"lasomo"} (leave all subsets of models
out). The \texttt{subset\_wt} argument is employed only for the
\texttt{"lasomo"} algorithm. This argument has two options:
\texttt{"equal"} assigns equal weight to all subsets and
\texttt{"perm\_based"} assigns weight averaged over all possible
permutations as in the formula of Shapley values (Algorithm
\ref{alg:lasomo}). The default values of \texttt{importance\_algorithm}
and \texttt{subset\_wt} are \texttt{"lomo"} and \texttt{"equal"},
respectively.

The \texttt{na\_action} argument allows for specifying how to handle
missing values in the \texttt{forecast\_data}. Three options are
available: \texttt{"worst"}, \texttt{"average"}, and \texttt{"drop"}. In
each specific prediction task, if a model has any missing predictions,
the \texttt{"worst"} option replaces those missing values with the
smallest value from the other models, while the \texttt{"average"}
option replaces them with the average of the other models' predictions
in that task. The \texttt{"drop"} option removes missing values, which
results in the exclusion of the model from the evaluation for that task.

\section{Examples}\label{sec:examples}

The examples in this section illustrate the use of the
\texttt{model\_importance()} function to evaluate the importance of
component models within an ensemble, using various combinations of the
arguments described in Section \ref{sec:main-function}.

\subsection{Evaluation using untrained ensemble in LOMO algorithm for
quantile-based forecasts}\label{sec:untrained-lomo-qntl}

\subsection{Evaluation using untrained ensemble in LASOMO algorithm for
mean forecasts}\label{sec:untrained-lasomo-mean}

\subsection{Evaluation using trained ensemble in LOMO algorithm for
quantile-based forecasts}\label{sec:trained-lomo-qntl}

TO DO

\subsection{Evaluation using trained ensemble in LOMO algorithm for mean
forecasts}\label{sec:trained-lasomo-mean}

TO DO

\section{Summary and discussion}\label{sec:discussion}

Multi-model ensemble forecasts often provide better accuracy and
robustness than single models, and are widely used in decision-making
and policy planning across various domains. The contribution of each
component model to the accuracy of the ensemble depends on its own
unique characteristics. The \pkg{modelimportance} package enables the
quantification of the value that each component model adds to the
ensemble performance in different evaluation contexts.

The primary function of the package is \texttt{model\_importance()},
which returns a data frame with component models and their importance
metrics. It allows users to choose the ensemble approach and model
importance algorithm, selecting between LOMO and LASOMO. Additionally,
it provides customizable options for handling missing values. These
features enable the package to serve as a versatile tool to aid
collaborative efforts to construct an effective ensemble model across a
wide range of forecasting tasks.

There is a room to enhance the current version of this package.

\(\vdots\)

These extensions would aim to broaden the scope of applications in
real-world forecasting tasks.


  \bibliography{references.bib}



\end{document}
