---
title: "[modelimportance]{.pkg}: An [R]{.proglang} package for evaluating model importance within an ensemble"
format:
  jss-pdf:
    keep-tex: true
    header-includes:
        - \usepackage{algorithm, algcompatible, setspace}
        - \renewcommand{\algorithmicrequire}{\textbf{Input:}}
        - \renewcommand{\algorithmicensure}{\textbf{Output:}}
    journal:
      type: article
      cite-shortnames: true
      suppress: [title]
      include-jss-default: false
author:
  - name: Minsu Kim
    affiliations: 
      - name: University of Massachusetts Amherst
    orcid: 0009-0008-4637-3589
    email: minsu@umass.edu
  - name: Evan Rays
    affiliations: 
      - name: CVS
    orcid: 0000-0003-4035-0243
  - name: Nicholas G. Reich
    affiliations:
      - University of Massachusetts Amherst
    orcid: 0000-0003-3503-9899
execute:
  echo: true
  warning: false
  message: false
bibliography: references.bib
abstract: |
  Ensemble forecasts are commonly used to support decision-making and policy planning across various fields because they often offer improved accuracy and stability compared to individual models. As each model has its own unique characteristics, understanding and measuring the value each constituent model adds to the overall accuracy of the ensemble is of great interest to building effective ensembles. The [R]{.proglang} package [modelimportance]{.pkg}  provides tools to quantify how each component model contributes to the acccuracy of ensemble performance for different forecast types. It supports multiple functionalities; it allows users to specify which ensemble approach to implement and which model importance metric to use. Additionally, the software offers customizable options for handling missing values. These features enable the package to serve as a versatile tool for researchers and practitioners aiming to construct an effective ensemble model across a wide range of forecasting tasks.
keywords: [ensemble, forecast, prediction, model importance, Shapley value, R]
keywords-formatted: [ensemble, forecast, prediction, model importance, Shapley value, R]
---

```{r}
#| echo: false
library(hubEnsembles)
library(hubUtils)
library(hubExamples)
library(readr)
library(dplyr)
library(tidyr)
library(lubridate)
library(here)
library(ggplot2)
library(kableExtra)
library(modelimportance)
options(tibble.width = Inf)
```


## Introduction {#sec:intro}

Ensemble forecasting is a method to produce a single, consolidated prediction by combining forecasts generated from different models. This technique it utilizes each model’s strengths to counterbalance their individual weaknesses, leading to more robust and accurate predictions [@gneiting2005weather; @hastie01statisticallearning; @lutz_applying_2019; @viboud_rapidd_2018].
Specifically, ensembles aggregate diverse insights from various models and effectively handle individual models models biases and variances by averaging them out, which can reduce prediction errors and improve overall model performance. These enhanced prediction accuracy and robustness enable ensemble forecasting to be widely used across various domains such as weather forecasting [@Guerra_2020; @gneiting2005weather], financial modeling [@SUN2020101160; @math11041054], and infectious disease outbreak forecasting [@ray_prediction_2018; @reich_accuracy_2019] to improve decision-making and achieve more reliable predictions. For example, throughout the COVID-19 pandemic, the US COVID-19 Forecast Hub collected individual models developed by over 90 different research groups and built a probabilistic ensemble forecasting model for COVID-19 cases, hospitalizations, and deaths in the US based on those models’ predictions, which served as the official short-term forecasts for the US Centers for Disease Control and Prevention (CDC) [@kim2024].

The quality of forecasts is assessed by evaluating their error, bias, sharpness, and/or calibration using different scoring metrics.
The selection of the scoring metrics depends on the type of forecast, such as point forecasts and probabilistic forecasts, and their corresponding formats, such as median/mean, quantiles, and predictive cumulative distribution functions. 
In the case of point forecasts, accuracy is commonly measured by the mean absolute error (MAE) and the mean squared error (MSE), which are direct assessment tools that calculate the average magnitude of forecast errors. 
Probabilistic forecasts are typically assessed using proper scoring rules, which consider the uncertainty and variability in predictions, providing concise evaluations through numerical scores [@gneiting_strictly_2007].  Some examples include the weighted interval score (WIS) for the quantile-based forecasts and the continuous ranked probability score (CRPS) for the forecasts taking the form of predictive cumulative distribution functions [@bracher_evaluating_2021]. 

Several [R]{.proglang} packages have been developed for this purpose. To name a few, the [forecast]{.pkg} package [@Rpackage-forecast] is widely used for univariate time series forecasting and includes functions for accuracy measurement. The [Metrics]{.pkg} [@Rpackage-Metrics] and [MLmetrics]{.pkg} [@Rpackage-MLmetrics] provide a wide range of performance metrics specifically designed for evaluating machine learning models. The [scoringRules]{.pkg} [@Rpackage-scoringRules] package offers a comprehensive set of proper scoring rules for evaluating probabilistic forecasts and supports both univariate and multivariate settings. 
The [scoringutils]{.pkg} [@bosse2022evaluating] package offers additional features to the functionality provided by
[scoringRules]{.pkg}, which makes it more useful for certain tasks, such as summarizing, comparing, and visualizing forecast performance. Additionally, the [scoringutils]{.pkg} supports forecasts represented by predictive samples or quantiles of predictive distributions, allowing for the assessment of any forecast type, even when a closed-form expression for a parametric distribution is not available.
These packages have been valuable to evaluate individual models as independent entities, using performance metrics selected for each specific situation or problem type. However, they do not measure the individual models' contributions to the enhanced predictive accuracy when used as part of an ensemble. 
Kim et al. [@kim2024] demonstrate that a model's individual performance does not necessarily correspond to its contribution as a component within an ensemble.
Our developed package introduces this capability.
The [modelimportance]{.pkg} package provides tools to evaluate the role of each model as an ensemble member within an ensemble model, rather than focusing on the individual predictive performance per se. 

In ensemble forecasting, certain models contribute more significantly to the overall predictions than others. Assessing the impact of each component model on ensemble predictions is methodologically similar to determining variable importance in traditional regression and machine learning models, where variable importance measures evaluate how much individual variables enhance the model's predictive performance. [R]{.proglang} packages that implement these functions include [randomForest]{.pkg} [@Rpackage-randomForest], [caret]{.pkg} [@Rpackage-caret], [xgboost]{.pkg} [@Rpackage-xgboost], and [gbm]{.pkg} [@Rpackage-gbm], each providing variable importance measures for different types of models: random forest models, general machine learning models, extreme gradient boosting models, and generalized boosted regression models, respectively.
Similarly, the tools in the [modelimportance]{.pkg} package quantify the contribution of each component model within an ensemble to the ensemble model’s predictive performance. They assign numerical scores to each model based on a selected metric that measures forecast accuracy, depending on the forecast type. 

These capabilities provide unique support for hub organizers, such as the CDC in the US and the European Centre for Disease Prevention and Control in the EU. These organizations can create more effective ensemble forecasts by gathering and utilizing forecasts from various models based on the precise evaluation of each model's contribution via the [modelimportance]{.pkg} package. This, in turn, facilitates better communication with the public and decision-makers and enhances the overall decision-making process. Specifically, [modelimportance]{.pkg} is incorporated into the `hubverse', which is a collection of open-source software and data tools developed to promote collaborative modeling hub efforts and reduce the effort required to set up and operate them [@hubverse_docs]. This package adheres to the model output formats specified by the hubverse convention, which enables seamless integration and interoperability with other forecasting tools and systems.

The paper proceeds as follows. In Section 2, we address the model output formats proposed within the hubverse framework and the structure of forecasts, followed by a motivating example. Section 3 presents two algorithms implemented in [modelimportance]{.pkg} for calculating the model importance metric: leave-one-model-out and leave-all-subsets-of-models-out. We demonstrate the various functionalities [modelimportance]{.pkg} supports in Section 4 and give some examples in Section 5. We close this paper with some concluding remarks and a discussion of possible extensions.   


## Data {#sec:data}

### Model output format {#subsec:model_output_format}
Model outputs are structured in a tabular format designed specifically for predictions.
In the hubverse standard, each row represents an individual prediction for a single task, and its details are described in multiple columns through which one can identify the model IDs, task characteristics, prediction representation type, and predicted values [@Shandross2024]. In @tbl-example-model_output, for example, the `model_id` column contains the uniquely identified name of the model that produced the prediction in each row. Task characteristics are represented by `reference_date`, `target`, `horizon`, `location`, and `target_end_date` columns, collectively referred to as the task ID columns. The prediction representation type is specified in the `output_type` and `output_type_id` columns. This example illustrates short-term forecasts of incident influenza hospitalizations in the US for Massachusetts (FIPS code 25), generated by the model 'Flusight-baseline' on November 19, 2022. The forecasts are provided in seven quantiles (0.05, 0.1, 0.25,  0.5, 0.75, 0.9, 0.95) for each target end date.  


```{r example-model_output}
#| echo: false
#| label: tbl-example-model_output
#| tbl-cap: "Example of the model output for incident influenza
#| hospitalizations extracted from `forecast_outputs` data in the
#| [hubExamples]{.pkg} package."

forecast_data <- hubExamples::forecast_outputs |>
  dplyr::filter(
    output_type %in% c("quantile"),
    location == "25"
  )

forecast_data |>
  head(10) |>
  knitr::kable(format = "latex", booktabs = TRUE) |>
  kable_styling(latex_options = c("scale_down"))
```

### Structure of forecasts {#subsec:structure_of_forecasts}

The forecasting model supports multiple output types. Generally, quantitative forecasts can be categorized into either point forecasts or probabilistic forecasts. 
For a specific task, point forecasts, represented by a single predicted value, provide a clear and concise prediction, making them easy to interpret and communicate. 
Probabilistic forecasts, on the other hand, describe the likelihood of different outcomes, conveying the uncertainty inherent in the prediction. These forecasts are represented by a probability distribution over possible future values in various ways, such as probability mass functions (pmf), cumulative distribution functions (cdf), or probability quantiles (or intervals). 

The `output_type` and `output_type_id` columns in the model output format, as defined by the hubverse convention, specify the forecast structure. 
The `output_type` column takes the character value 'mean' or 'median' for point forecasts, and 'pmf', 'cdf', or 'quantile' for probabilistic forecasts. The `output_type_id` column provides additional details for probabilistic forecasts; for instance, when the `output_type` is 'quantile', the `output_type_id` identifies specific quantile levels, as illustrated in @tbl-example-model_output. In the case of point forecasts, `output_type_id` column has 'NA'. 
Different output types correspond to different scoring rules for evaluating a model's prediction performance. @tbl-pair-output-scoringrule presents the output types and their associated scoring rules supported by the [modelimportance]{.pkg} package.


```{r}
#| echo: false
#| label: tbl-pair-output-scoringrule
#| tbl-cap: "Pairs of output types and their associated scoring rules for evaluating prediction performance."

data.frame(
  "Output Type" = c("mean", "median", "quantile", "pmf"),
  "Scoring Rule" = c("MSE", "MAE", "WIS", "Log Score"),
  Description = c(
    "Evaluate using the mean squared error (MSE)",
    "Evaluate using the mean absolute error (MAE)",
    "Evaluate using the weighted interval score (WIS)",
    "Evaluate using the logarithm of the probability assigned to the true outcome (LogScore)"
  ),
  check.names = FALSE
) |>
  knitr::kable(format = "latex", booktabs = TRUE) |>
  column_spec(3, width = "10cm")
```



## Algorithms {#sec:algorithms}

This section provides a brief description of the leave one model out (LOMO) and leave all subsets of models out (LASOMO) algorithms. (Details can be found in @kim2024) 

LOMO involves creating an ensemble by excluding one component model from the entire set of models. 
Let ${\cal A}$ be a set of $n$ models and $F^i$ be a forecast produced by model $i$, where $i = 1,2, \dots, n.$
Each ensemble excludes exactly one model while including all the others. Denoting by $F^{{\cal A}^{-i}}$ an ensemble forecast constructed without $F^i$ and by $F^{\cal A}$ the ensemble forecast built from the entire set of models, the importance score using LOMO is calculated by the difference of measures of these two ensemble performances, $F^{{\cal A}^{-i}}$ and $F^{\cal A}$. 
For example, when evaluating model 1 within an ensemble of three models ($n=3$), LOMO creates an ensemble forecast $F^{\{2,3\}}$ using only $F^2$ and $F^3$. The performance of this reduced ensemble is then compared to the full ensemble forecast $F^{\{1,2,3\}}$, which incorporates all three models.

On the other hand, LASOMO involves ensemble constructions from all possible subsets of models. For each subset $S$ that does not contain the model $i$, $S \cup \{i\}$ plays a role of ${\cal A}$ in the LOMO; the score associated with the subset $S$ is the difference of measures between $F^S$ and $F^{S \cup \{i\}}$. Then, all scores are aggregated across all possible subsets that the model $i$ does not belong to.
For example, using the earlier setup of three forecast models, LASOMO considers three subsets, $\{2\}$, $\{3\}$, and $\{2, 3\}$, to calculate the importance score of model 1 (excluding all subsets that include model 1). The ensemble forecasts $F^{\{2\}}, F^{\{3\}}$, and $F^{\{2,3\}}$ are then compared to $F^{\{1,2\}}, F^{\{1,3\}}$, and $F^{\{1,2,3\}}$, respectively. The performance differences attributable to model 1's inclusion are aggregated, which results in the importance score of model 1. We note that the subsets may have different weights during the aggregating process. 
The [modelimportance]{.pkg} package offers two weighting options for subsets: one assigns equal (uniform) weights to all subsets, and the other assigns weights based on their size, similar to the concept of Shapley values. Users can choose one to evaluate the contribution of each model in a manner suited to their preferred framework.

\begin{algorithm}[bh!]
\caption{Importance score using leave one model out (LOMO) algorithm} 
\label{alg:lomo}
 \begin{spacing}{1.2}
  \begin{algorithmic}[1]
   \REQUIRE{
    \Statex $\bullet$ Set of $n$ individual models, ${\cal A}=\{1,2,\dots, n\},$ and their forecasts $\{F^1,F^2,...,F^n\}$
    \Statex $\bullet$ Truth value, $y$
    \Statex $\bullet$ Function $g$ to build an ensemble
    \Statex $\bullet$ Scoring rule $\psi$ to evaluate forecast skills}
   \ENSURE importance metric of model $i$, $\phi^{i,\text{lomo}}$
    \STATE{Create an ensemble forecast $F^{\cal A}$ using $g$: $F^{\cal A} \gets g(\{F^1,F^2,...,F^n\})$}
    \STATE{Evaluate $F^{\cal A}$ using $\psi$: $\psi(F^{\cal A},y).$}
    \FOR{each $i, (i = 1,2,..., n)$}
      \STATE {$F^{{\cal A}^{-i}} \gets g(\{F^j|\, j\ne i, j\in {\cal A}\})$}
      \STATE{Compute $\psi(F^{{\cal A}^{-i}},y).$}
      \STATE $\phi^{i,\text{lomo}} \gets   \psi(F^{{\cal A}^{-i}},y) - \psi(F^{\cal A},y)$
    \ENDFOR
  \end{algorithmic}
 \end{spacing}
\end{algorithm}


Algorithms \ref{alg:lomo} and \ref{alg:lasomo} outline the steps to implement LOMO and LASOMO for a single prediction task, respectively.

\begin{algorithm}[bh!]
\caption{Importance score using leave all subsets of models out (LASOMO) algorithm} 
\label{alg:lasomo}
 \begin{spacing}{1.2}
  \begin{algorithmic}[1]
   \REQUIRE{
    \Statex $\bullet$ Set of $n$ individual models, ${\cal A}=\{1,2,\dots, n\},$ and their forecasts $\{F^1,F^2,...,F^n\}$
    \Statex $\bullet$ Truth value, $y$
    \Statex $\bullet$ Function $g$ to build an ensemble
    \Statex $\bullet$ Scoring rule $\psi$ to evaluate forecast skills}
   \ENSURE Importance metric of model $i$, $\phi^{i,\text{lasomo}}$
   \FOR{$i = 1 \text{ to } n$}
     \STATE $\phi^{i,\text{lasomo}} \gets 0$
     \STATE Make a list of non-empty subsets of $\cal A$ that does not contain $i$: $S_1, S_2, \cdots, S_{2^{n-1}-1}$
     \FOR{$j = 1 \text{ to } 2^{n-1}-1$}
       \STATE{Assign a weight to the subset $S_j$: }
         \IF{all subsets have uniform weights}
           \STATE $\gamma_{S_j} \gets \displaystyle\frac{1}{2^{n-1}-1}$
         \ELSE{ (\textit{subset's weight depends on its size})}
           \STATE {$\gamma_{S_j} \gets \displaystyle\frac{1}{(n-1)\binom{n-1}{|S_j|}}$}
         \ENDIF
       \STATE{$F^{S_j}\gets g(\{F^j|\, j\in S_j\})$}
       \STATE{$F^{{S_j}\cup \{i\}}\gets g(\{F^i, F^j|\, j\in S_j\})$}
       \STATE{Compute $\psi(F^{S_j},y)$ and
       $\psi(F^{{S_j}\cup\{i\}},y)$}
       \STATE {$\phi^{i,\text{lasomo}} \gets \phi^{i,\text{lasomo}} + \gamma_{S_j}\times\Big[ \psi(F^{S_j},y) - \psi(F^{{S_j}\cup\{i\}},y) \Big]$}
     \ENDFOR
   \ENDFOR
  \end{algorithmic}
 \end{spacing}
\end{algorithm}


## Main funtion: Evaluating ensemble members {#sec:main-function}

In this section, we describe the functionalities of the main function `model_importance()`, where multiple options are available to customize the evaluation framework (@tbl-arguments).

```r
> model_importance(forecast_data, oracle_output_data, ensemble_fun, weighted,
                   training_window_length, importance_algorithm, subset_wt, 
                   na_action, ...)
```


```{r}
#| echo: false
#| label: tbl-arguments
#| tbl-cap: "Description of the arguments for the `model_importance()` function, including their purpose, possible values, and default settings."

tbl_argument <- data.frame(
  "Argument" = c(
    "\\texttt{forecast\\_data}", "\\texttt{oracle\\_output\\_data}",
    "\\texttt{ensemble\\_fun}", "\\texttt{training\\_window\\_length}",
    "\\texttt{importance\\_algorithm}", "\\texttt{subset\\_wt}",
    "\\texttt{na\\_action}", "\\texttt{...}"
  ),
  "Description" = c(
    "Forecasts",
    "Ground truth data",
    "Ensemble method",
    "Time interval of historical data used during the training process",
    "Algorithm to calculate importance",
    "Method for assigning weight to subsets when using lasomo algorithm",
    "Method to handle missing data",
    "Optional arguments for \\texttt{simple\\_ensemble}"
  ),
  "Possible Values" = c(
    "Must be the model output format",
    "Must be the oracle output format",
    "\\texttt{simple\\_ensemble}, \\texttt{linear\\_pool}",
    "Non-negative integer",
    "\\texttt{lomo, lasomo}",
    "\\texttt{equal, perm\\_based}",
    "\\texttt{worst, average, drop}",
    "Varies"
  ),
  Default = c(
    "N/A",
    "N/A",
    "\\texttt{simple\\_ensemble}",
    "0",
    "\\texttt{lomo}",
    "\\texttt{equal}",
    "\\texttt{worst}",
    "\\texttt{agg\\_fun = mean}"
  ),
  check.names = FALSE
)


tbl_argument |>
  knitr::kable(format = "latex", booktabs = TRUE, escape = FALSE) |>
  column_spec(c(2, 3, 4), width = c("3.8cm", "3.5cm", "2.7cm")) |>
  kable_styling(latex_options = c("striped", "t"), font_size = 10.5)
```

`forecast_data` is a data frame containing predictions and should be or can be coerced to a `model_out_tbl` format, which is the standard S3 class model output format defined by the hubverse convention. Only one `output_type` is allowed in the data frame, and it must be one of the `mean`, `median`, `quantile`, or `pmf`.

`oracle_output_data` is a data frame containing the ground truth data for the variables that are used to define modeling targets. This data must follow the oracle output format, which includes independent task ID columns (e.g., `location`, `target_date`, and `age_group`), the `output_type` column specifying the output type of the predictions and an `oracle_value` column for the observed values. If the `output_type` is either `"quantile"` or `"pmf"`, the `output_type_id` column is required to provide further identifying information. For `"quantile"`, it should contain numeric values between 0 and 1 indicating quantile levels (e.g., "0.1", "0.25", "0.5", "0.75", "0.9"). For `"pmf"`, it should contain categorical values such as "low", "moderate", "high", and "very high". 

The `forecast_data` and `oracle_output_data` must have the same task ID columns and `output_type`, including `output_type_id` if necessary, which are used to match the predictions with the ground truth data. As aforementioned, these data frames should follow the model output format and the oracle output format, respectively, as defined by the hubverse convention. 

The `ensemble_fun` argument specifies the ensemble method to be used for evaluating model importance. 
The currently supported methods are `"simple_ensemble"` and `"linear_pool"`. 
The `"simple_ensemble"` method returns the average of the predicted values from all component models per prediction task defined by task IDs, `output_type`, and `output_type_id` columns. 
The default aggregation function for this method is `"mean"`, but it can be customized by 
specifying additional arguments through `...`, such as `agg_fun="median"`.
When `"linear_pool"` is specified, ensemble model outputs are created as a linear pool of component model outputs. This method supports only an `output_type` of `"mean"`, `"quantile"`, or `"pmf"`.

The `weighted` argument is a logical value that indicates whether model weighting should be done when building an ensemble using the specified `ensemble_fun`. 
If it is set to `TRUE`, model weights are estimated based on the previous performance of each model, and these weights are used to build the ensemble. 

The `importance_algorithm` argument specifies the algorithm for model importance calculation, which can be either `"lomo"` (leave-one-model-out) and `"lasomo"` (leave all subsets of models out). The `subset_wt` argument is employed only for the `"lasomo"` algorithm. This argument has two options: `"equal"` assigns equal weight to all subsets and `"perm_based"` assigns weight averaged over all possible permutations as in
the formula of Shapley values (Algorithm \ref{alg:lasomo}). The default values of `importance_algorithm` and `subset_wt` are `"lomo"` and `"equal"`, respectively.


The `na_action` argument allows for specifying how to handle missing values in the `forecast_data`. 
Three options are available: `"worst"`, `"average"`, and `"drop"`. 
In each specific prediction task, if a model has any missing predictions, the `"worst"` option replaces those missing values with the smallest value from the other models, while the `"average"` option replaces them with the average of the other models' predictions in that task. The `"drop"` option removes missing values, which results in the exclusion of the model from the evaluation for that task.


## Examples {#sec:examples}

The examples in this section illustrate the use of the `model_importance()` function to evaluate the importance of component models within an ensemble, using various combinations of the arguments described in Section \ref{sec:main-function}. We use some example forecast and target data from the [hubExamples]{.pkg} package, which provides sample datasets for multiple modeling hubs in the hubverse format. 



### Evaluation using untrained ensemble in LOMO algorithm for mean forecasts  {#sec:untrained-lomo-mean}

The forecast data used here contains forecasts of weekly incident influenza hospitalizations in the US for Massachusetts (FIPS code 25) and Texas (FIPS code 48), generated on November 19, 2022. 
These forecasts are for two target end dates, November 26, 2022 (horizon 1), and  December 10, 2022 (horizon 3), and were produced by three models: 'Flusight-baseline', 'MOBS-GLEAM_FLUH', and 'PSI-DICE'. The output type is `mean` and the `output_type_id` column has `NA`s as no further specification is required for this output type. 
We have modified the example data slightly: the forecast values were rounded to the nearest integer and some forecasts have been removed to demonstrate the handling of missing values. Therefore, 'MOBS-GLEAM_FLUH''s forecast for Massachusetts on November 26, 2022, and 'PSI-DICE's forecast for Texas on December 10, 2022, are missing.


```r
> forecast_data
``` 
\small
```{r tidy = FALSE}
#| echo: false
forecast_data <- hubExamples::forecast_outputs |>
  dplyr::filter(
    output_type %in% c("mean"),
    target_end_date %in% as.Date(c("2022-11-26", "2022-12-10"))
  ) |>
  mutate(value = round(value)) |> 
  filter(
    !(model_id == "MOBS-GLEAM_FLUH" & location == "25" & target_end_date == as.Date("2022-11-26")),
    !(model_id == "PSI-DICE" & location == "48" & target_end_date == as.Date("2022-12-10"))
  )

print(forecast_data, width = Inf)
```

\normalsize

The corresponding target data contains the observed hospitalization counts for these dates and locations.

```r
> target_data
``` 
\small
```{r}
#| echo: false
target_data <- hubExamples::forecast_target_ts |>
  dplyr::filter(
    date %in% unique(forecast_data$target_end_date),
    location %in% unique(forecast_data$location)
  ) |>
  # Rename columns to match the oracle output format
  rename(
    target_end_date = date,
    oracle_value = observation
  )

print(target_data, width = Inf)
```


\normalsize

We can evaluate the importance of each model in the ensemble using the `model_importance()` function. 
The following code evaluates the importance of each model in the simple mean ensemble using the LOMO algorithm, without training the ensemble (`weighted = FALSE`). The `na_action` argument is set to `"drop"`, which represents that any missing values in the forecasts will be excluded from the evaluation. 

```r
> model_importance(
        forecast_data = forecast_data, oracle_output_data = target_data,
        ensemble_fun = "simple_ensemble", weighted = FALSE,
        importance_algorithm = "lomo", na_action = "drop"
        )
```

This call generates both the result and informative messages, summarizing the input data, including the number of dates on which forecasts were produced and the number of models with ids as follows.

\small
```r
The input data has forecast from 2022-11-19 to 2022-11-19: a total of 1 forecast date(s).
The available model IDs are:
 	 Flusight-baseline
	 MOBS-GLEAM_FLUH
	 PSI-DICE 
(a total of 3 models)
```
\normalsize

The function output is a data frame containing model IDs and their corresponding importance scores in the `mean_importance` column, ordered from most to least importance.
The column name `mean_importance` indicates the average of task-specific importance scores.


\small
```{r}
#| echo: false
model_importance(
        forecast_data = forecast_data, oracle_output_data = target_data,
        ensemble_fun = "simple_ensemble", weighted = FALSE,
        importance_algorithm = "lomo",
        na_action = "drop"
) |> arrange(desc(mean_importance))
```
\normalsize
The results show that the model 'Flusight-baseline' has the highest importance score, followed by 'PSI-DICE' and 'MOBS-GLEAM_FLUH'.

Another approach to handling missing values is to use the `"worst"` option for `na_action`, which replaces missing values with the worst (i.e., minimum) score among the other models for the same task.

```r
> model_importance(
        forecast_data = forecast_data, oracle_output_data = target_data,
        ensemble_fun = "simple_ensemble", weighted = FALSE,
        importance_algorithm = "lomo", na_action = "worst"
        )
```

\small 
```{r}
#| echo: false
model_importance(
        forecast_data = forecast_data, oracle_output_data = target_data,
        ensemble_fun = "simple_ensemble", weighted = FALSE,
        importance_algorithm = "lomo", na_action = "worst"
) |> arrange(desc(mean_importance))
```

\normalsize

Replacing missing values with the average score returns a different result, as shown below.


```r
> model_importance(
        forecast_data = forecast_data, oracle_output_data = target_data,
        ensemble_fun = "simple_ensemble", weighted = FALSE,
        importance_algorithm = "lomo", na_action = "worst"
        )
```

\small 
```{r}
#| echo: false
model_importance(
        forecast_data = forecast_data, oracle_output_data = target_data,
        ensemble_fun = "simple_ensemble", weighted = FALSE,
        importance_algorithm = "lomo", na_action = "average"
) |> arrange(desc(mean_importance))
```


\normalsize


The conservative strategy ensures that missing forecasts are penalized.















### Evaluation using trained ensemble in LOMO algorithm for quantile-based forecasts  {#sec:trained-lomo-qntl}

TO DO



## Summary and discussion {#sec:discussion}

Multi-model ensemble forecasts often provide better accuracy and robustness than single models, and are widely used in decision-making and policy planning across various domains. The contribution of each component model to the accuracy of the ensemble depends on its own unique characteristics. The [modelimportance]{.pkg} package enables the quantification of the value that each component model adds to the ensemble performance in different evaluation contexts. 

The primary function of the package is `model_importance()`, which returns a data frame with component models and their importance metrics. Users can choose the various ensemble methods to apply and model importance algorithm between LOMO and LASOMO. Additionally, customizable options are available for handling missing values. These features enable the package to serve as a versatile tool to aid collaborative efforts to construct an effective ensemble model across a wide range of forecasting tasks. 

There is a room to enhance the current version of this package. Although this package supports four different output types ('mean', 'median', 'quantile, and 'pmf'), other output types are widely used in practice. For example, 'sample' output type is commonly used in the US Flu Scenario Modeling Hub [@FluSMH]. This format includes multiple simulated values (samples) from the forecast distribution. The `output_type_id` is specified for each sample, which typically indexes the samples or indicates their source, depending on the context. The package can be extended to support this output type, which is under consideration for future releases. 
These extensions would aim to broaden the scope of applications in real-world forecasting tasks.
