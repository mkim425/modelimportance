---
title: "[modelimportance]{.pkg}: An [R]{.proglang} package for evaluating model importance within an ensemble"
format:
  jss-pdf:
    keep-tex: true
    journal:
      type: article
      cite-shortnames: true
      suppress: [title]
      include-jss-default: false
author:
  - name: Minsu Kim
    affiliations: 
      - name: University of Massachusetts Amherst
    orcid: 0009-0008-4637-3589
    email: minsu@umass.edu
  - name: Nicholas G. Reich
    affiliations:
      - University of Massachusetts Amherst
    orcid: 0000-0003-3503-9899
execute:
  echo: true
  warning: false
  message: false
bibliography: references.bib
abstract: |
  Ensemble forecasts are commonly used to support decision-making and policy planning across various fields because they often offer improved accuracy and stability compared to individual models. As each model has its own unique characteristics, understanding and measuring the value each constituent model adds to the overall accuracy of the ensemble is of great interest to building effective ensembles. The [R]{.proglang} package [modelimportance]{.pkg}  provides tools to quantify how each component model contributes to the acccuracy of ensemble performance for different forecast types. It supports multiple functionalities; it allows users to specify which ensemble approach to implement and which model importance metric to use. Additionally, the software offers customizable options for handling missing values. These features enable the package to serve as a versatile tool for researchers and practitioners aiming to construct an effective ensemble model across a wide range of forecasting tasks.
keywords: [ensemble, forecast, prediction, model importance, Shapley value, R]
keywords-formatted: [ensemble, forecast, prediction, model importance, Shapley value, R]
---

```{r}
#| echo: false
library(hubEnsembles)
library(hubUtils)
library(hubExamples)
library(readr)
library(dplyr)
library(tidyr)
library(lubridate)
library(here)
library(ggplot2)
library(kableExtra)
```


## Introduction {#sec-intro}

Ensemble forecasting is a method to produce a single, consolidated prediction by combining forecasts generated from different models. This technique leverages the strengths of individual models while mitigating their weaknesses, leading to more robust and accurate predictions [@gneiting2005weather; @hastie01statisticallearning; @lutz_applying_2019; @viboud_rapidd_2018].
Specifically, ensembles aggregate diverse insights from various models and handle variance and bias more effectively by averaging out the biases and variations of individual models, which can reduce prediction errors and improve overall model performance. Due to these enhanced prediction accuracy and robustness, it is widely used across various domains such as weather forecasting [@Guerra_2020; @gneiting2005weather], financial modeling [@SUN2020101160; @math11041054], and infectious disease outbreak forecasting [@ray_prediction_2018; @reich_accuracy_2019] to improve decision-making and achieve more reliable predictions. For example, throughout the COVID-19 pandemic, the US COVID-19 Forecast Hub collected individual models developed by over 90 different research groups and built a probabilistic ensemble forecasting model for COVID-19 cases, hospitalizations, and deaths in the US based on those models’ predictions, which served as the official short-term forecasts for the US Centers for Disease Control and Prevention (CDC) [@kim2024].

The quality of forecasts is assessed by evaluating their error, bias, sharpness, and/or calibration using different scoring metrics.
The selection of the scoring metrics depends on the type of forecast, such as point forecasts and probabilistic forecasts, and their corresponding formats, such as median/mean, quantiles, and predictive cumulative distribution functions. 
The mean absolute error (MAE) and the mean squared error (MSE) are the commonly used measures for point forecast accuracy. They provide a direct assessment of the accuracy of specific forecasted values by calculating the average magnitude of errors. 
Probabilistic forecasts are typically assessed using proper scoring rules, which consider the uncertainty and variability in predictions, providing concise evaluations through numerical scores [@gneiting_strictly_2007].  Some examples include the weighted interval score (WIS) for the quantile-based forecasts and the continuous ranked probability score (CRPS) for the forecasts taking the form of predictive cumulative distribution functions [@bracher_evaluating_2021]. 

Several [R]{.proglang} packages have been developed for this purpose. To name a few, the [forecast]{.pkg} package [@Rpackage-forecast] is widely used for univariate time series forecasting and includes functions for accuracy measurement. The [Metrics]{.pkg} [@Rpackage-Metrics] and [MLmetrics]{.pkg} [@Rpackage-MLmetrics] provide a wide range of performance metrics specifically designed for evaluating machine learning models. The [scoringRules]{.pkg} [@Rpackage-scoringRules] package offers a comprehensive set of proper scoring rules for evaluating probabilistic forecasts and supports both univariate and multivariate settings. The [scoringutils]{.pkg} [@bosse2022evaluating] package adds to the functionality provided by [scoringRules]{.pkg}, offering additional features that make it more useful for certain tasks, such as summarizing, comparing, and visualizing forecast performance. Additionally, the [scoringutils]{.pkg} supports forecasts represented by predictive samples or quantiles of predictive distributions, allowing for the assessment of any forecast type, even when a closed-form expression for a parametric distribution is not available.
These packages have been valuable to evaluate individual models as independent entities, using performance metrics selected for each specific situation or problem type. However, they do not measure the individual models' contributions to the enhanced predictive accuracy when used as part of an ensemble. 
Kim et al. [@kim2024] demonstrate that a model's individual performance does not necessarily correspond to its contribution as a component within an ensemble.
Our developed package introduces this capability.
The [modelimportance]{.pkg} package provides tools to evaluate the role of each model as an ensemble member within an ensemble model, rather than focusing on the individual predictive performance per se. 

In ensemble forecasting, certain models contribute more significantly to the overall predictions than others. Assessing the impact of each component model on ensemble predictions is methodologically similar to determining variable importance in traditional regression and machine learning models, where variable importance measures evaluate how much individual variables enhance the model's predictive performance. [R]{.proglang} packages that implement these functions include [randomForest]{.pkg} [@Rpackage-randomForest], [caret]{.pkg} [@Rpackage-caret], [xgboost]{.pkg} [@Rpackage-xgboost], and [gbm]{.pkg} [@Rpackage-gbm], each providing variable importance measures for different types of models: random forest models, general machine learning models, extreme gradient boosting models, and generalized boosted regression models, respectively.
Similarly, the tools in the [modelimportance]{.pkg} package quantify the contribution of each component model within an ensemble to the ensemble model’s predictive performance. They assign numerical scores to each model based on a selected metric that measures forecast accuracy, depending on the forecast type. 

These capabilities provide unique support for hub organizers, such as the CDC in the US and the European Centre for Disease Prevention and Control in the EU. By enabling precise evaluation of each model's contribution, the [modelimportance]{.pkg} package helps these organizations gather and utilize forecasts from various models to create more effective ensemble forecasts. This, in turn, facilitates better communication with the public and decision-makers and enhances the overall decision-making process. Specifically, [modelimportance]{.pkg} is incorporated into the `hubverse', which is a collection of open-source software and data tools developed to promote collaborative modeling hub efforts and reduce the effort required to set up and operate them [@hubverse_docs]. This package adheres to the model output formats specified by the hubverse convention, which enables seamless integration and interoperability with other forecasting tools and systems.

The paper proceeds as follows. In Section 2, we address the model output formats proposed within the hubverse framework and the structure of forecasts, followed by a motivating example. Section 3 presents two algorithms implemented in [modelimportance]{.pkg} for calculating the model importance metric: leave-one-model-out and leave-all-subsets-of-models-out. We demonstrate the various functionalities [modelimportance]{.pkg} supports in Section 4 and give some examples in Section 5. We close this paper with some concluding remarks and a discussion of possible extensions.   


## Data {#sec-data}

### Model output format {#subsec-model_output_format}
Model outputs are structured in a tabular format designed specifically for predictions.
In the hubverse standard, each row represents an individual prediction for a single task, and its details are described in multiple columns through which one can identify the model IDs, task characteristics, prediction representation type, and predicted values [@Shandross2024]. In @tbl-example-model_output, for example, the `model_id` column contains the uniquely identified name of the model that produced the prediction in each row. Task characteristics are represented by `reference_date`, `target`, `horizon`, `location`, and `target_end_date` columns, collectively referred to as the task ID columns. The type of prediction outputs is identified by the prediction representation type specified in the `output_type` and `output_type_id` columns. This example illustrates short-term forecasts of incident influenza hospitalizations in the U.S. for Massachusetts (FIPS code 25), generated by the model 'Flusight-baseline' on November 19, 2022. The forecasts are provided in seven quantiles (0.05, 0.1, 0.25,  0.5, 0.75, 0.9, 0.95) for each target end date.  


```{r example-model_output}
#| echo: false
#| label: tbl-example-model_output
#| tbl-cap: "Example of the model output for incident influenza
#| hospitalizations extracted from `forecast_outputs` data in the
#| [hubExamples]{.pkg} package."

forecast_data <- hubExamples::forecast_outputs |>
        dplyr::filter(
        output_type %in% c("quantile"),
        location == "25"
)

forecast_data |> 
        head(10) |>
        knitr::kable(format = "latex", booktabs = TRUE) |>
        kable_styling(latex_options = c("scale_down"))
```

### Structure of forecasts {#subsec-structure_of_forecasts}

The output of the forecasting model can be represented in various types. Generally, quantitative forecasts can be categorized into either point forecasts or probabilistic forecasts. 
For a specific task, point forecasts, represented by a single predicted value, provide a clear and concise prediction, making them easy to interpret and communicate. 
Probabilistic forecasts, on the other hand, describe the likelihood of different outcomes, conveying the uncertainty inherent in the prediction. These forecasts are represented by a probability distribution over possible future values in various ways, such as probability mass functions (pmf), cumulative distribution functions (cdf), or probability quantiles (or intervals). 





